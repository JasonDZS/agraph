"""
åŸºäºInterface Segregation Principleçš„LLMçŸ¥è¯†å›¾è°±æ„å»ºå™¨

éµå¾ªISPåŸåˆ™ï¼Œå°†LLMæ„å»ºå™¨åˆ†è§£ä¸ºå¤šä¸ªä¸“é—¨çš„æ¥å£å’Œå®ç°ï¼Œ
å®¢æˆ·ç«¯åªéœ€è¦ä¾èµ–ä»–ä»¬å®é™…ä½¿ç”¨çš„åŠŸèƒ½ã€‚
"""

import asyncio
import json
import logging
from datetime import datetime
from typing import Any, Dict, List, Optional, Tuple

from openai.types.chat import (
    ChatCompletionMessageParam,
    ChatCompletionSystemMessageParam,
    ChatCompletionUserMessageParam,
)

from ..embeddings import JsonVectorStorage, OpenAIEmbedding, VectorStorage
from ..entities import Entity
from ..extractors.llm_entity_extractor import LLMEntityExtractor
from ..extractors.llm_relation_extractor import LLMRelationExtractor
from ..graph import KnowledgeGraph
from ..relations import Relation
from ..types import EntityType, RelationType
from .interfaces import (
    BasicGraphBuilder,
    BatchGraphBuilder,
    FullFeaturedGraphBuilder,
    StreamingGraphBuilder,
    UpdatableGraphBuilder,
)
from .mixins import (
    GraphExporterMixin,
    GraphMergerMixin,
    GraphStatisticsMixin,
    GraphValidatorMixin,
    IncrementalBuilderMixin,
)

logger = logging.getLogger(__name__)


class LLMUsageTracker:
    """LLMä½¿ç”¨ç»Ÿè®¡è·Ÿè¸ªå™¨ - ç‹¬ç«‹çš„å…³æ³¨ç‚¹"""

    def __init__(self, llm_model: str, embedding_model: str):
        self.usage_stats: Dict[str, Any] = {
            "llm_model": {
                "model_name": llm_model,
                "total_calls": 0,
                "entity_extraction_calls": 0,
                "relation_extraction_calls": 0,
                "deduplication_calls": 0,
                "errors": 0,
                "call_history": [],
            },
            "embedding_model": {
                "model_name": embedding_model,
                "total_calls": 0,
                "entity_embedding_calls": 0,
                "relation_embedding_calls": 0,
                "errors": 0,
                "call_history": [],
            },
            "session_info": {
                "start_time": datetime.now(),
                "end_time": None,
                "total_texts_processed": 0,
                "total_entities_extracted": 0,
                "total_relations_extracted": 0,
            },
        }

    def track_llm_call(
        self,
        call_type: str,
        input_msg: str = "",
        output_msg: str = "",
        success: bool = True,
        error_msg: Optional[str] = None,
    ) -> None:
        """è·Ÿè¸ªLLM APIè°ƒç”¨ç»Ÿè®¡ä¿¡æ¯"""
        llm_stats = self.usage_stats["llm_model"]

        if success:
            llm_stats["total_calls"] += 1
            if call_type == "entity_extraction":
                llm_stats["entity_extraction_calls"] += 1
            elif call_type == "relation_extraction":
                llm_stats["relation_extraction_calls"] += 1
            elif call_type == "deduplication":
                llm_stats["deduplication_calls"] += 1
        else:
            llm_stats["errors"] += 1

        # è®°å½•è°ƒç”¨å†å²
        call_record = {
            "timestamp": datetime.now(),
            "call_type": call_type,
            "input_msg": input_msg,
            "output_msg": output_msg,
            "success": success,
            "error_msg": error_msg,
        }
        llm_stats["call_history"].append(call_record)

    def track_embedding_call(self, call_type: str, success: bool = True, error_msg: Optional[str] = None) -> None:
        """è·Ÿè¸ªembedding APIè°ƒç”¨ç»Ÿè®¡ä¿¡æ¯"""
        embedding_stats = self.usage_stats["embedding_model"]

        if success:
            embedding_stats["total_calls"] += 1
            if call_type == "entity_embedding":
                embedding_stats["entity_embedding_calls"] += 1
            elif call_type == "relation_embedding":
                embedding_stats["relation_embedding_calls"] += 1
        else:
            embedding_stats["errors"] += 1

        # è®°å½•è°ƒç”¨å†å²
        call_record = {
            "timestamp": datetime.now(),
            "call_type": call_type,
            "success": success,
            "error_msg": error_msg,
        }
        embedding_stats["call_history"].append(call_record)

    def update_session_stats(self, texts_processed: int, entities_extracted: int, relations_extracted: int) -> None:
        """æ›´æ–°ä¼šè¯ç»Ÿè®¡"""
        session = self.usage_stats["session_info"]
        session["total_texts_processed"] += texts_processed
        session["total_entities_extracted"] += entities_extracted
        session["total_relations_extracted"] += relations_extracted

    def get_usage_statistics(self) -> Dict[str, Any]:
        """è·å–ä½¿ç”¨ç»Ÿè®¡ä¿¡æ¯"""
        return self.usage_stats.copy()

    def export_usage_stats(self, file_path: str) -> None:
        """å¯¼å‡ºä½¿ç”¨ç»Ÿè®¡åˆ°JSONæ–‡ä»¶"""
        stats_copy = json.loads(json.dumps(self.usage_stats, default=str))
        with open(file_path, "w", encoding="utf-8") as f:
            json.dump(stats_copy, f, ensure_ascii=False, indent=2)
        logger.info(f"Usage statistics exported to {file_path}")

    def print_usage_summary(self) -> None:
        """æ‰“å°ä½¿ç”¨ç»Ÿè®¡æ‘˜è¦"""
        stats = self.usage_stats

        print("\n" + "=" * 60)
        print("ğŸ“Š LLM Graph Builder ä½¿ç”¨ç»Ÿè®¡")
        print("=" * 60)

        # ä¼šè¯ä¿¡æ¯
        session = stats["session_info"]
        start_time = session["start_time"]
        end_time = session.get("end_time") or datetime.now()
        duration = end_time - start_time

        print(f"â±ï¸  ä¼šè¯æ—¶é•¿: {duration}")
        print(f"ğŸ“„ å¤„ç†æ–‡æœ¬æ•°: {session['total_texts_processed']}")
        print(f"ğŸ·ï¸  æå–å®ä½“æ•°: {session['total_entities_extracted']}")
        print(f"ğŸ”— æå–å…³ç³»æ•°: {session['total_relations_extracted']}")
        print()

        # LLMç»Ÿè®¡
        llm = stats["llm_model"]
        print(f"ğŸ¤– LLMæ¨¡å‹: {llm['model_name']}")
        print(f"   ğŸ“ æ€»è°ƒç”¨æ¬¡æ•°: {llm['total_calls']}")
        print(f"   âŒ é”™è¯¯æ¬¡æ•°: {llm['errors']}")
        print(f"   â”œâ”€â”€ å®ä½“æå–: {llm['entity_extraction_calls']} æ¬¡")
        print(f"   â”œâ”€â”€ å…³ç³»æå–: {llm['relation_extraction_calls']} æ¬¡")
        print(f"   â””â”€â”€ å®ä½“å»é‡: {llm['deduplication_calls']} æ¬¡")
        print()

        # Embeddingç»Ÿè®¡
        embedding = stats["embedding_model"]
        print(f"ğŸ”¤ Embeddingæ¨¡å‹: {embedding['model_name']}")
        print(f"   ğŸ“ æ€»è°ƒç”¨æ¬¡æ•°: {embedding['total_calls']}")
        print(f"   âŒ é”™è¯¯æ¬¡æ•°: {embedding['errors']}")
        print(f"   â”œâ”€â”€ å®ä½“åµŒå…¥: {embedding['entity_embedding_calls']} æ¬¡")
        print(f"   â””â”€â”€ å…³ç³»åµŒå…¥: {embedding['relation_embedding_calls']} æ¬¡")
        print()

    def cleanup(self) -> None:
        """æ¸…ç†èµ„æº"""
        self.usage_stats["session_info"]["end_time"] = str(datetime.now())


class LLMAsyncProcessor:
    """LLMå¼‚æ­¥å¤„ç†å™¨ - ç‹¬ç«‹çš„å¤„ç†é€»è¾‘"""

    def __init__(
        self,
        entity_extractor: LLMEntityExtractor,
        relation_extractor: LLMRelationExtractor,
        usage_tracker: LLMUsageTracker,
        max_concurrent: int = 10,
    ):
        self.entity_extractor = entity_extractor
        self.relation_extractor = relation_extractor
        self.usage_tracker = usage_tracker
        self.max_concurrent = max_concurrent
        self._deduplication_calls_count = 0

    async def process_texts_async(self, texts: List[str]) -> Tuple[List[Dict], List[Dict]]:
        """å¼‚æ­¥å¤„ç†æ–‡æœ¬åˆ—è¡¨"""
        if len(texts) <= 1:
            return await self._process_texts_sequential(texts)

        all_entities: List[Dict[str, Any]] = []
        all_relations: List[Dict[str, Any]] = []

        # ä½¿ç”¨ä¿¡å·é‡æ§åˆ¶å¹¶å‘æ•°
        semaphore = asyncio.Semaphore(self.max_concurrent)

        async def process_with_semaphore(
            text: str, index: int
        ) -> Tuple[int, List[Dict[str, Any]], List[Dict[str, Any]]]:
            async with semaphore:
                return await self._process_single_text_async(text, index)

        # åˆ›å»ºå¼‚æ­¥ä»»åŠ¡
        tasks = [process_with_semaphore(text, i) for i, text in enumerate(texts)]

        # æ‰¹é‡æ‰§è¡Œ
        logger.info(f"Processing {len(texts)} texts with max_concurrent={self.max_concurrent}")
        results = await asyncio.gather(*tasks, return_exceptions=True)

        # å¤„ç†ç»“æœ
        valid_results = []
        for result in results:
            if isinstance(result, Exception):
                logger.error(f"Error in async execution: {result}")
            else:
                # ç¡®ä¿ result æ˜¯æ­£ç¡®çš„ç±»å‹
                if isinstance(result, tuple) and len(result) == 3:
                    valid_results.append(result)
                else:
                    logger.warning(f"Unexpected result format: {result}")

        # æŒ‰ç´¢å¼•æ’åºå¹¶åˆå¹¶ç»“æœ
        if valid_results:
            valid_results.sort(key=lambda x: x[0])
            for _, entities_data, relations_data in valid_results:
                all_entities.extend(entities_data)
                all_relations.extend(relations_data)

        return all_entities, all_relations

    async def _process_single_text_async(self, text: str, index: int) -> Tuple[int, List[Dict], List[Dict]]:
        """å¼‚æ­¥å¤„ç†å•ä¸ªæ–‡æœ¬"""
        try:
            # å¼‚æ­¥æå–å®ä½“
            entities = await self.entity_extractor._extract_entities_async(text)
            self.usage_tracker.track_llm_call(
                call_type="entity_extraction",
                input_msg=text,
                output_msg=str(entities),
                success=True,
            )

            entities_data = [
                {
                    "name": entity.name,
                    "type": entity.entity_type.value,
                    "description": entity.description,
                    "aliases": entity.aliases,
                    "properties": entity.properties,
                }
                for entity in entities
            ]

            # å¼‚æ­¥æå–å…³ç³»
            relations = await self.relation_extractor._extract_relations_async(text, entities)
            self.usage_tracker.track_llm_call(call_type="relation_extraction", success=True)

            relations_data = [
                {
                    "head_entity": relation.head_entity.name if relation.head_entity else "",
                    "tail_entity": relation.tail_entity.name if relation.tail_entity else "",
                    "relation_type": relation.relation_type.value,
                    "description": relation.description,
                    "properties": relation.properties,
                    "confidence": relation.confidence,
                }
                for relation in relations
            ]

            return index, entities_data, relations_data

        except Exception as e:
            logger.error(f"Error processing text {index}: {e}")
            self.usage_tracker.track_llm_call(call_type="entity_extraction", success=False, error_msg=str(e))
            return index, [], []

    async def _process_texts_sequential(self, texts: List[str]) -> Tuple[List[Dict], List[Dict]]:
        """ä¸²è¡Œå¤„ç†æ–‡æœ¬åˆ—è¡¨"""
        all_entities = []
        all_relations = []

        for i, text in enumerate(texts):
            logger.info(f"Processing text {i+1}/{len(texts)}")
            try:
                entities = await self.entity_extractor._extract_entities_async(text)
                self.usage_tracker.track_llm_call(call_type="entity_extraction", success=True)

                entities_data = [
                    {
                        "name": entity.name,
                        "type": entity.entity_type.value,
                        "description": entity.description,
                        "aliases": entity.aliases,
                        "properties": entity.properties,
                    }
                    for entity in entities
                ]
                all_entities.extend(entities_data)

                relations = await self.relation_extractor._extract_relations_async(text, entities)
                self.usage_tracker.track_llm_call(call_type="relation_extraction", success=True)

                relations_data = [
                    {
                        "head_entity": relation.head_entity.name if relation.head_entity else "",
                        "tail_entity": relation.tail_entity.name if relation.tail_entity else "",
                        "relation_type": relation.relation_type.value,
                        "description": relation.description,
                        "properties": relation.properties,
                        "confidence": relation.confidence,
                    }
                    for relation in relations
                ]
                all_relations.extend(relations_data)

            except Exception as e:
                logger.error(f"Error processing text {i+1}: {e}")
                self.usage_tracker.track_llm_call(call_type="entity_extraction", success=False, error_msg=str(e))

        return all_entities, all_relations

    async def deduplicate_entities(self, entities: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """ä¼˜åŒ–çš„å®ä½“å»é‡"""
        if len(entities) <= 1:
            return entities

        logger.info(f"Starting optimized deduplication for {len(entities)} entities")

        # Convert dict entities to Entity objects
        entity_objects = []
        for entity_data in entities:
            entity = Entity(
                id=self._generate_entity_id(entity_data["name"]),
                name=entity_data["name"],
                entity_type=EntityType(entity_data["type"]),
                description=entity_data.get("description", ""),
                aliases=entity_data.get("aliases", []),
                properties=entity_data.get("properties", {}),
                created_at=datetime.now(),
                updated_at=datetime.now(),
            )
            entity_objects.append(entity)

        # ä½¿ç”¨ä¼˜åŒ–çš„å¹¶å‘å»é‡æ–¹æ³•
        deduplicated_entities = await self._deduplicate_entities_concurrent(entity_objects)

        # è·Ÿè¸ªå»é‡è°ƒç”¨ç»Ÿè®¡
        if len(entity_objects) > 1:
            actual_calls = self._deduplication_calls_count
            for _ in range(actual_calls):
                self.usage_tracker.track_llm_call(call_type="deduplication", success=True)

        logger.info(f"Deduplication completed: {len(entity_objects)} -> {len(deduplicated_entities)} entities")

        # Convert back to dict format
        return [
            {
                "name": entity.name,
                "type": entity.entity_type.value,
                "description": entity.description,
                "aliases": entity.aliases,
                "properties": entity.properties,
            }
            for entity in deduplicated_entities
        ]

    async def _deduplicate_entities_concurrent(self, entities: List[Entity]) -> List[Entity]:
        """å¹¶å‘ä¼˜åŒ–çš„å®ä½“å»é‡æ–¹æ³•"""
        if len(entities) <= 1:
            return entities

        self._deduplication_calls_count = 0

        # ç¬¬ä¸€æ­¥ï¼šå¿«é€Ÿé¢„ç­›é€‰
        logger.info("Step 1: Fast pre-filtering based on name and type similarity")
        candidate_pairs = self._prefilter_duplicate_candidates(entities)

        if not candidate_pairs:
            logger.info("No potential duplicates found in pre-filtering")
            return entities

        logger.info(f"Found {len(candidate_pairs)} potential duplicate pairs for LLM verification")

        # ç¬¬äºŒæ­¥ï¼šå¹¶å‘LLMéªŒè¯
        logger.info("Step 2: Concurrent LLM verification of potential duplicates")
        duplicate_pairs = await self._verify_duplicates_concurrent(entities, candidate_pairs)

        # ç¬¬ä¸‰æ­¥ï¼šåˆå¹¶é‡å¤å®ä½“
        logger.info("Step 3: Merging duplicate entities")
        return self._merge_duplicate_entities(entities, duplicate_pairs)

    def _prefilter_duplicate_candidates(self, entities: List[Entity]) -> List[Tuple[int, int]]:
        """é¢„ç­›é€‰æ½œåœ¨é‡å¤å®ä½“å¯¹"""
        candidate_pairs = []

        for i in range(len(entities)):
            for j in range(i + 1, len(entities)):
                entity1, entity2 = entities[i], entities[j]

                # åªå¯¹ç›¸åŒç±»å‹çš„å®ä½“è¿›è¡Œæ¯”è¾ƒ
                if entity1.entity_type != entity2.entity_type:
                    continue

                # è®¡ç®—åç§°ç›¸ä¼¼åº¦
                name_similarity = self._calculate_name_similarity(entity1.name, entity2.name)

                # æ£€æŸ¥åˆ«ååŒ¹é…
                alias_match = self._check_alias_match(entity1, entity2)

                # å¦‚æœåç§°ç›¸ä¼¼åº¦é«˜æˆ–æœ‰åˆ«ååŒ¹é…ï¼Œåˆ™è®¤ä¸ºæ˜¯å€™é€‰é‡å¤å¯¹
                if name_similarity > 0.7 or alias_match:
                    candidate_pairs.append((i, j))

        return candidate_pairs

    def _calculate_name_similarity(self, name1: str, name2: str) -> float:
        """è®¡ç®—ä¸¤ä¸ªåç§°çš„ç›¸ä¼¼åº¦"""
        name1_lower = name1.lower().strip()
        name2_lower = name2.lower().strip()

        if name1_lower == name2_lower:
            return 1.0

        if name1_lower in name2_lower or name2_lower in name1_lower:
            return 0.8

        return self._levenshtein_similarity(name1_lower, name2_lower)

    def _levenshtein_similarity(self, s1: str, s2: str) -> float:
        """è®¡ç®—ç¼–è¾‘è·ç¦»ç›¸ä¼¼åº¦"""
        if len(s1) == 0 or len(s2) == 0:
            return 0.0

        if len(s1) > len(s2):
            s1, s2 = s2, s1

        distances = list(range(len(s1) + 1))
        for i2, c2 in enumerate(s2):
            new_distances = [i2 + 1]
            for i1, c1 in enumerate(s1):
                if c1 == c2:
                    new_distances.append(distances[i1])
                else:
                    new_distances.append(1 + min(distances[i1], distances[i1 + 1], new_distances[-1]))
            distances = new_distances

        max_len = max(len(s1), len(s2))
        return 1.0 - (distances[-1] / max_len)

    def _check_alias_match(self, entity1: Entity, entity2: Entity) -> bool:
        """æ£€æŸ¥ä¸¤ä¸ªå®ä½“æ˜¯å¦æœ‰åˆ«ååŒ¹é…"""
        all_names1 = {entity1.name.lower()} | {alias.lower() for alias in entity1.aliases}
        all_names2 = {entity2.name.lower()} | {alias.lower() for alias in entity2.aliases}
        return bool(all_names1 & all_names2)

    async def _verify_duplicates_concurrent(
        self, entities: List[Entity], candidate_pairs: List[Tuple[int, int]]
    ) -> List[Tuple[int, int]]:
        """å¹¶å‘éªŒè¯æ½œåœ¨é‡å¤å®ä½“å¯¹"""
        if not candidate_pairs:
            return []

        semaphore = asyncio.Semaphore(min(self.max_concurrent, 5))

        async def verify_single_pair(pair_idx: int, entity_pair: Tuple[int, int]) -> Tuple[int, bool]:
            async with semaphore:
                entity1_idx, entity2_idx = entity_pair
                entity1, entity2 = entities[entity1_idx], entities[entity2_idx]
                is_duplicate = await self.entity_extractor._check_entity_duplicate_llm(entity1, entity2)
                self._deduplication_calls_count += 1
                return pair_idx, is_duplicate

        tasks = [verify_single_pair(i, pair) for i, pair in enumerate(candidate_pairs)]

        logger.info(f"Verifying {len(candidate_pairs)} potential duplicate pairs concurrently")
        results = await asyncio.gather(*tasks, return_exceptions=True)

        duplicate_pairs = []
        for result in results:
            if isinstance(result, Exception):
                logger.error(f"Error in duplicate verification: {result}")
            elif isinstance(result, tuple) and len(result) == 2:
                pair_idx, is_duplicate = result
                if is_duplicate:
                    duplicate_pairs.append(candidate_pairs[pair_idx])
            else:
                logger.error(f"Unexpected result format: {result}")

        logger.info(f"Found {len(duplicate_pairs)} confirmed duplicate pairs")
        return duplicate_pairs

    def _merge_duplicate_entities(self, entities: List[Entity], duplicate_pairs: List[Tuple[int, int]]) -> List[Entity]:
        """åˆå¹¶é‡å¤å®ä½“"""
        if not duplicate_pairs:
            return entities

        # ä½¿ç”¨å¹¶æŸ¥é›†å¤„ç†ä¼ é€’æ€§é‡å¤å…³ç³»
        parent = list(range(len(entities)))

        def find(x: int) -> int:
            if parent[x] != x:
                parent[x] = find(parent[x])
            return parent[x]

        def union(x: int, y: int) -> None:
            px, py = find(x), find(y)
            if px != py:
                parent[px] = py

        for i, j in duplicate_pairs:
            union(i, j)

        # åˆ†ç»„å®ä½“
        groups: Dict[int, List[int]] = {}
        for i in range(len(entities)):
            root = find(i)
            if root not in groups:
                groups[root] = []
            groups[root].append(i)

        # åˆå¹¶æ¯ä¸ªç»„çš„å®ä½“
        merged_entities = []
        for group_indices in groups.values():
            if len(group_indices) == 1:
                merged_entities.append(entities[group_indices[0]])
            else:
                merged_entity = self._merge_multiple_entities([entities[i] for i in group_indices])
                merged_entities.append(merged_entity)

        return merged_entities

    def _merge_multiple_entities(self, entities_to_merge: List[Entity]) -> Entity:
        """åˆå¹¶å¤šä¸ªå®ä½“ä¸ºä¸€ä¸ª"""
        if not entities_to_merge:
            raise ValueError("No entities to merge")

        if len(entities_to_merge) == 1:
            return entities_to_merge[0]

        base_entity = entities_to_merge[0]
        all_aliases = set(base_entity.aliases)
        all_descriptions = [base_entity.description] if base_entity.description else []
        merged_properties = base_entity.properties.copy()

        for entity in entities_to_merge[1:]:
            all_aliases.update(entity.aliases)
            if entity.name != base_entity.name:
                all_aliases.add(entity.name)

            if entity.description and entity.description not in all_descriptions:
                all_descriptions.append(entity.description)

            merged_properties.update(entity.properties)

        merged_entity = Entity(
            id=base_entity.id,
            name=base_entity.name,
            entity_type=base_entity.entity_type,
            description="; ".join(all_descriptions),
            aliases=list(all_aliases),
            properties=merged_properties,
            created_at=base_entity.created_at,
            updated_at=datetime.now(),
        )

        return merged_entity

    @staticmethod
    def _generate_entity_id(name: str) -> str:
        """ç”Ÿæˆå®ä½“ID"""
        import hashlib

        return f"entity_{hashlib.md5(name.encode()).hexdigest()[:8]}"


class LLMGraphUtils:
    """LLMå›¾æ„å»ºå·¥å…·ç±» - å…¬å…±å·¥å…·æ–¹æ³•"""

    @staticmethod
    def generate_entity_id(name: str) -> str:
        """ç”Ÿæˆå®ä½“ID"""
        import hashlib

        return f"entity_{hashlib.md5(name.encode()).hexdigest()[:8]}"

    @staticmethod
    def generate_relation_id(head: str, tail: str, relation_type: str) -> str:
        """ç”Ÿæˆå…³ç³»ID"""
        import hashlib

        relation_str = f"{head}_{relation_type}_{tail}"
        return f"relation_{hashlib.md5(relation_str.encode()).hexdigest()[:8]}"

    @staticmethod
    def find_similar_entity(graph: KnowledgeGraph, entity_data: Dict[str, Any]) -> Optional[Entity]:
        """åœ¨ç°æœ‰å›¾è°±ä¸­æŸ¥æ‰¾ç›¸ä¼¼å®ä½“"""
        entity_name = entity_data.get("name", "").lower()

        for entity in graph.entities.values():
            if entity.name.lower() == entity_name:
                return entity

            if entity_name in [alias.lower() for alias in entity.aliases]:
                return entity

            for alias in entity_data.get("aliases", []):
                if alias.lower() == entity.name.lower():
                    return entity
                if alias.lower() in [a.lower() for a in entity.aliases]:
                    return entity

        return None

    @staticmethod
    def merge_entity_data(entity: Entity, entity_data: Dict[str, Any]) -> None:
        """å°†æ–°çš„å®ä½“æ•°æ®åˆå¹¶åˆ°ç°æœ‰å®ä½“"""
        new_desc = entity_data.get("description", "")
        if new_desc and new_desc not in entity.description:
            entity.description = f"{entity.description}; {new_desc}" if entity.description else new_desc

        new_aliases = entity_data.get("aliases", [])
        for alias in new_aliases:
            if alias not in entity.aliases:
                entity.aliases.append(alias)

        new_props = entity_data.get("properties", {})
        entity.properties.update(new_props)
        entity.updated_at = datetime.now()

    @staticmethod
    def find_existing_relation(
        graph: KnowledgeGraph, head_name: str, tail_name: str, relation_type: str
    ) -> Optional[Relation]:
        """æŸ¥æ‰¾ç°æœ‰å…³ç³»"""
        for relation in graph.relations.values():
            if (
                relation.head_entity
                and relation.tail_entity
                and relation.head_entity.name == head_name
                and relation.tail_entity.name == tail_name
                and relation.relation_type.value == relation_type
            ):
                return relation
        return None

    @staticmethod
    def build_entities_from_data(graph: KnowledgeGraph, entity_data_list: List[Dict[str, Any]]) -> Dict[str, str]:
        """ä»æ•°æ®æ„å»ºå®ä½“å¹¶è¿”å›åç§°åˆ°IDçš„æ˜ å°„"""
        entity_mapping = {}

        for entity_data in entity_data_list:
            entity = Entity(
                id=LLMGraphUtils.generate_entity_id(entity_data["name"]),
                name=entity_data["name"],
                entity_type=EntityType(entity_data["type"]),
                description=entity_data.get("description", ""),
                aliases=entity_data.get("aliases", []),
                properties=entity_data.get("properties", {}),
                created_at=datetime.now(),
                updated_at=datetime.now(),
            )
            graph.add_entity(entity)
            entity_mapping[entity_data["name"]] = entity.id

        return entity_mapping

    @staticmethod
    def build_relations_from_data(
        graph: KnowledgeGraph, relations_data: List[Dict[str, Any]], entity_mapping: Dict[str, str]
    ) -> int:
        """ä»æ•°æ®æ„å»ºå…³ç³»å¹¶è¿”å›æ·»åŠ çš„å…³ç³»æ•°é‡"""
        relations_added = 0

        for relation_data in relations_data:
            head_name = relation_data["head_entity"]
            tail_name = relation_data["tail_entity"]

            if head_name in entity_mapping and tail_name in entity_mapping:
                head_entity = graph.get_entity(entity_mapping[head_name])
                tail_entity = graph.get_entity(entity_mapping[tail_name])

                if head_entity and tail_entity:
                    relation = Relation(
                        id=LLMGraphUtils.generate_relation_id(head_name, tail_name, relation_data["relation_type"]),
                        head_entity=head_entity,
                        tail_entity=tail_entity,
                        relation_type=RelationType(relation_data["relation_type"]),
                        description=relation_data.get("description", ""),
                        properties=relation_data.get("properties", {}),
                        confidence=relation_data.get("confidence", 1.0),
                        created_at=datetime.now(),
                        updated_at=datetime.now(),
                    )
                    graph.add_relation(relation)
                    relations_added += 1

        return relations_added


# ============================================================================
# ISP-compliant LLM Builder Implementations
# ============================================================================


class MinimalLLMGraphBuilder(BasicGraphBuilder):
    """
    æœ€å°åŒ–çš„LLMå›¾æ„å»ºå™¨ - åªå®ç°æ ¸å¿ƒæ„å»ºåŠŸèƒ½

    é€‚ç”¨äºåªéœ€è¦åŸºæœ¬å›¾æ„å»ºè€Œä¸éœ€è¦æ›´æ–°ã€åˆå¹¶æˆ–å…¶ä»–é«˜çº§åŠŸèƒ½çš„å®¢æˆ·ç«¯ã€‚
    """

    def __init__(
        self,
        openai_api_key: str,
        openai_api_base: Optional[str] = None,
        llm_model: str = "gpt-4o-mini",
        max_tokens: int = 4000,
        temperature: float = 0.1,
        entity_extractor: Optional[LLMEntityExtractor] = None,
        relation_extractor: Optional[LLMRelationExtractor] = None,
    ):
        """
        åˆå§‹åŒ–æœ€å°åŒ–LLMå›¾æ„å»ºå™¨

        Args:
            openai_api_key: OpenAI APIå¯†é’¥
            openai_api_base: OpenAI APIåŸºç¡€URL
            llm_model: ä½¿ç”¨çš„LLMæ¨¡å‹
            max_tokens: æœ€å¤§tokenæ•°
            temperature: æ¸©åº¦å‚æ•°
            entity_extractor: å®ä½“æå–å™¨
            relation_extractor: å…³ç³»æå–å™¨
        """
        self.entity_extractor = entity_extractor or LLMEntityExtractor(
            openai_api_key=openai_api_key,
            openai_api_base=openai_api_base,
            llm_model=llm_model,
            max_tokens=max_tokens,
            temperature=temperature,
        )
        self.relation_extractor = relation_extractor or LLMRelationExtractor(
            openai_api_key=openai_api_key,
            openai_api_base=openai_api_base,
            llm_model=llm_model,
            max_tokens=max_tokens,
            temperature=temperature,
        )

    async def build_graph(
        self,
        texts: Optional[List[str]] = None,
        database_schema: Optional[Dict[str, Any]] = None,
        graph_name: str = "minimal_llm_graph",
    ) -> KnowledgeGraph:
        """æ„å»ºçŸ¥è¯†å›¾è°± - å¼‚æ­¥ç‰ˆæœ¬"""
        if not texts:
            return KnowledgeGraph(name=graph_name)

        try:
            graph = KnowledgeGraph(name=graph_name)
            all_entities = []
            all_relations = []

            # ä¸²è¡Œå¤„ç†æ¯ä¸ªæ–‡æœ¬
            for text in texts:
                # å¼‚æ­¥æå–å®ä½“
                entities = await self.entity_extractor._extract_entities_async(text)
                entity_data = [
                    {
                        "name": entity.name,
                        "type": entity.entity_type.value,
                        "description": entity.description,
                        "aliases": entity.aliases,
                        "properties": entity.properties,
                    }
                    for entity in entities
                ]
                all_entities.extend(entity_data)

                # å¼‚æ­¥æå–å…³ç³»
                relations = await self.relation_extractor._extract_relations_async(text, entities)
                relation_data = [
                    {
                        "head_entity": relation.head_entity.name if relation.head_entity else "",
                        "tail_entity": relation.tail_entity.name if relation.tail_entity else "",
                        "relation_type": relation.relation_type.value,
                        "description": relation.description,
                        "properties": relation.properties,
                        "confidence": relation.confidence,
                    }
                    for relation in relations
                ]
                all_relations.extend(relation_data)

            # æ„å»ºå›¾è°±
            entity_mapping = LLMGraphUtils.build_entities_from_data(graph, all_entities)
            LLMGraphUtils.build_relations_from_data(graph, all_relations, entity_mapping)

            logger.info(f"Minimal LLM graph built: {len(graph.entities)} entities, {len(graph.relations)} relations")
            return graph

        except Exception as e:
            logger.error(f"Error building minimal LLM graph: {e}")
            raise


class FlexibleLLMGraphBuilder(UpdatableGraphBuilder):
    """
    çµæ´»çš„LLMå›¾æ„å»ºå™¨ - æ”¯æŒæ„å»ºå’Œæ›´æ–°

    é€‚ç”¨äºéœ€è¦æ›´æ–°å›¾è°±ä½†ä¸éœ€è¦åˆå¹¶ã€éªŒè¯ç­‰é«˜çº§åŠŸèƒ½çš„å®¢æˆ·ç«¯ã€‚
    """

    def __init__(
        self,
        openai_api_key: str,
        openai_api_base: Optional[str] = None,
        llm_model: str = "gpt-4o-mini",
        embedding_model: str = "text-embedding-3-small",
        max_tokens: int = 4000,
        temperature: float = 0.1,
        vector_storage: Optional[VectorStorage] = None,
        entity_extractor: Optional[LLMEntityExtractor] = None,
        relation_extractor: Optional[LLMRelationExtractor] = None,
        max_concurrent: int = 10,
    ):
        """
        åˆå§‹åŒ–çµæ´»LLMå›¾æ„å»ºå™¨

        Args:
            openai_api_key: OpenAI APIå¯†é’¥
            openai_api_base: OpenAI APIåŸºç¡€URL
            llm_model: ä½¿ç”¨çš„LLMæ¨¡å‹
            embedding_model: ä½¿ç”¨çš„åµŒå…¥æ¨¡å‹
            max_tokens: æœ€å¤§tokenæ•°
            temperature: æ¸©åº¦å‚æ•°
            vector_storage: å‘é‡å­˜å‚¨åç«¯
            entity_extractor: å®ä½“æå–å™¨
            relation_extractor: å…³ç³»æå–å™¨
            max_concurrent: æœ€å¤§å¹¶å‘æ•°
        """
        self.entity_extractor = entity_extractor or LLMEntityExtractor(
            openai_api_key=openai_api_key,
            openai_api_base=openai_api_base,
            llm_model=llm_model,
            max_tokens=max_tokens,
            temperature=temperature,
        )
        self.relation_extractor = relation_extractor or LLMRelationExtractor(
            openai_api_key=openai_api_key,
            openai_api_base=openai_api_base,
            llm_model=llm_model,
            max_tokens=max_tokens,
            temperature=temperature,
        )

        # åˆå§‹åŒ–ä½¿ç”¨ç»Ÿè®¡è·Ÿè¸ªå™¨
        self.usage_tracker = LLMUsageTracker(llm_model, embedding_model)

        # åˆå§‹åŒ–å¼‚æ­¥å¤„ç†å™¨
        self.async_processor = LLMAsyncProcessor(
            self.entity_extractor,
            self.relation_extractor,
            self.usage_tracker,
            max_concurrent,
        )

        # åˆå§‹åŒ–å‘é‡åµŒå…¥ï¼ˆå¯é€‰ï¼‰
        self.vector_storage = vector_storage or JsonVectorStorage("flexible_llm_graph_vectors.json")
        self.graph_embedding = OpenAIEmbedding(
            vector_storage=self.vector_storage,
            openai_api_key=openai_api_key,
            openai_api_base=openai_api_base,
            embedding_model=embedding_model,
        )

    async def build_graph(
        self,
        texts: Optional[List[str]] = None,
        database_schema: Optional[Dict[str, Any]] = None,
        graph_name: str = "flexible_llm_graph",
    ) -> KnowledgeGraph:
        """å¼‚æ­¥æ„å»ºçŸ¥è¯†å›¾è°±"""
        if not texts:
            return KnowledgeGraph(name=graph_name)

        logger.info("Starting flexible LLM-based knowledge graph construction")

        try:
            graph = KnowledgeGraph(name=graph_name)

            # æ›´æ–°ä¼šè¯ç»Ÿè®¡
            self.usage_tracker.update_session_stats(len(texts), 0, 0)

            # å¼‚æ­¥å¤„ç†æ–‡æœ¬
            all_entities, all_relations = await self.async_processor.process_texts_async(texts)

            # å»é‡å®ä½“
            deduplicated_entities = await self.async_processor.deduplicate_entities(all_entities)

            # æ„å»ºå›¾è°±
            entity_mapping = LLMGraphUtils.build_entities_from_data(graph, deduplicated_entities)
            relations_added = LLMGraphUtils.build_relations_from_data(graph, all_relations, entity_mapping)

            # æ›´æ–°ç»Ÿè®¡
            self.usage_tracker.update_session_stats(0, len(deduplicated_entities), relations_added)

            # æ„å»ºåµŒå…¥ï¼ˆå¯é€‰ï¼‰
            if self.graph_embedding:
                await self._build_embeddings(graph)

            logger.info(f"Flexible LLM graph built: {len(graph.entities)} entities, {len(graph.relations)} relations")
            return graph

        except Exception as e:
            logger.error(f"Error building flexible LLM graph: {e}")
            raise

    async def update_graph(
        self,
        graph: KnowledgeGraph,
        new_entities: Optional[List[Entity]] = None,
        new_relations: Optional[List[Relation]] = None,
    ) -> KnowledgeGraph:
        """æ›´æ–°ç°æœ‰å›¾è°±"""
        logger.info("Updating flexible LLM graph")

        try:
            if new_entities:
                for entity in new_entities:
                    if entity.id not in graph.entities:
                        graph.add_entity(entity)

            if new_relations:
                for relation in new_relations:
                    if (
                        relation.head_entity
                        and relation.head_entity.id in graph.entities
                        and relation.tail_entity
                        and relation.tail_entity.id in graph.entities
                        and relation.id not in graph.relations
                    ):
                        graph.add_relation(relation)

            # æ›´æ–°åµŒå…¥
            if self.graph_embedding:
                await self._build_embeddings(graph)

            logger.info(f"Flexible LLM graph updated: {len(graph.entities)} entities, {len(graph.relations)} relations")
            return graph

        except Exception as e:
            logger.error(f"Error updating flexible LLM graph: {e}")
            raise

    async def update_graph_with_texts(
        self,
        graph: KnowledgeGraph,
        texts: List[str],
    ) -> KnowledgeGraph:
        """ä½¿ç”¨æ–°æ–‡æœ¬æ›´æ–°å›¾è°±"""
        if not texts:
            return graph

        logger.info(f"Updating graph with {len(texts)} new texts")

        # å¤„ç†æ–°æ–‡æœ¬
        all_entities, all_relations = await self.async_processor.process_texts_async(texts)

        # ä¸ç°æœ‰å®ä½“è¿›è¡ŒåŒ¹é…å’Œå»é‡
        new_entities = []
        for entity_data in all_entities:
            existing_entity = LLMGraphUtils.find_similar_entity(graph, entity_data)
            if existing_entity:
                LLMGraphUtils.merge_entity_data(existing_entity, entity_data)
            else:
                new_entities.append(entity_data)

        # æ·»åŠ æ–°å®ä½“
        entity_mapping = {entity.name: entity.id for entity in graph.entities.values()}
        new_entity_mapping = LLMGraphUtils.build_entities_from_data(graph, new_entities)
        entity_mapping.update(new_entity_mapping)

        # æ·»åŠ æ–°å…³ç³»
        relations_added = LLMGraphUtils.build_relations_from_data(graph, all_relations, entity_mapping)

        # æ›´æ–°åµŒå…¥
        if self.graph_embedding:
            await self._build_embeddings(graph)

        logger.info(f"Graph updated with texts: {len(new_entities)} new entities, {relations_added} new relations")
        return graph

    async def _build_embeddings(self, graph: KnowledgeGraph) -> None:
        """æ„å»ºå›¾è°±åµŒå…¥"""
        try:
            await self.graph_embedding.build_text_embeddings(graph)
            self.usage_tracker.track_embedding_call("entity_embedding", success=True)
            self.usage_tracker.track_embedding_call("relation_embedding", success=True)
            self.graph_embedding.save_embeddings()
        except Exception as e:
            logger.error(f"Error building embeddings: {e}")
            self.usage_tracker.track_embedding_call("entity_embedding", success=False, error_msg=str(e))

    def get_usage_statistics(self) -> Dict[str, Any]:
        """è·å–ä½¿ç”¨ç»Ÿè®¡ä¿¡æ¯"""
        return self.usage_tracker.get_usage_statistics()

    def print_usage_summary(self) -> None:
        """æ‰“å°ä½¿ç”¨ç»Ÿè®¡æ‘˜è¦"""
        self.usage_tracker.print_usage_summary()

    def cleanup(self) -> None:
        """æ¸…ç†èµ„æº"""
        self.usage_tracker.cleanup()
        if self.graph_embedding:
            self.graph_embedding.save_embeddings()


class LLMGraphBuilder(
    GraphMergerMixin,
    GraphValidatorMixin,
    GraphExporterMixin,
    GraphStatisticsMixin,
    FullFeaturedGraphBuilder,
):
    """
    å…¨åŠŸèƒ½LLMå›¾æ„å»ºå™¨ - åŒ…å«æ‰€æœ‰åŠŸèƒ½

    åªæœ‰éœ€è¦æ‰€æœ‰åŠŸèƒ½çš„å®¢æˆ·ç«¯æ‰åº”è¯¥ä½¿ç”¨è¿™ä¸ªç±»ã€‚
    å¤§å¤šæ•°å®¢æˆ·ç«¯åº”è¯¥ä½¿ç”¨æ›´ä¸“æ³¨çš„æ¥å£ã€‚
    """

    def __init__(
        self,
        openai_api_key: str,
        openai_api_base: Optional[str] = None,
        llm_model: str = "gpt-4o-mini",
        embedding_model: str = "text-embedding-3-small",
        max_tokens: int = 4000,
        temperature: float = 0.1,
        vector_storage: Optional[VectorStorage] = None,
        entity_extractor: Optional[LLMEntityExtractor] = None,
        relation_extractor: Optional[LLMRelationExtractor] = None,
        max_concurrent: int = 10,
    ):
        """åˆå§‹åŒ–å…¨åŠŸèƒ½LLMå›¾æ„å»ºå™¨"""
        super().__init__()

        # ä½¿ç”¨ç»„åˆè€Œä¸æ˜¯ç»§æ‰¿æ¥å¤ç”¨FlexibleLLMGraphBuilderçš„åŠŸèƒ½
        self.flexible_builder = FlexibleLLMGraphBuilder(
            openai_api_key=openai_api_key,
            openai_api_base=openai_api_base,
            llm_model=llm_model,
            embedding_model=embedding_model,
            max_tokens=max_tokens,
            temperature=temperature,
            vector_storage=vector_storage,
            entity_extractor=entity_extractor,
            relation_extractor=relation_extractor,
            max_concurrent=max_concurrent,
        )

    async def build_graph(
        self,
        texts: Optional[List[str]] = None,
        database_schema: Optional[Dict[str, Any]] = None,
        graph_name: str = "comprehensive_llm_graph",
    ) -> KnowledgeGraph:
        """æ„å»ºå…¨åŠŸèƒ½å›¾è°±"""
        # å§”æ‰˜ç»™çµæ´»æ„å»ºå™¨
        graph = await self.flexible_builder.build_graph(texts, database_schema, graph_name)

        # æ‰§è¡ŒéªŒè¯
        validation_result = await self.validate_graph(graph)
        if not validation_result.get("valid", True):
            logger.warning(f"Graph validation issues: {validation_result.get('issues', [])}")

        return graph

    async def update_graph(
        self,
        graph: KnowledgeGraph,
        new_entities: Optional[List[Entity]] = None,
        new_relations: Optional[List[Relation]] = None,
    ) -> KnowledgeGraph:
        """æ›´æ–°å›¾è°±å¹¶éªŒè¯"""
        # å§”æ‰˜ç»™çµæ´»æ„å»ºå™¨
        updated_graph = await self.flexible_builder.update_graph(graph, new_entities, new_relations)

        # æ‰§è¡ŒéªŒè¯
        validation_result = await self.validate_graph(updated_graph)
        if not validation_result.get("valid", True):
            logger.warning(f"Graph validation issues: {validation_result.get('issues', [])}")

        return updated_graph

    def get_usage_statistics(self) -> Dict[str, Any]:
        """è·å–ä½¿ç”¨ç»Ÿè®¡ä¿¡æ¯"""
        return self.flexible_builder.get_usage_statistics()

    def print_usage_summary(self) -> None:
        """æ‰“å°ä½¿ç”¨ç»Ÿè®¡æ‘˜è¦"""
        self.flexible_builder.print_usage_summary()

    def cleanup(self) -> None:
        """æ¸…ç†èµ„æº"""
        self.flexible_builder.cleanup()


class StreamingLLMGraphBuilder(StreamingGraphBuilder, IncrementalBuilderMixin, GraphStatisticsMixin):
    """
    æµå¼LLMå›¾æ„å»ºå™¨ - é€‚ç”¨äºå®æ—¶å¢é‡æ›´æ–°

    ä¸“ä¸ºéœ€è¦å®æ—¶å¤„ç†æ–‡æ¡£æµè€Œä¸éœ€è¦åˆå¹¶æˆ–éªŒè¯åŠŸèƒ½çš„åº”ç”¨è®¾è®¡ã€‚
    """

    def __init__(
        self,
        openai_api_key: str,
        openai_api_base: Optional[str] = None,
        llm_model: str = "gpt-4o-mini",
        max_tokens: int = 4000,
        temperature: float = 0.1,
        entity_extractor: Optional[LLMEntityExtractor] = None,
        relation_extractor: Optional[LLMRelationExtractor] = None,
        max_concurrent: int = 5,  # æµå¼å¤„ç†ä½¿ç”¨è¾ƒå°çš„å¹¶å‘æ•°
    ):
        """åˆå§‹åŒ–æµå¼LLMå›¾æ„å»ºå™¨"""
        super().__init__()

        self.entity_extractor = entity_extractor or LLMEntityExtractor(
            openai_api_key=openai_api_key,
            openai_api_base=openai_api_base,
            llm_model=llm_model,
            max_tokens=max_tokens,
            temperature=temperature,
        )
        self.relation_extractor = relation_extractor or LLMRelationExtractor(
            openai_api_key=openai_api_key,
            openai_api_base=openai_api_base,
            llm_model=llm_model,
            max_tokens=max_tokens,
            temperature=temperature,
        )

        self.usage_tracker = LLMUsageTracker(llm_model, "")
        self.async_processor = LLMAsyncProcessor(
            self.entity_extractor,
            self.relation_extractor,
            self.usage_tracker,
            max_concurrent,
        )

    async def build_graph(
        self,
        texts: Optional[List[str]] = None,
        database_schema: Optional[Dict[str, Any]] = None,
        graph_name: str = "streaming_llm_graph",
    ) -> KnowledgeGraph:
        """æ„å»ºåˆå§‹å›¾è°±ç”¨äºæµå¼å¤„ç†"""
        if not texts:
            graph = KnowledgeGraph(name=graph_name)
        else:
            # ä½¿ç”¨æœ€å°æ„å»ºå™¨åˆ›å»ºåˆå§‹å›¾è°±
            minimal_builder = MinimalLLMGraphBuilder(
                openai_api_key=self.entity_extractor.openai_client.api_key,
                openai_api_base=(
                    str(self.entity_extractor.openai_client.base_url)
                    if self.entity_extractor.openai_client.base_url
                    else None
                ),
                llm_model=self.entity_extractor.llm_model,
                entity_extractor=self.entity_extractor,
                relation_extractor=self.relation_extractor,
            )
            graph = await minimal_builder.build_graph(texts, database_schema, graph_name)

        self._current_graph = graph
        return graph

    async def add_documents_async(
        self, documents: List[str], document_ids: Optional[List[str]] = None
    ) -> KnowledgeGraph:
        """å¼‚æ­¥æ·»åŠ æ–‡æ¡£åˆ°æµå¼å›¾è°±"""
        if document_ids is None:
            document_ids = [f"doc_{i}_{datetime.now().timestamp()}" for i in range(len(documents))]

        if len(document_ids) != len(documents):
            raise ValueError("Number of document IDs must match number of documents")

        logger.info(f"Adding {len(documents)} documents to streaming graph")

        try:
            # å¤„ç†æ–°æ–‡æ¡£
            all_entities, all_relations = await self.async_processor.process_texts_async(documents)

            # ç®€å•å»é‡ï¼ˆæµå¼å¤„ç†é¿å…å¤æ‚å»é‡ï¼‰
            simple_dedup_entities = self._simple_entity_dedup(all_entities)

            # ç¡®ä¿å½“å‰å›¾è°±å­˜åœ¨
            if self._current_graph is None:
                raise ValueError("Current graph is not initialized. Call build_initial_graph first.")

            # æ·»åŠ åˆ°å½“å‰å›¾è°±
            entity_mapping = {entity.name: entity.id for entity in self._current_graph.entities.values()}

            # æ„å»ºæ–°å®ä½“
            new_entities = []
            for entity_data in simple_dedup_entities:
                if entity_data["name"] not in entity_mapping:
                    new_entities.append(entity_data)

            new_entity_mapping = LLMGraphUtils.build_entities_from_data(self._current_graph, new_entities)
            entity_mapping.update(new_entity_mapping)

            # æ„å»ºæ–°å…³ç³»
            relations_added = LLMGraphUtils.build_relations_from_data(
                self._current_graph, all_relations, entity_mapping
            )

            # è®°å½•æ–‡æ¡£æ˜ å°„
            for doc_id in document_ids:
                self._document_registry[doc_id] = list(new_entity_mapping.values())

            logger.info(f"Added {len(new_entities)} entities and {relations_added} relations from streaming documents")
            return self._current_graph

        except Exception as e:
            logger.error(f"Error adding documents to streaming graph: {e}")
            raise

    def _simple_entity_dedup(self, entities: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """ç®€å•çš„å®ä½“å»é‡ - åŸºäºåç§°å’Œç±»å‹"""
        seen = set()
        deduplicated = []

        for entity_data in entities:
            key = (entity_data["name"].lower(), entity_data["type"])
            if key not in seen:
                seen.add(key)
                deduplicated.append(entity_data)

        return deduplicated

    async def add_documents(self, documents: List[str], document_ids: Optional[List[str]] = None) -> KnowledgeGraph:
        """å¼‚æ­¥ç‰ˆæœ¬çš„æ·»åŠ æ–‡æ¡£æ–¹æ³• - å®ç°IncrementalBuilderæ¥å£"""
        return await self.add_documents_async(documents, document_ids)

    async def remove_documents(self, document_ids: List[str]) -> KnowledgeGraph:
        """ç§»é™¤æ–‡æ¡£æ–¹æ³• - å®ç°IncrementalBuilderæ¥å£"""
        if self._current_graph is None:
            raise ValueError("No graph exists to remove documents from")

        try:
            entities_to_remove = set()

            # æ”¶é›†è¦ç§»é™¤çš„æ–‡æ¡£çš„å®ä½“
            for doc_id in document_ids:
                if doc_id in self._document_registry:
                    entities_to_remove.update(self._document_registry[doc_id])
                    del self._document_registry[doc_id]

            # ç§»é™¤å®ä½“ï¼ˆè¿™ä¹Ÿä¼šç§»é™¤ç›¸å…³å…³ç³»ï¼‰
            for entity_id in entities_to_remove:
                if entity_id in self._current_graph.entities:
                    self._current_graph.remove_entity(entity_id)

            return self._current_graph

        except Exception as e:
            logger.error(f"Error removing documents: {e}")
            raise

    def get_usage_statistics(self) -> Dict[str, Any]:
        """è·å–ä½¿ç”¨ç»Ÿè®¡ä¿¡æ¯"""
        return self.usage_tracker.get_usage_statistics()


class BatchLLMGraphBuilder(GraphMergerMixin, BatchGraphBuilder):
    """
    æ‰¹é‡LLMå›¾æ„å»ºå™¨ - ä¼˜åŒ–æ‰¹é‡å¤„ç†å¤šä¸ªæ•°æ®æº

    é€‚ç”¨äºéœ€è¦å¤„ç†å¤šä¸ªæ•°æ®æºå¹¶åˆå¹¶å®ƒä»¬ï¼Œä½†ä¸éœ€è¦å¢é‡æ›´æ–°æˆ–éªŒè¯çš„åœºæ™¯ã€‚
    """

    def __init__(
        self,
        openai_api_key: str,
        openai_api_base: Optional[str] = None,
        llm_model: str = "gpt-4o-mini",
        embedding_model: str = "text-embedding-3-small",
        max_tokens: int = 4000,
        temperature: float = 0.1,
        vector_storage: Optional[VectorStorage] = None,
        entity_extractor: Optional[LLMEntityExtractor] = None,
        relation_extractor: Optional[LLMRelationExtractor] = None,
        max_concurrent: int = 15,  # æ‰¹é‡å¤„ç†ä½¿ç”¨æ›´é«˜çš„å¹¶å‘æ•°
    ):
        """åˆå§‹åŒ–æ‰¹é‡LLMå›¾æ„å»ºå™¨"""
        super().__init__()

        # ä½¿ç”¨ç»„åˆå¤ç”¨FlexibleLLMGraphBuilder
        self.flexible_builder = FlexibleLLMGraphBuilder(
            openai_api_key=openai_api_key,
            openai_api_base=openai_api_base,
            llm_model=llm_model,
            embedding_model=embedding_model,
            max_tokens=max_tokens,
            temperature=temperature,
            vector_storage=vector_storage,
            entity_extractor=entity_extractor,
            relation_extractor=relation_extractor,
            max_concurrent=max_concurrent,
        )

    async def build_graph(
        self,
        texts: Optional[List[str]] = None,
        database_schema: Optional[Dict[str, Any]] = None,
        graph_name: str = "batch_llm_graph",
    ) -> KnowledgeGraph:
        """æ„å»ºæ‰¹é‡å›¾è°±"""
        return await self.flexible_builder.build_graph(texts, database_schema, graph_name)

    async def build_from_multiple_sources(
        self, sources: List[Dict[str, Any]], graph_name: str = "multi_source_batch_llm_graph"
    ) -> KnowledgeGraph:
        """ä»å¤šä¸ªå¼‚æ„æ•°æ®æºæ„å»ºå›¾è°±"""
        sub_graphs = []

        # å¹¶è¡Œå¤„ç†å¤šä¸ªæ•°æ®æº
        tasks = []
        for i, source in enumerate(sources):
            source_type = source.get("type")
            source_data = source.get("data")
            source_name = f"{graph_name}_source_{i}"

            if source_type == "text":
                texts = source_data if isinstance(source_data, list) else [source_data]
                task = self.flexible_builder.build_graph(texts=texts, graph_name=source_name)
                tasks.append(task)
            elif source_type == "mixed":
                if source_data is not None:
                    texts = source_data.get("texts", [])
                else:
                    texts = []
                task = self.flexible_builder.build_graph(texts=texts, graph_name=source_name)
                tasks.append(task)
            else:
                logger.warning(f"Unknown source type: {source_type}")

        if not tasks:
            return KnowledgeGraph(name=graph_name)

        # å¹¶è¡Œæ‰§è¡Œæ‰€æœ‰æ„å»ºä»»åŠ¡
        logger.info(f"Building {len(tasks)} sub-graphs in parallel")
        sub_graphs = await asyncio.gather(*tasks, return_exceptions=True)

        # è¿‡æ»¤å¼‚å¸¸ç»“æœ
        valid_graphs: List[KnowledgeGraph] = []
        for result in sub_graphs:
            if isinstance(result, Exception):
                logger.error(f"Error building sub-graph: {result}")
            elif isinstance(result, KnowledgeGraph):
                valid_graphs.append(result)

        if not valid_graphs:
            return KnowledgeGraph(name=graph_name)

        # åˆå¹¶æ‰€æœ‰å­å›¾
        logger.info(f"Merging {len(valid_graphs)} sub-graphs")
        merged_graph = await self.merge_graphs(valid_graphs)
        merged_graph.name = graph_name
        return merged_graph

    def get_usage_statistics(self) -> Dict[str, Any]:
        """è·å–ä½¿ç”¨ç»Ÿè®¡ä¿¡æ¯"""
        return self.flexible_builder.get_usage_statistics()

    def cleanup(self) -> None:
        """æ¸…ç†èµ„æº"""
        self.flexible_builder.cleanup()


class LLMSearchBuilder(GraphExporterMixin):
    """
    LLMæœç´¢æ„å»ºå™¨ - ä¸“é—¨ç”¨äºæœç´¢å’Œé—®ç­”åŠŸèƒ½

    éµå¾ªISPï¼šåªå®ç°æœç´¢ã€æ£€ç´¢å’Œå¯¼å‡ºç›¸å…³çš„æ¥å£ï¼Œä¸åŒ…å«æ„å»ºåŠŸèƒ½ã€‚
    åŸºäºå‘é‡ç›¸ä¼¼åº¦å’ŒLLMæ¨ç†æä¾›æ™ºèƒ½æœç´¢å’Œé—®ç­”èƒ½åŠ›ã€‚
    """

    def __init__(
        self,
        graph: KnowledgeGraph,
        openai_api_key: str,
        openai_api_base: Optional[str] = None,
        llm_model: str = "gpt-4o-mini",
        embedding_model: str = "text-embedding-3-small",
        max_tokens: int = 4000,
        temperature: float = 0.1,
        vector_storage: Optional[VectorStorage] = None,
    ):
        """
        åˆå§‹åŒ–LLMæœç´¢æ„å»ºå™¨

        Args:
            graph: è¦æœç´¢çš„çŸ¥è¯†å›¾è°±
            openai_api_key: OpenAI APIå¯†é’¥
            openai_api_base: OpenAI APIåŸºç¡€URL
            llm_model: ä½¿ç”¨çš„LLMæ¨¡å‹
            embedding_model: ä½¿ç”¨çš„åµŒå…¥æ¨¡å‹
            max_tokens: æœ€å¤§tokenæ•°
            temperature: æ¸©åº¦å‚æ•°
            vector_storage: å‘é‡å­˜å‚¨åç«¯
        """
        super().__init__()
        self.graph = graph
        self.openai_api_key = openai_api_key
        self.openai_api_base = openai_api_base
        self.llm_model = llm_model
        self.max_tokens = max_tokens
        self.temperature = temperature

        # åˆå§‹åŒ–å‘é‡å­˜å‚¨å’ŒåµŒå…¥
        self.vector_storage = vector_storage or JsonVectorStorage("llm_search_vectors.json")
        self.graph_embedding = OpenAIEmbedding(
            vector_storage=self.vector_storage,
            openai_api_key=openai_api_key,
            openai_api_base=openai_api_base,
            embedding_model=embedding_model,
        )

        # åˆå§‹åŒ–LLMå®¢æˆ·ç«¯
        try:
            import openai

            self.llm_client = openai.OpenAI(
                api_key=openai_api_key,
                base_url=openai_api_base,
            )
        except ImportError:
            logger.error("OpenAI library not installed. Please install with: pip install openai")
            raise

    async def search_entities(
        self, query: str, top_k: int = 10, similarity_threshold: float = 0.7
    ) -> List[Dict[str, Any]]:
        """
        åŸºäºæŸ¥è¯¢æœç´¢ç›¸å…³å®ä½“

        Args:
            query: æœç´¢æŸ¥è¯¢
            top_k: è¿”å›çš„æœ€å¤§ç»“æœæ•°
            similarity_threshold: ç›¸ä¼¼åº¦é˜ˆå€¼

        Returns:
            List[Dict[str, Any]]: æœç´¢ç»“æœåˆ—è¡¨
        """
        try:
            logger.info(f"Searching entities for query: {query}")

            # ç¡®ä¿å›¾è°±åµŒå…¥å·²æ„å»º
            await self._ensure_embeddings_built()

            # è·å–æŸ¥è¯¢æ–‡æœ¬çš„åµŒå…¥å‘é‡
            query_embedding = await self.graph_embedding.embed_text(query)
            if query_embedding is None:
                logger.warning("Failed to get query embedding")
                return []

            # æœç´¢ç›¸ä¼¼çš„å®ä½“å‘é‡
            similar_vectors = self.vector_storage.search_similar_vectors(
                query_vector=query_embedding, top_k=top_k * 2, threshold=similarity_threshold  # æœç´¢æ›´å¤šç»“æœï¼Œç„¶åè¿‡æ»¤
            )

            # æ ¼å¼åŒ–ç»“æœ - åªä¿ç•™å®ä½“å‘é‡
            results = []
            for vector_id, similarity in similar_vectors:
                if vector_id.startswith("entity_"):
                    # æå–å®ä½“ID (å»æ‰"entity_"å‰ç¼€)
                    entity_id = vector_id[7:]
                    entity = self.graph.get_entity(entity_id)

                    if entity:
                        result = {
                            "entity_id": entity.id,
                            "entity_name": entity.name,
                            "entity_type": entity.entity_type.value,
                            "description": entity.description,
                            "similarity_score": similarity,
                            "properties": entity.properties,
                            "aliases": entity.aliases,
                            "source": entity.source,
                        }
                        results.append(result)

                        if len(results) >= top_k:
                            break

            logger.info(f"Found {len(results)} relevant entities")
            return results

        except Exception as e:
            logger.error(f"Error searching entities: {e}")
            raise

    async def search_relations(
        self, query: str, top_k: int = 10, similarity_threshold: float = 0.7
    ) -> List[Dict[str, Any]]:
        """
        åŸºäºæŸ¥è¯¢æœç´¢ç›¸å…³å…³ç³»

        Args:
            query: æœç´¢æŸ¥è¯¢
            top_k: è¿”å›çš„æœ€å¤§ç»“æœæ•°
            similarity_threshold: ç›¸ä¼¼åº¦é˜ˆå€¼

        Returns:
            List[Dict[str, Any]]: æœç´¢ç»“æœåˆ—è¡¨
        """
        try:
            logger.info(f"Searching relations for query: {query}")

            # ç¡®ä¿å›¾è°±åµŒå…¥å·²æ„å»º
            await self._ensure_embeddings_built()

            # è·å–æŸ¥è¯¢æ–‡æœ¬çš„åµŒå…¥å‘é‡
            query_embedding = await self.graph_embedding.embed_text(query)
            if query_embedding is None:
                logger.warning("Failed to get query embedding")
                return []

            # æœç´¢ç›¸ä¼¼çš„å…³ç³»å‘é‡
            similar_vectors = self.vector_storage.search_similar_vectors(
                query_vector=query_embedding, top_k=top_k * 2, threshold=similarity_threshold  # æœç´¢æ›´å¤šç»“æœï¼Œç„¶åè¿‡æ»¤
            )

            # æ ¼å¼åŒ–ç»“æœ - åªä¿ç•™å…³ç³»å‘é‡
            results = []
            for vector_id, similarity in similar_vectors:
                if vector_id.startswith("relation_"):
                    # æå–å…³ç³»ID (å»æ‰"relation_"å‰ç¼€)
                    relation_id = vector_id[9:]
                    relation = self.graph.get_relation(relation_id)

                    if relation:
                        result = {
                            "relation_id": relation.id,
                            "head_entity": {
                                "id": relation.head_entity.id if relation.head_entity else None,
                                "name": relation.head_entity.name if relation.head_entity else None,
                                "type": relation.head_entity.entity_type.value if relation.head_entity else None,
                            },
                            "tail_entity": {
                                "id": relation.tail_entity.id if relation.tail_entity else None,
                                "name": relation.tail_entity.name if relation.tail_entity else None,
                                "type": relation.tail_entity.entity_type.value if relation.tail_entity else None,
                            },
                            "relation_type": relation.relation_type.value,
                            "description": relation.description,
                            "similarity_score": similarity,
                            "confidence": relation.confidence,
                            "properties": relation.properties,
                            "source": relation.source,
                        }
                        results.append(result)

                        if len(results) >= top_k:
                            break

            logger.info(f"Found {len(results)} relevant relations")
            return results

        except Exception as e:
            logger.error(f"Error searching relations: {e}")
            raise

    async def search_graph(
        self, query: str, search_type: str = "hybrid", top_k: int = 10, similarity_threshold: float = 0.7
    ) -> Dict[str, Any]:
        """
        ç»¼åˆæœç´¢å›¾è°±ï¼ˆå®ä½“å’Œå…³ç³»ï¼‰

        Args:
            query: æœç´¢æŸ¥è¯¢
            search_type: æœç´¢ç±»å‹ ("entities", "relations", "hybrid")
            top_k: è¿”å›çš„æœ€å¤§ç»“æœæ•°
            similarity_threshold: ç›¸ä¼¼åº¦é˜ˆå€¼

        Returns:
            Dict[str, Any]: ç»¼åˆæœç´¢ç»“æœ
        """
        try:
            logger.info(f"Searching graph with query: {query}, type: {search_type}")

            results: Dict[str, Any] = {
                "query": query,
                "search_type": search_type,
                "timestamp": datetime.now().isoformat(),
                "entities": [],
                "relations": [],
            }

            if search_type in ["entities", "hybrid"]:
                results["entities"] = await self.search_entities(query, top_k, similarity_threshold)

            if search_type in ["relations", "hybrid"]:
                results["relations"] = await self.search_relations(query, top_k, similarity_threshold)

            return results

        except Exception as e:
            logger.error(f"Error searching graph: {e}")
            raise

    async def answer_question(
        self, question: str, context_entities: int = 5, context_relations: int = 5, include_reasoning: bool = True
    ) -> Dict[str, Any]:
        """
        åŸºäºçŸ¥è¯†å›¾è°±å›ç­”é—®é¢˜

        Args:
            question: è¦å›ç­”çš„é—®é¢˜
            context_entities: ä¸Šä¸‹æ–‡å®ä½“æ•°é‡
            context_relations: ä¸Šä¸‹æ–‡å…³ç³»æ•°é‡
            include_reasoning: æ˜¯å¦åŒ…å«æ¨ç†è¿‡ç¨‹

        Returns:
            Dict[str, Any]: é—®ç­”ç»“æœ
        """
        try:
            logger.info(f"Answering question: {question}")

            # 1. æœç´¢ç›¸å…³ä¸Šä¸‹æ–‡
            search_results = await self.search_graph(
                query=question,
                search_type="hybrid",
                top_k=max(context_entities, context_relations),
                similarity_threshold=0.6,
            )

            # 2. æ„å»ºä¸Šä¸‹æ–‡ä¿¡æ¯
            context_info = self._build_context_for_qa(search_results, context_entities, context_relations)

            # 3. ç”Ÿæˆå›ç­”
            answer_result = await self._generate_answer_with_llm(
                question=question, context=context_info, include_reasoning=include_reasoning
            )

            # 4. æ•´åˆç»“æœ
            result = {
                "question": question,
                "answer": answer_result["answer"],
                "confidence": answer_result.get("confidence", 0.0),
                "context": {
                    "entities_used": len(search_results["entities"][:context_entities]),
                    "relations_used": len(search_results["relations"][:context_relations]),
                    "search_results": search_results if include_reasoning else None,
                },
                "timestamp": datetime.now().isoformat(),
            }

            if include_reasoning:
                result["reasoning"] = answer_result.get("reasoning", "")

            logger.info("Question answered successfully")
            return result

        except Exception as e:
            logger.error(f"Error answering question: {e}")
            raise

    async def _ensure_embeddings_built(self) -> None:
        """ç¡®ä¿å›¾è°±åµŒå…¥å·²æ„å»º"""
        try:
            # æ£€æŸ¥å‘é‡å­˜å‚¨ä¸­æ˜¯å¦å·²æœ‰åµŒå…¥
            vectors, _ = self.vector_storage.load_vectors()
            if not vectors:
                logger.info("Building embeddings for search functionality")
                await self.graph_embedding.build_text_embeddings(self.graph)
                self.graph_embedding.save_embeddings()
            else:
                logger.info(f"Found {len(vectors)} existing embeddings")
        except Exception as e:
            logger.warning(f"Error building embeddings: {e}")
            # ç»§ç»­æ‰§è¡Œï¼Œä½¿ç”¨åŸºäºæ–‡æœ¬çš„æœç´¢

    def _build_context_for_qa(self, search_results: Dict[str, Any], max_entities: int, max_relations: int) -> str:
        """æ„å»ºé—®ç­”çš„ä¸Šä¸‹æ–‡ä¿¡æ¯"""
        context_parts = []

        # æ·»åŠ å®ä½“ä¿¡æ¯
        entities = search_results.get("entities", [])[:max_entities]
        if entities:
            context_parts.append("ç›¸å…³å®ä½“ä¿¡æ¯:")
            for entity in entities:
                entity_info = f"- {entity['entity_name']} ({entity['entity_type']}): {entity['description']}"
                if entity.get("aliases"):
                    entity_info += f" [åˆ«å: {', '.join(entity['aliases'])}]"
                context_parts.append(entity_info)

        # æ·»åŠ å…³ç³»ä¿¡æ¯
        relations = search_results.get("relations", [])[:max_relations]
        if relations:
            context_parts.append("\nç›¸å…³å…³ç³»ä¿¡æ¯:")
            for relation in relations:
                head_name = relation["head_entity"]["name"] if relation["head_entity"] else "Unknown"
                tail_name = relation["tail_entity"]["name"] if relation["tail_entity"] else "Unknown"
                relation_info = f"- {head_name} --({relation['relation_type']})--> {tail_name}"
                if relation["description"]:
                    relation_info += f": {relation['description']}"
                context_parts.append(relation_info)

        return "\n".join(context_parts)

    async def _generate_answer_with_llm(self, question: str, context: str, include_reasoning: bool) -> Dict[str, Any]:
        """ä½¿ç”¨LLMç”Ÿæˆç­”æ¡ˆ"""
        try:
            # æ„å»ºç³»ç»Ÿæç¤º
            system_prompt = """ä½ æ˜¯ä¸€ä¸ªçŸ¥è¯†å›¾è°±é—®ç­”åŠ©æ‰‹ã€‚åŸºäºæä¾›çš„çŸ¥è¯†å›¾è°±ä¿¡æ¯å›ç­”ç”¨æˆ·é—®é¢˜ã€‚

è¦æ±‚ï¼š
1. åŸºäºæä¾›çš„ä¸Šä¸‹æ–‡ä¿¡æ¯å›ç­”é—®é¢˜
2. å¦‚æœä¿¡æ¯ä¸è¶³ï¼Œæ˜ç¡®è¯´æ˜æ— æ³•å›ç­”æˆ–ä¿¡æ¯ä¸å®Œæ•´
3. ä¿æŒå›ç­”å‡†ç¡®ã€ç®€æ´ä¸”æœ‰æ¡ç†
4. ä¼˜å…ˆä½¿ç”¨ä¸Šä¸‹æ–‡ä¸­çš„å…·ä½“å®ä½“å’Œå…³ç³»ä¿¡æ¯"""

            if include_reasoning:
                system_prompt += "\n5. åœ¨å›ç­”åç®€è¦è¯´æ˜ä½ çš„æ¨ç†è¿‡ç¨‹"

            # æ„å»ºç”¨æˆ·æ¶ˆæ¯
            user_message = f"""é—®é¢˜: {question}

çŸ¥è¯†å›¾è°±ä¸Šä¸‹æ–‡ä¿¡æ¯:
{context}

è¯·åŸºäºä¸Šè¿°ä¿¡æ¯å›ç­”é—®é¢˜ã€‚"""

            # è°ƒç”¨LLM
            messages: List[ChatCompletionMessageParam] = [
                ChatCompletionSystemMessageParam(role="system", content=system_prompt),
                ChatCompletionUserMessageParam(role="user", content=user_message),
            ]

            response = self.llm_client.chat.completions.create(
                model=self.llm_model,
                messages=messages,
                max_tokens=self.max_tokens,
                temperature=self.temperature,
            )

            answer_text = response.choices[0].message.content
            if answer_text is None:
                answer_text = "No response generated"
            else:
                answer_text = answer_text.strip()

            # è§£æå›ç­”å’Œæ¨ç†ï¼ˆå¦‚æœéœ€è¦ï¼‰
            result = {"answer": answer_text}

            if include_reasoning and "æ¨ç†è¿‡ç¨‹" in answer_text:
                # å°è¯•åˆ†ç¦»å›ç­”å’Œæ¨ç†
                parts = answer_text.split("æ¨ç†è¿‡ç¨‹")
                if len(parts) == 2:
                    result["answer"] = parts[0].strip()
                    result["reasoning"] = ("æ¨ç†è¿‡ç¨‹" + parts[1]).strip()

            # ä¼°ç®—ç½®ä¿¡åº¦ï¼ˆåŸºäºä¸Šä¸‹æ–‡åŒ¹é…ç¨‹åº¦ï¼‰
            confidence = self._estimate_answer_confidence(question, context, answer_text)
            result["confidence"] = str(confidence)

            return result

        except Exception as e:
            logger.error(f"Error generating answer with LLM: {e}")
            return {
                "answer": "æŠ±æ­‰ï¼Œåœ¨ç”Ÿæˆå›ç­”æ—¶é‡åˆ°äº†é”™è¯¯ã€‚",
                "confidence": 0.0,
                "reasoning": f"é”™è¯¯ä¿¡æ¯: {str(e)}" if include_reasoning else None,
            }

    def _estimate_answer_confidence(self, question: str, context: str, answer: str) -> float:
        """ä¼°ç®—ç­”æ¡ˆçš„ç½®ä¿¡åº¦"""
        try:
            # ç®€å•çš„ç½®ä¿¡åº¦ä¼°ç®—ï¼šåŸºäºä¸Šä¸‹æ–‡é•¿åº¦å’Œç­”æ¡ˆä¸­æ˜¯å¦æåŠä¸Šä¸‹æ–‡å®ä½“
            if not context.strip():
                return 0.1  # æ²¡æœ‰ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œç½®ä¿¡åº¦å¾ˆä½

            # æ£€æŸ¥ç­”æ¡ˆä¸­æ˜¯å¦åŒ…å«ä¸Šä¸‹æ–‡ä¿¡æ¯
            answer_lower = answer.lower()

            # è®¡ç®—ä¸Šä¸‹æ–‡å®ä½“åœ¨ç­”æ¡ˆä¸­çš„å‡ºç°æ¯”ä¾‹
            context_entities = []
            for line in context.split("\n"):
                if line.strip().startswith("- "):
                    # æå–å®ä½“åç§°
                    entity_part = line.strip()[2:].split("(")[0].strip()
                    if entity_part:
                        context_entities.append(entity_part.lower())

            if not context_entities:
                return 0.3

            mentioned_entities = sum(1 for entity in context_entities if entity in answer_lower)
            mention_ratio = mentioned_entities / len(context_entities)

            # åŸºäºæåŠæ¯”ä¾‹å’Œç­”æ¡ˆé•¿åº¦è®¡ç®—ç½®ä¿¡åº¦
            base_confidence = 0.3 + (mention_ratio * 0.5)

            # å¦‚æœç­”æ¡ˆæ˜ç¡®è¡¨ç¤ºæ— æ³•å›ç­”ï¼Œé™ä½ç½®ä¿¡åº¦
            uncertainty_phrases = ["æ— æ³•å›ç­”", "ä¿¡æ¯ä¸è¶³", "ä¸çŸ¥é“", "ä¸ç¡®å®š", "æŠ±æ­‰"]
            if any(phrase in answer_lower for phrase in uncertainty_phrases):
                base_confidence *= 0.5

            return min(max(base_confidence, 0.0), 1.0)

        except Exception:
            return 0.5  # é»˜è®¤ä¸­ç­‰ç½®ä¿¡åº¦

    async def export_to_format(self, graph: KnowledgeGraph, format_type: str) -> Dict[str, Any]:
        """å¯¼å‡ºå›¾è°±åˆ°æŒ‡å®šæ ¼å¼"""
        try:
            format_lower = format_type.lower()

            if format_lower == "json":
                return graph.to_dict()
            elif format_lower == "summary":
                # ç”Ÿæˆå›¾è°±æ‘˜è¦
                return {
                    "graph_name": graph.name,
                    "entities_count": len(graph.entities),
                    "relations_count": len(graph.relations),
                    "entity_types": list(set(e.entity_type.value for e in graph.entities.values())),
                    "relation_types": list(set(r.relation_type.value for r in graph.relations.values())),
                    "export_timestamp": datetime.now().isoformat(),
                }
            else:
                raise ValueError(f"Unsupported export format: {format_type}")

        except Exception as e:
            logger.error(f"Error exporting graph: {e}")
            raise

    def get_statistics(self) -> Dict[str, Any]:
        """è·å–æœç´¢ç»Ÿè®¡ä¿¡æ¯"""
        try:
            return {
                "graph_name": self.graph.name,
                "entities_count": len(self.graph.entities),
                "relations_count": len(self.graph.relations),
                "has_embeddings": bool(self.vector_storage.load_vectors()[0]),
                "llm_model": self.llm_model,
                "search_capabilities": ["entity_search", "relation_search", "question_answering"],
                "last_updated": datetime.now().isoformat(),
            }
        except Exception as e:
            logger.error(f"Error getting statistics: {e}")
            return {"error": str(e)}

    def cleanup(self) -> None:
        """æ¸…ç†èµ„æº"""
        try:
            if self.graph_embedding:
                self.graph_embedding.save_embeddings()
            logger.info("LLM search builder resources cleaned up")
        except Exception as e:
            logger.error(f"Error during cleanup: {e}")


# å¯¼å‡ºæ‰€æœ‰å…¬å…±ç±»å’Œå‡½æ•°
__all__ = [
    "LLMUsageTracker",
    "LLMAsyncProcessor",
    "LLMGraphUtils",
    "MinimalLLMGraphBuilder",
    "FlexibleLLMGraphBuilder",
    "LLMGraphBuilder",
    "StreamingLLMGraphBuilder",
    "BatchLLMGraphBuilder",
    "LLMSearchBuilder",
]
