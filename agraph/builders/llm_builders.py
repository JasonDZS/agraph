"""
åŸºäºInterface Segregation Principleçš„LLMçŸ¥è¯†å›¾è°±æ„å»ºå™¨

éµå¾ªISPåŸåˆ™ï¼Œå°†LLMæ„å»ºå™¨åˆ†è§£ä¸ºå¤šä¸ªä¸“é—¨çš„æ¥å£å’Œå®ç°ï¼Œ
å®¢æˆ·ç«¯åªéœ€è¦ä¾èµ–ä»–ä»¬å®é™…ä½¿ç”¨çš„åŠŸèƒ½ã€‚
"""

import asyncio
import json
import os.path
from datetime import datetime
from typing import Any, Dict, List, Optional, Tuple

from ..chunker import TokenChunker
from ..config import settings
from ..embeddings import OpenAIEmbedding
from ..entities import Entity
from ..extractors.llm_entity_extractor import LLMEntityExtractor
from ..extractors.llm_relation_extractor import LLMRelationExtractor
from ..graph import KnowledgeGraph
from ..logger import logger
from ..relations import Relation
from ..storage import JsonVectorStorage, VectorStorage
from ..text import TextChunk
from ..types import EntityType, RelationType
from ..utils import get_type_value
from .interfaces import (
    BasicGraphBuilder,
    BatchGraphBuilder,
    FullFeaturedGraphBuilder,
    StreamingGraphBuilder,
    UpdatableGraphBuilder,
)
from .mixins import (
    GraphExporterMixin,
    GraphMergerMixin,
    GraphStatisticsMixin,
    GraphValidatorMixin,
    IncrementalBuilderMixin,
)


class LLMUsageTracker:
    """LLMä½¿ç”¨ç»Ÿè®¡è·Ÿè¸ªå™¨ - ç‹¬ç«‹çš„å…³æ³¨ç‚¹"""

    def __init__(self, llm_model: str, embedding_model: str):
        self.usage_stats: Dict[str, Any] = {
            "llm_model": {
                "model_name": llm_model,
                "total_calls": 0,
                "entity_extraction_calls": 0,
                "relation_extraction_calls": 0,
                "deduplication_calls": 0,
                "errors": 0,
                "call_history": [],
            },
            "embedding_model": {
                "model_name": embedding_model,
                "total_calls": 0,
                "entity_embedding_calls": 0,
                "relation_embedding_calls": 0,
                "errors": 0,
                "call_history": [],
            },
            "session_info": {
                "start_time": datetime.now(),
                "end_time": None,
                "total_texts_processed": 0,
                "total_entities_extracted": 0,
                "total_relations_extracted": 0,
            },
        }

    def track_llm_call(
        self,
        call_type: str,
        input_msg: str = "",
        output_msg: str = "",
        success: bool = True,
        error_msg: Optional[str] = None,
    ) -> None:
        """è·Ÿè¸ªLLM APIè°ƒç”¨ç»Ÿè®¡ä¿¡æ¯"""
        llm_stats = self.usage_stats["llm_model"]

        if success:
            llm_stats["total_calls"] += 1
            if call_type == "entity_extraction":
                llm_stats["entity_extraction_calls"] += 1
            elif call_type == "relation_extraction":
                llm_stats["relation_extraction_calls"] += 1
            elif call_type == "deduplication":
                llm_stats["deduplication_calls"] += 1
        else:
            llm_stats["errors"] += 1

        # è®°å½•è°ƒç”¨å†å²
        call_record = {
            "timestamp": datetime.now(),
            "call_type": call_type,
            "input_msg": input_msg,
            "output_msg": output_msg,
            "success": success,
            "error_msg": error_msg,
        }
        llm_stats["call_history"].append(call_record)

    def track_embedding_call(self, call_type: str, success: bool = True, error_msg: Optional[str] = None) -> None:
        """è·Ÿè¸ªembedding APIè°ƒç”¨ç»Ÿè®¡ä¿¡æ¯"""
        embedding_stats = self.usage_stats["embedding_model"]

        if success:
            embedding_stats["total_calls"] += 1
            if call_type == "entity_embedding":
                embedding_stats["entity_embedding_calls"] += 1
            elif call_type == "relation_embedding":
                embedding_stats["relation_embedding_calls"] += 1
        else:
            embedding_stats["errors"] += 1

        # è®°å½•è°ƒç”¨å†å²
        call_record = {
            "timestamp": datetime.now(),
            "call_type": call_type,
            "success": success,
            "error_msg": error_msg,
        }
        embedding_stats["call_history"].append(call_record)

    def update_session_stats(self, texts_processed: int, entities_extracted: int, relations_extracted: int) -> None:
        """æ›´æ–°ä¼šè¯ç»Ÿè®¡"""
        session = self.usage_stats["session_info"]
        session["total_texts_processed"] += texts_processed
        session["total_entities_extracted"] += entities_extracted
        session["total_relations_extracted"] += relations_extracted

    def get_usage_statistics(self) -> Dict[str, Any]:
        """è·å–ä½¿ç”¨ç»Ÿè®¡ä¿¡æ¯"""
        return self.usage_stats.copy()

    def export_usage_stats(self, file_path: str) -> None:
        """å¯¼å‡ºä½¿ç”¨ç»Ÿè®¡åˆ°JSONæ–‡ä»¶"""
        stats_copy = json.loads(json.dumps(self.usage_stats, default=str))
        with open(file_path, "w", encoding="utf-8") as f:
            json.dump(stats_copy, f, ensure_ascii=False, indent=2)
        logger.info(f"Usage statistics exported to {file_path}")

    def print_usage_summary(self) -> None:
        """æ‰“å°ä½¿ç”¨ç»Ÿè®¡æ‘˜è¦"""
        stats = self.usage_stats

        print("\n" + "=" * 60)
        print("ğŸ“Š LLM Graph Builder ä½¿ç”¨ç»Ÿè®¡")
        print("=" * 60)

        # ä¼šè¯ä¿¡æ¯
        session = stats["session_info"]
        start_time = session["start_time"]
        end_time = session.get("end_time") or datetime.now()
        duration = end_time - start_time

        print(f"â±ï¸  ä¼šè¯æ—¶é•¿: {duration}")
        print(f"ğŸ“„ å¤„ç†æ–‡æœ¬æ•°: {session['total_texts_processed']}")
        print(f"ğŸ·ï¸  æå–å®ä½“æ•°: {session['total_entities_extracted']}")
        print(f"ğŸ”— æå–å…³ç³»æ•°: {session['total_relations_extracted']}")
        print()

        # LLMç»Ÿè®¡
        llm = stats["llm_model"]
        print(f"ğŸ¤– LLMæ¨¡å‹: {llm['model_name']}")
        print(f"   ğŸ“ æ€»è°ƒç”¨æ¬¡æ•°: {llm['total_calls']}")
        print(f"   âŒ é”™è¯¯æ¬¡æ•°: {llm['errors']}")
        print(f"   â”œâ”€â”€ å®ä½“æå–: {llm['entity_extraction_calls']} æ¬¡")
        print(f"   â”œâ”€â”€ å…³ç³»æå–: {llm['relation_extraction_calls']} æ¬¡")
        print(f"   â””â”€â”€ å®ä½“å»é‡: {llm['deduplication_calls']} æ¬¡")
        print()

        # Embeddingç»Ÿè®¡
        embedding = stats["embedding_model"]
        print(f"ğŸ”¤ Embeddingæ¨¡å‹: {embedding['model_name']}")
        print(f"   ğŸ“ æ€»è°ƒç”¨æ¬¡æ•°: {embedding['total_calls']}")
        print(f"   âŒ é”™è¯¯æ¬¡æ•°: {embedding['errors']}")
        print(f"   â”œâ”€â”€ å®ä½“åµŒå…¥: {embedding['entity_embedding_calls']} æ¬¡")
        print(f"   â””â”€â”€ å…³ç³»åµŒå…¥: {embedding['relation_embedding_calls']} æ¬¡")
        print()

    def cleanup(self) -> None:
        """æ¸…ç†èµ„æº"""
        self.usage_stats["session_info"]["end_time"] = str(datetime.now())


class LLMAsyncProcessor:
    """LLMå¼‚æ­¥å¤„ç†å™¨ - ç‹¬ç«‹çš„å¤„ç†é€»è¾‘"""

    def __init__(
        self,
        entity_extractor: LLMEntityExtractor,
        relation_extractor: LLMRelationExtractor,
        usage_tracker: LLMUsageTracker,
        max_concurrent: int = 10,
    ):
        self.entity_extractor = entity_extractor
        self.relation_extractor = relation_extractor
        self.usage_tracker = usage_tracker
        self.max_concurrent = max_concurrent
        self._deduplication_calls_count = 0

    async def process_texts_async(self, texts: List[str]) -> Tuple[List[Dict], List[Dict]]:
        """å¼‚æ­¥å¤„ç†æ–‡æœ¬åˆ—è¡¨"""
        if len(texts) <= 1:
            return await self._process_texts_sequential(texts)

        all_entities: List[Dict[str, Any]] = []
        all_relations: List[Dict[str, Any]] = []

        # ä½¿ç”¨ä¿¡å·é‡æ§åˆ¶å¹¶å‘æ•°
        semaphore = asyncio.Semaphore(self.max_concurrent)

        async def process_with_semaphore(
            text: str, index: int
        ) -> Tuple[int, List[Dict[str, Any]], List[Dict[str, Any]]]:
            async with semaphore:
                return await self._process_single_text_async(text, index)

        # åˆ›å»ºå¼‚æ­¥ä»»åŠ¡
        tasks = [process_with_semaphore(text, i) for i, text in enumerate(texts)]

        # æ‰¹é‡æ‰§è¡Œ
        logger.info(f"Processing {len(texts)} texts with max_concurrent={self.max_concurrent}")
        results = await asyncio.gather(*tasks, return_exceptions=True)

        # å¤„ç†ç»“æœ
        valid_results = []
        for result in results:
            if isinstance(result, Exception):
                logger.error(f"Error in async execution: {result}")
            else:
                # ç¡®ä¿ result æ˜¯æ­£ç¡®çš„ç±»å‹
                if isinstance(result, tuple) and len(result) == 3:
                    valid_results.append(result)
                else:
                    logger.warning(f"Unexpected result format: {result}")

        # æŒ‰ç´¢å¼•æ’åºå¹¶åˆå¹¶ç»“æœ
        if valid_results:
            valid_results.sort(key=lambda x: x[0])
            for _, entities_data, relations_data in valid_results:
                all_entities.extend(entities_data)
                all_relations.extend(relations_data)

        return all_entities, all_relations

    async def _process_single_text_async(self, text: str, index: int) -> Tuple[int, List[Dict], List[Dict]]:
        """å¼‚æ­¥å¤„ç†å•ä¸ªæ–‡æœ¬"""
        try:
            # å¼‚æ­¥æå–å®ä½“
            entities = await self.entity_extractor._extract_entities_async(text)
            self.usage_tracker.track_llm_call(
                call_type="entity_extraction",
                input_msg=text,
                output_msg=str(entities),
                success=True,
            )

            entities_data = [
                {
                    "name": entity.name,
                    "type": get_type_value(entity.entity_type),
                    "description": entity.description,
                    "aliases": entity.aliases,
                    "properties": entity.properties,
                }
                for entity in entities
            ]

            # å¼‚æ­¥æå–å…³ç³»
            relations = await self.relation_extractor._extract_relations_async(text, entities)
            self.usage_tracker.track_llm_call(call_type="relation_extraction", success=True)

            relations_data = [
                {
                    "head_entity": relation.head_entity.name if relation.head_entity else "",
                    "tail_entity": relation.tail_entity.name if relation.tail_entity else "",
                    "relation_type": get_type_value(relation.relation_type),
                    "description": relation.description,
                    "properties": relation.properties,
                    "confidence": relation.confidence,
                }
                for relation in relations
            ]

            return index, entities_data, relations_data

        except Exception as e:
            logger.error(f"Error processing text {index}: {e}")
            self.usage_tracker.track_llm_call(call_type="entity_extraction", success=False, error_msg=str(e))
            return index, [], []

    async def _process_texts_sequential(self, texts: List[str]) -> Tuple[List[Dict], List[Dict]]:
        """ä¸²è¡Œå¤„ç†æ–‡æœ¬åˆ—è¡¨"""
        all_entities = []
        all_relations = []

        for i, text in enumerate(texts):
            logger.info(f"Processing text {i+1}/{len(texts)}")
            try:
                entities = await self.entity_extractor._extract_entities_async(text)
                self.usage_tracker.track_llm_call(call_type="entity_extraction", success=True)

                entities_data = [
                    {
                        "name": entity.name,
                        "type": get_type_value(entity.entity_type),
                        "description": entity.description,
                        "aliases": entity.aliases,
                        "properties": entity.properties,
                    }
                    for entity in entities
                ]
                all_entities.extend(entities_data)

                relations = await self.relation_extractor._extract_relations_async(text, entities)
                self.usage_tracker.track_llm_call(call_type="relation_extraction", success=True)

                relations_data = [
                    {
                        "head_entity": relation.head_entity.name if relation.head_entity else "",
                        "tail_entity": relation.tail_entity.name if relation.tail_entity else "",
                        "relation_type": get_type_value(relation.relation_type),
                        "description": relation.description,
                        "properties": relation.properties,
                        "confidence": relation.confidence,
                    }
                    for relation in relations
                ]
                all_relations.extend(relations_data)

            except Exception as e:
                logger.error(f"Error processing text {i+1}: {e}")
                self.usage_tracker.track_llm_call(call_type="entity_extraction", success=False, error_msg=str(e))

        return all_entities, all_relations

    async def deduplicate_entities(self, entities: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """ä¼˜åŒ–çš„å®ä½“å»é‡"""
        if len(entities) <= 1:
            return entities

        logger.info(f"Starting optimized deduplication for {len(entities)} entities")

        # Convert dict entities to Entity objects
        entity_objects = []
        for entity_data in entities:
            entity = Entity(
                id=self._generate_entity_id(entity_data["name"]),
                name=entity_data["name"],
                entity_type=EntityType(entity_data["type"]),
                description=entity_data.get("description", ""),
                aliases=entity_data.get("aliases", []),
                properties=entity_data.get("properties", {}),
                created_at=datetime.now(),
                updated_at=datetime.now(),
            )
            entity_objects.append(entity)

        # ä½¿ç”¨ä¼˜åŒ–çš„å¹¶å‘å»é‡æ–¹æ³•
        deduplicated_entities = await self._deduplicate_entities_concurrent(entity_objects)

        # è·Ÿè¸ªå»é‡è°ƒç”¨ç»Ÿè®¡
        if len(entity_objects) > 1:
            actual_calls = self._deduplication_calls_count
            for _ in range(actual_calls):
                self.usage_tracker.track_llm_call(call_type="deduplication", success=True)

        logger.info(f"Deduplication completed: {len(entity_objects)} -> {len(deduplicated_entities)} entities")

        # Convert back to dict format
        return [
            {
                "name": entity.name,
                "type": get_type_value(entity.entity_type),
                "description": entity.description,
                "aliases": entity.aliases,
                "properties": entity.properties,
            }
            for entity in deduplicated_entities
        ]

    async def _deduplicate_entities_concurrent(self, entities: List[Entity]) -> List[Entity]:
        """å¹¶å‘ä¼˜åŒ–çš„å®ä½“å»é‡æ–¹æ³•"""
        if len(entities) <= 1:
            return entities

        self._deduplication_calls_count = 0

        # ç¬¬ä¸€æ­¥ï¼šå¿«é€Ÿé¢„ç­›é€‰
        logger.info("Step 1: Fast pre-filtering based on name and type similarity")
        candidate_pairs = self._prefilter_duplicate_candidates(entities)

        if not candidate_pairs:
            logger.info("No potential duplicates found in pre-filtering")
            return entities

        logger.info(f"Found {len(candidate_pairs)} potential duplicate pairs for LLM verification")

        # ç¬¬äºŒæ­¥ï¼šå¹¶å‘LLMéªŒè¯
        logger.info("Step 2: Concurrent LLM verification of potential duplicates")
        duplicate_pairs = await self._verify_duplicates_concurrent(entities, candidate_pairs)

        # ç¬¬ä¸‰æ­¥ï¼šåˆå¹¶é‡å¤å®ä½“
        logger.info("Step 3: Merging duplicate entities")
        return self._merge_duplicate_entities(entities, duplicate_pairs)

    def _prefilter_duplicate_candidates(self, entities: List[Entity]) -> List[Tuple[int, int]]:
        """é¢„ç­›é€‰æ½œåœ¨é‡å¤å®ä½“å¯¹"""
        candidate_pairs = []

        for i in range(len(entities)):
            for j in range(i + 1, len(entities)):
                entity1, entity2 = entities[i], entities[j]

                # åªå¯¹ç›¸åŒç±»å‹çš„å®ä½“è¿›è¡Œæ¯”è¾ƒ
                if entity1.entity_type != entity2.entity_type:
                    continue

                # è®¡ç®—åç§°ç›¸ä¼¼åº¦
                name_similarity = self._calculate_name_similarity(entity1.name, entity2.name)

                # æ£€æŸ¥åˆ«ååŒ¹é…
                alias_match = self._check_alias_match(entity1, entity2)

                # å¦‚æœåç§°ç›¸ä¼¼åº¦é«˜æˆ–æœ‰åˆ«ååŒ¹é…ï¼Œåˆ™è®¤ä¸ºæ˜¯å€™é€‰é‡å¤å¯¹
                if name_similarity > 0.7 or alias_match:
                    candidate_pairs.append((i, j))

        return candidate_pairs

    def _calculate_name_similarity(self, name1: str, name2: str) -> float:
        """è®¡ç®—ä¸¤ä¸ªåç§°çš„ç›¸ä¼¼åº¦"""
        name1_lower = name1.lower().strip()
        name2_lower = name2.lower().strip()

        if name1_lower == name2_lower:
            return 1.0

        if name1_lower in name2_lower or name2_lower in name1_lower:
            return 0.8

        return self._levenshtein_similarity(name1_lower, name2_lower)

    def _levenshtein_similarity(self, s1: str, s2: str) -> float:
        """è®¡ç®—ç¼–è¾‘è·ç¦»ç›¸ä¼¼åº¦"""
        if len(s1) == 0 or len(s2) == 0:
            return 0.0

        if len(s1) > len(s2):
            s1, s2 = s2, s1

        distances = list(range(len(s1) + 1))
        for i2, c2 in enumerate(s2):
            new_distances = [i2 + 1]
            for i1, c1 in enumerate(s1):
                if c1 == c2:
                    new_distances.append(distances[i1])
                else:
                    new_distances.append(1 + min(distances[i1], distances[i1 + 1], new_distances[-1]))
            distances = new_distances

        max_len = max(len(s1), len(s2))
        return 1.0 - (distances[-1] / max_len)

    def _check_alias_match(self, entity1: Entity, entity2: Entity) -> bool:
        """æ£€æŸ¥ä¸¤ä¸ªå®ä½“æ˜¯å¦æœ‰åˆ«ååŒ¹é…"""
        all_names1 = {entity1.name.lower()} | {alias.lower() for alias in entity1.aliases}
        all_names2 = {entity2.name.lower()} | {alias.lower() for alias in entity2.aliases}
        return bool(all_names1 & all_names2)

    async def _verify_duplicates_concurrent(
        self, entities: List[Entity], candidate_pairs: List[Tuple[int, int]]
    ) -> List[Tuple[int, int]]:
        """å¹¶å‘éªŒè¯æ½œåœ¨é‡å¤å®ä½“å¯¹"""
        if not candidate_pairs:
            return []

        semaphore = asyncio.Semaphore(min(self.max_concurrent, 5))

        async def verify_single_pair(pair_idx: int, entity_pair: Tuple[int, int]) -> Tuple[int, bool]:
            async with semaphore:
                entity1_idx, entity2_idx = entity_pair
                entity1, entity2 = entities[entity1_idx], entities[entity2_idx]
                is_duplicate = await self.entity_extractor._check_entity_duplicate_llm(entity1, entity2)
                self._deduplication_calls_count += 1
                return pair_idx, is_duplicate

        tasks = [verify_single_pair(i, pair) for i, pair in enumerate(candidate_pairs)]

        logger.info(f"Verifying {len(candidate_pairs)} potential duplicate pairs concurrently")
        results = await asyncio.gather(*tasks, return_exceptions=True)

        duplicate_pairs = []
        for result in results:
            if isinstance(result, Exception):
                logger.error(f"Error in duplicate verification: {result}")
            elif isinstance(result, tuple) and len(result) == 2:
                pair_idx, is_duplicate = result
                if is_duplicate:
                    duplicate_pairs.append(candidate_pairs[pair_idx])
            else:
                logger.error(f"Unexpected result format: {result}")

        logger.info(f"Found {len(duplicate_pairs)} confirmed duplicate pairs")
        return duplicate_pairs

    def _merge_duplicate_entities(self, entities: List[Entity], duplicate_pairs: List[Tuple[int, int]]) -> List[Entity]:
        """åˆå¹¶é‡å¤å®ä½“"""
        if not duplicate_pairs:
            return entities

        # ä½¿ç”¨å¹¶æŸ¥é›†å¤„ç†ä¼ é€’æ€§é‡å¤å…³ç³»
        parent = list(range(len(entities)))

        def find(x: int) -> int:
            if parent[x] != x:
                parent[x] = find(parent[x])
            return parent[x]

        def union(x: int, y: int) -> None:
            px, py = find(x), find(y)
            if px != py:
                parent[px] = py

        for i, j in duplicate_pairs:
            union(i, j)

        # åˆ†ç»„å®ä½“
        groups: Dict[int, List[int]] = {}
        for i in range(len(entities)):
            root = find(i)
            if root not in groups:
                groups[root] = []
            groups[root].append(i)

        # åˆå¹¶æ¯ä¸ªç»„çš„å®ä½“
        merged_entities = []
        for group_indices in groups.values():
            if len(group_indices) == 1:
                merged_entities.append(entities[group_indices[0]])
            else:
                merged_entity = self._merge_multiple_entities([entities[i] for i in group_indices])
                merged_entities.append(merged_entity)

        return merged_entities

    def _merge_multiple_entities(self, entities_to_merge: List[Entity]) -> Entity:
        """åˆå¹¶å¤šä¸ªå®ä½“ä¸ºä¸€ä¸ª"""
        if not entities_to_merge:
            raise ValueError("No entities to merge")

        if len(entities_to_merge) == 1:
            return entities_to_merge[0]

        base_entity = entities_to_merge[0]
        all_aliases = set(base_entity.aliases)
        all_descriptions = [base_entity.description] if base_entity.description else []
        merged_properties = base_entity.properties.copy()

        for entity in entities_to_merge[1:]:
            all_aliases.update(entity.aliases)
            if entity.name != base_entity.name:
                all_aliases.add(entity.name)

            if entity.description and entity.description not in all_descriptions:
                all_descriptions.append(entity.description)

            merged_properties.update(entity.properties)

        merged_entity = Entity(
            id=base_entity.id,
            name=base_entity.name,
            entity_type=base_entity.entity_type,
            description="; ".join(all_descriptions),
            aliases=list(all_aliases),
            properties=merged_properties,
            created_at=base_entity.created_at,
            updated_at=datetime.now(),
        )

        return merged_entity

    @staticmethod
    def _generate_entity_id(name: str) -> str:
        """ç”Ÿæˆå®ä½“ID"""
        import hashlib

        return f"entity_{hashlib.md5(name.encode()).hexdigest()[:8]}"


class LLMGraphUtils:
    """LLMå›¾æ„å»ºå·¥å…·ç±» - å…¬å…±å·¥å…·æ–¹æ³•"""

    @staticmethod
    def generate_entity_id(name: str) -> str:
        """ç”Ÿæˆå®ä½“ID"""
        import hashlib

        return f"entity_{hashlib.md5(name.encode()).hexdigest()[:8]}"

    @staticmethod
    def generate_relation_id(head: str, tail: str, relation_type: str) -> str:
        """ç”Ÿæˆå…³ç³»ID"""
        import hashlib

        relation_str = f"{head}_{relation_type}_{tail}"
        return f"relation_{hashlib.md5(relation_str.encode()).hexdigest()[:8]}"

    @staticmethod
    def find_similar_entity(graph: KnowledgeGraph, entity_data: Dict[str, Any]) -> Optional[Entity]:
        """åœ¨ç°æœ‰å›¾è°±ä¸­æŸ¥æ‰¾ç›¸ä¼¼å®ä½“"""
        entity_name = entity_data.get("name", "").lower()

        for entity in graph.entities.values():
            if entity.name.lower() == entity_name:
                return entity

            if entity_name in [alias.lower() for alias in entity.aliases]:
                return entity

            for alias in entity_data.get("aliases", []):
                if alias.lower() == entity.name.lower():
                    return entity
                if alias.lower() in [a.lower() for a in entity.aliases]:
                    return entity

        return None

    @staticmethod
    def merge_entity_data(entity: Entity, entity_data: Dict[str, Any]) -> None:
        """å°†æ–°çš„å®ä½“æ•°æ®åˆå¹¶åˆ°ç°æœ‰å®ä½“"""
        new_desc = entity_data.get("description", "")
        if new_desc and new_desc not in entity.description:
            entity.description = f"{entity.description}; {new_desc}" if entity.description else new_desc

        new_aliases = entity_data.get("aliases", [])
        for alias in new_aliases:
            if alias not in entity.aliases:
                entity.aliases.append(alias)

        new_props = entity_data.get("properties", {})
        entity.properties.update(new_props)
        entity.updated_at = datetime.now()

    @staticmethod
    def find_existing_relation(
        graph: KnowledgeGraph, head_name: str, tail_name: str, relation_type: str
    ) -> Optional[Relation]:
        """æŸ¥æ‰¾ç°æœ‰å…³ç³»"""
        for relation in graph.relations.values():
            if (
                relation.head_entity
                and relation.tail_entity
                and relation.head_entity.name == head_name
                and relation.tail_entity.name == tail_name
                and get_type_value(relation.relation_type) == relation_type
            ):
                return relation
        return None

    @staticmethod
    def build_entities_from_data(graph: KnowledgeGraph, entity_data_list: List[Dict[str, Any]]) -> Dict[str, str]:
        """ä»æ•°æ®æ„å»ºå®ä½“å¹¶è¿”å›åç§°åˆ°IDçš„æ˜ å°„"""
        entity_mapping = {}

        for entity_data in entity_data_list:
            entity = Entity(
                id=LLMGraphUtils.generate_entity_id(entity_data["name"]),
                name=entity_data["name"],
                entity_type=EntityType(entity_data["type"]),
                description=entity_data.get("description", ""),
                aliases=entity_data.get("aliases", []),
                properties=entity_data.get("properties", {}),
                created_at=datetime.now(),
                updated_at=datetime.now(),
            )
            graph.add_entity(entity)
            entity_mapping[entity_data["name"]] = entity.id

        return entity_mapping

    @staticmethod
    def build_relations_from_data(
        graph: KnowledgeGraph, relations_data: List[Dict[str, Any]], entity_mapping: Dict[str, str]
    ) -> int:
        """ä»æ•°æ®æ„å»ºå…³ç³»å¹¶è¿”å›æ·»åŠ çš„å…³ç³»æ•°é‡"""
        relations_added = 0

        for relation_data in relations_data:
            head_name = relation_data["head_entity"]
            tail_name = relation_data["tail_entity"]

            if head_name in entity_mapping and tail_name in entity_mapping:
                head_entity = graph.get_entity(entity_mapping[head_name])
                tail_entity = graph.get_entity(entity_mapping[tail_name])

                if head_entity and tail_entity:
                    relation = Relation(
                        id=LLMGraphUtils.generate_relation_id(head_name, tail_name, relation_data["relation_type"]),
                        head_entity=head_entity,
                        tail_entity=tail_entity,
                        relation_type=RelationType(relation_data["relation_type"]),
                        description=relation_data.get("description", ""),
                        properties=relation_data.get("properties", {}),
                        confidence=relation_data.get("confidence", 1.0),
                        created_at=datetime.now(),
                        updated_at=datetime.now(),
                    )
                    graph.add_relation(relation)
                    relations_added += 1

        return relations_added

    @staticmethod
    def create_text_chunks(
        texts: List[str], chunk_size: int = 1000, chunk_overlap: int = 200, source: str = "unknown"
    ) -> List[TextChunk]:
        """ä»æ–‡æœ¬åˆ—è¡¨åˆ›å»ºTextChunk"""
        chunker = TokenChunker(chunk_size=chunk_size, chunk_overlap=chunk_overlap)
        text_chunks = []

        for text_idx, text in enumerate(texts):
            if not text.strip():
                continue

            # å¦‚æœæ–‡æœ¬è¶³å¤Ÿå°ï¼Œç›´æ¥åˆ›å»ºä¸€ä¸ªchunk
            if chunker.count_tokens(text) <= chunk_size:
                chunk = TextChunk(
                    content=text,
                    source=f"{source}_doc_{text_idx}",
                    chunk_type="document" if len(texts) == 1 else "section",
                    start_index=0,
                    end_index=len(text),
                    created_at=datetime.now(),
                    updated_at=datetime.now(),
                )
                text_chunks.append(chunk)
            else:
                # å°†å¤§æ–‡æœ¬åˆ†å—
                chunks = chunker.split_text(text)
                start_pos = 0

                for chunk_idx, chunk_text in enumerate(chunks):
                    end_pos = start_pos + len(chunk_text)
                    chunk = TextChunk(
                        content=chunk_text,
                        source=f"{source}_doc_{text_idx}",
                        chunk_type="chunk",
                        start_index=start_pos,
                        end_index=end_pos,
                        metadata={
                            "original_doc_index": text_idx,
                            "chunk_index": chunk_idx,
                            "total_chunks": len(chunks),
                        },
                        created_at=datetime.now(),
                        updated_at=datetime.now(),
                    )
                    text_chunks.append(chunk)
                    start_pos = end_pos

        return text_chunks

    @staticmethod
    def link_chunks_to_entities_and_relations(
        text_chunks: List[TextChunk],
        graph: KnowledgeGraph,
        entity_data_list: List[Dict[str, Any]],
        relations_data: List[Dict[str, Any]],
    ) -> None:
        """å°†æ–‡æœ¬å—ä¸å®ä½“å’Œå…³ç³»å»ºç«‹è¿æ¥"""
        # ä¸ºæ¯ä¸ªchunkå»ºç«‹ä¸å®ä½“å’Œå…³ç³»çš„è¿æ¥
        for chunk in text_chunks:
            chunk_text_lower = chunk.content.lower()

            # è¿æ¥å®ä½“
            for entity_data in entity_data_list:
                entity_name = entity_data["name"].lower()
                aliases = [alias.lower() for alias in entity_data.get("aliases", [])]
                all_names = [entity_name] + aliases

                # æ£€æŸ¥å®ä½“åç§°æˆ–åˆ«åæ˜¯å¦åœ¨chunkä¸­
                for name in all_names:
                    if name in chunk_text_lower:
                        # æŸ¥æ‰¾å¯¹åº”çš„å®ä½“ID
                        for entity in graph.entities.values():
                            if entity.name.lower() == entity_name:
                                chunk.add_entity(entity.id)
                                break

            # è¿æ¥å…³ç³»
            for relation_data in relations_data:
                head_name = relation_data["head_entity"].lower()
                tail_name = relation_data["tail_entity"].lower()

                # å¦‚æœchunkåŒ…å«å…³ç³»çš„ä¸¤ä¸ªå®ä½“ï¼Œåˆ™å»ºç«‹è¿æ¥
                if head_name in chunk_text_lower and tail_name in chunk_text_lower:
                    for relation in graph.relations.values():
                        if (
                            relation.head_entity
                            and relation.tail_entity
                            and relation.head_entity.name.lower() == head_name
                            and relation.tail_entity.name.lower() == tail_name
                        ):
                            chunk.add_relation(relation.id)
                            break

    @staticmethod
    def store_text_chunks(
        graph: KnowledgeGraph, text_chunks: List[TextChunk], vector_storage: Optional[VectorStorage] = None
    ) -> bool:
        """å°†TextChunkå­˜å‚¨åˆ°å‘é‡å­˜å‚¨ä¸­"""
        if not vector_storage:
            return False

        try:
            # æ£€æŸ¥storageæ˜¯å¦æ”¯æŒtext chunkæ“ä½œ
            if hasattr(vector_storage, "batch_add_text_chunks"):
                result = vector_storage.batch_add_text_chunks(graph.id, text_chunks)
                return bool(result)
            elif hasattr(vector_storage, "add_text_chunk"):
                success = True
                for chunk in text_chunks:
                    if not vector_storage.add_text_chunk(graph.id, chunk):
                        success = False
                return success
            else:
                logger.warning("Vector storage does not support text chunk operations")
                return False
        except Exception as e:
            logger.error(f"Error storing text chunks: {e}")
            return False


# ============================================================================
# ISP-compliant LLM Builder Implementations
# ============================================================================


class MinimalLLMGraphBuilder(BasicGraphBuilder):
    """
    æœ€å°åŒ–çš„LLMå›¾æ„å»ºå™¨ - åªå®ç°æ ¸å¿ƒæ„å»ºåŠŸèƒ½

    é€‚ç”¨äºåªéœ€è¦åŸºæœ¬å›¾æ„å»ºè€Œä¸éœ€è¦æ›´æ–°ã€åˆå¹¶æˆ–å…¶ä»–é«˜çº§åŠŸèƒ½çš„å®¢æˆ·ç«¯ã€‚
    """

    def __init__(
        self,
        openai_api_key: str,
        openai_api_base: Optional[str] = None,
        llm_model: str = "gpt-4o-mini",
        max_tokens: int = 4000,
        temperature: float = 0.1,
        entity_extractor: Optional[LLMEntityExtractor] = None,
        relation_extractor: Optional[LLMRelationExtractor] = None,
    ):
        """
        åˆå§‹åŒ–æœ€å°åŒ–LLMå›¾æ„å»ºå™¨

        Args:
            openai_api_key: OpenAI APIå¯†é’¥
            openai_api_base: OpenAI APIåŸºç¡€URL
            llm_model: ä½¿ç”¨çš„LLMæ¨¡å‹
            max_tokens: æœ€å¤§tokenæ•°
            temperature: æ¸©åº¦å‚æ•°
            entity_extractor: å®ä½“æå–å™¨
            relation_extractor: å…³ç³»æå–å™¨
        """
        self.entity_extractor = entity_extractor or LLMEntityExtractor(
            openai_api_key=openai_api_key,
            openai_api_base=openai_api_base,
            llm_model=llm_model,
            max_tokens=max_tokens,
            temperature=temperature,
        )
        self.relation_extractor = relation_extractor or LLMRelationExtractor(
            openai_api_key=openai_api_key,
            openai_api_base=openai_api_base,
            llm_model=llm_model,
            max_tokens=max_tokens,
            temperature=temperature,
        )

    async def build_graph(
        self,
        texts: Optional[List[str]] = None,
        database_schema: Optional[Dict[str, Any]] = None,
        graph_name: str = "minimal_llm_graph",
    ) -> KnowledgeGraph:
        """æ„å»ºçŸ¥è¯†å›¾è°± - å¼‚æ­¥ç‰ˆæœ¬"""
        if not texts:
            return KnowledgeGraph(name=graph_name)

        try:
            graph = KnowledgeGraph(name=graph_name)
            all_entities = []
            all_relations = []

            # 1. åˆ›å»ºTextChunk
            text_chunks = LLMGraphUtils.create_text_chunks(
                texts, chunk_size=settings.MAX_CHUNK_SIZE, chunk_overlap=settings.CHUNK_OVERLAP, source=graph_name
            )
            logger.info(f"Created {len(text_chunks)} text chunks")

            # 2. ä¸²è¡Œå¤„ç†æ¯ä¸ªæ–‡æœ¬è¿›è¡Œå®ä½“å’Œå…³ç³»æå–
            for text in texts:
                # å¼‚æ­¥æå–å®ä½“
                entities = await self.entity_extractor._extract_entities_async(text)
                entity_data = [
                    {
                        "name": entity.name,
                        "type": get_type_value(entity.entity_type),
                        "description": entity.description,
                        "aliases": entity.aliases,
                        "properties": entity.properties,
                    }
                    for entity in entities
                ]
                all_entities.extend(entity_data)

                # å¼‚æ­¥æå–å…³ç³»
                relations = await self.relation_extractor._extract_relations_async(text, entities)
                relation_data = [
                    {
                        "head_entity": relation.head_entity.name if relation.head_entity else "",
                        "tail_entity": relation.tail_entity.name if relation.tail_entity else "",
                        "relation_type": get_type_value(relation.relation_type),
                        "description": relation.description,
                        "properties": relation.properties,
                        "confidence": relation.confidence,
                    }
                    for relation in relations
                ]
                all_relations.extend(relation_data)

            # 3. æ„å»ºå›¾è°±
            entity_mapping = LLMGraphUtils.build_entities_from_data(graph, all_entities)
            LLMGraphUtils.build_relations_from_data(graph, all_relations, entity_mapping)

            # 4. å»ºç«‹TextChunkä¸å®ä½“å’Œå…³ç³»çš„è¿æ¥
            LLMGraphUtils.link_chunks_to_entities_and_relations(text_chunks, graph, all_entities, all_relations)

            # 5. å°†TextChunkæ·»åŠ åˆ°å›¾è°±çš„metadataä¸­ï¼ˆç®€å•å­˜å‚¨ï¼‰
            if not hasattr(graph, "text_chunks"):
                graph.text_chunks = {}
            for chunk in text_chunks:
                graph.text_chunks[chunk.id] = chunk

            logger.info(
                f"Minimal LLM graph built: {len(graph.entities)} entities, {len(graph.relations)} relations, {len(text_chunks)} text chunks"
            )
            return graph

        except Exception as e:
            logger.error(f"Error building minimal LLM graph: {e}")
            raise


class FlexibleLLMGraphBuilder(UpdatableGraphBuilder):
    """
    çµæ´»çš„LLMå›¾æ„å»ºå™¨ - æ”¯æŒæ„å»ºå’Œæ›´æ–°

    é€‚ç”¨äºéœ€è¦æ›´æ–°å›¾è°±ä½†ä¸éœ€è¦åˆå¹¶ã€éªŒè¯ç­‰é«˜çº§åŠŸèƒ½çš„å®¢æˆ·ç«¯ã€‚
    """

    def __init__(
        self,
        openai_api_key: str,
        openai_api_base: Optional[str] = None,
        llm_model: str = "gpt-4o-mini",
        embedding_model: str = "text-embedding-3-small",
        max_tokens: int = 4000,
        temperature: float = 0.1,
        vector_storage: Optional[VectorStorage] = None,
        entity_extractor: Optional[LLMEntityExtractor] = None,
        relation_extractor: Optional[LLMRelationExtractor] = None,
        max_concurrent: int = 10,
    ):
        """
        åˆå§‹åŒ–çµæ´»LLMå›¾æ„å»ºå™¨

        Args:
            openai_api_key: OpenAI APIå¯†é’¥
            openai_api_base: OpenAI APIåŸºç¡€URL
            llm_model: ä½¿ç”¨çš„LLMæ¨¡å‹
            embedding_model: ä½¿ç”¨çš„åµŒå…¥æ¨¡å‹
            max_tokens: æœ€å¤§tokenæ•°
            temperature: æ¸©åº¦å‚æ•°
            vector_storage: å‘é‡å­˜å‚¨åç«¯
            entity_extractor: å®ä½“æå–å™¨
            relation_extractor: å…³ç³»æå–å™¨
            max_concurrent: æœ€å¤§å¹¶å‘æ•°
        """
        self.entity_extractor = entity_extractor or LLMEntityExtractor(
            openai_api_key=openai_api_key,
            openai_api_base=openai_api_base,
            llm_model=llm_model,
            max_tokens=max_tokens,
            temperature=temperature,
        )
        self.relation_extractor = relation_extractor or LLMRelationExtractor(
            openai_api_key=openai_api_key,
            openai_api_base=openai_api_base,
            llm_model=llm_model,
            max_tokens=max_tokens,
            temperature=temperature,
        )

        # åˆå§‹åŒ–ä½¿ç”¨ç»Ÿè®¡è·Ÿè¸ªå™¨
        self.usage_tracker = LLMUsageTracker(llm_model, embedding_model)

        # åˆå§‹åŒ–å¼‚æ­¥å¤„ç†å™¨
        self.async_processor = LLMAsyncProcessor(
            self.entity_extractor,
            self.relation_extractor,
            self.usage_tracker,
            max_concurrent,
        )

        # åˆå§‹åŒ–å‘é‡åµŒå…¥ï¼ˆå¯é€‰ï¼‰
        self.vector_storage = vector_storage or JsonVectorStorage("flexible_llm_graph_vectors.json")
        self.graph_embedding = OpenAIEmbedding(
            vector_storage=self.vector_storage,
            openai_api_key=openai_api_key,
            openai_api_base=openai_api_base,
            embedding_model=embedding_model,
        )

    async def build_graph(
        self,
        texts: Optional[List[str]] = None,
        database_schema: Optional[Dict[str, Any]] = None,
        graph_name: str = "flexible_llm_graph",
    ) -> KnowledgeGraph:
        """å¼‚æ­¥æ„å»ºçŸ¥è¯†å›¾è°±"""
        if not texts:
            return KnowledgeGraph(name=graph_name)

        logger.info("Starting flexible LLM-based knowledge graph construction")

        try:
            graph = KnowledgeGraph(name=graph_name)

            # æ›´æ–°ä¼šè¯ç»Ÿè®¡
            self.usage_tracker.update_session_stats(len(texts), 0, 0)

            # 1. åˆ›å»ºTextChunk
            text_chunks = LLMGraphUtils.create_text_chunks(texts, source=graph_name)
            logger.info(f"Created {len(text_chunks)} text chunks")

            # 2. å¼‚æ­¥å¤„ç†æ–‡æœ¬
            all_entities, all_relations = await self.async_processor.process_texts_async(texts)

            # 3. å»é‡å®ä½“
            deduplicated_entities = await self.async_processor.deduplicate_entities(all_entities)

            # 4. æ„å»ºå›¾è°±
            entity_mapping = LLMGraphUtils.build_entities_from_data(graph, deduplicated_entities)
            relations_added = LLMGraphUtils.build_relations_from_data(graph, all_relations, entity_mapping)

            # 5. å»ºç«‹TextChunkä¸å®ä½“å’Œå…³ç³»çš„è¿æ¥
            LLMGraphUtils.link_chunks_to_entities_and_relations(
                text_chunks, graph, deduplicated_entities, all_relations
            )

            # 6. å­˜å‚¨TextChunkåˆ°vector storage
            chunk_storage_success = LLMGraphUtils.store_text_chunks(graph, text_chunks, self.vector_storage)
            if chunk_storage_success:
                logger.info(f"Successfully stored {len(text_chunks)} text chunks to vector storage")
            else:
                logger.warning("Failed to store text chunks to vector storage")

            # 7. æ›´æ–°ç»Ÿè®¡
            self.usage_tracker.update_session_stats(0, len(deduplicated_entities), relations_added)

            # 8. æ„å»ºåµŒå…¥ï¼ˆå¯é€‰ï¼‰
            if self.graph_embedding:
                await self._build_embeddings(graph)

                # ä¸ºtext chunksæ„å»ºåµŒå…¥
                if hasattr(self.graph_embedding, "build_chunk_embeddings"):
                    await self.graph_embedding.build_chunk_embeddings(text_chunks)

            logger.info(
                f"Flexible LLM graph built: {len(graph.entities)} entities, {len(graph.relations)} relations, {len(text_chunks)} text chunks"
            )
            return graph

        except Exception as e:
            logger.error(f"Error building flexible LLM graph: {e}")
            raise

    async def update_graph(
        self,
        graph: KnowledgeGraph,
        new_entities: Optional[List[Entity]] = None,
        new_relations: Optional[List[Relation]] = None,
    ) -> KnowledgeGraph:
        """æ›´æ–°ç°æœ‰å›¾è°±"""
        logger.info("Updating flexible LLM graph")

        try:
            if new_entities:
                for entity in new_entities:
                    if entity.id not in graph.entities:
                        graph.add_entity(entity)

            if new_relations:
                for relation in new_relations:
                    if (
                        relation.head_entity
                        and relation.head_entity.id in graph.entities
                        and relation.tail_entity
                        and relation.tail_entity.id in graph.entities
                        and relation.id not in graph.relations
                    ):
                        graph.add_relation(relation)

            # æ›´æ–°åµŒå…¥
            if self.graph_embedding:
                await self._build_embeddings(graph)

            logger.info(f"Flexible LLM graph updated: {len(graph.entities)} entities, {len(graph.relations)} relations")
            return graph

        except Exception as e:
            logger.error(f"Error updating flexible LLM graph: {e}")
            raise

    async def update_graph_with_texts(
        self,
        graph: KnowledgeGraph,
        texts: List[str],
    ) -> KnowledgeGraph:
        """ä½¿ç”¨æ–°æ–‡æœ¬æ›´æ–°å›¾è°±"""
        if not texts:
            return graph

        logger.info(f"Updating graph with {len(texts)} new texts")

        # 1. åˆ›å»ºæ–°çš„TextChunk
        new_text_chunks = LLMGraphUtils.create_text_chunks(texts, source=f"{graph.name}_update")
        logger.info(f"Created {len(new_text_chunks)} new text chunks")

        # 2. å¤„ç†æ–°æ–‡æœ¬
        all_entities, all_relations = await self.async_processor.process_texts_async(texts)

        # 3. ä¸ç°æœ‰å®ä½“è¿›è¡ŒåŒ¹é…å’Œå»é‡
        new_entities = []
        for entity_data in all_entities:
            existing_entity = LLMGraphUtils.find_similar_entity(graph, entity_data)
            if existing_entity:
                LLMGraphUtils.merge_entity_data(existing_entity, entity_data)
            else:
                new_entities.append(entity_data)

        # 4. æ·»åŠ æ–°å®ä½“
        entity_mapping = {entity.name: entity.id for entity in graph.entities.values()}
        new_entity_mapping = LLMGraphUtils.build_entities_from_data(graph, new_entities)
        entity_mapping.update(new_entity_mapping)

        # 5. æ·»åŠ æ–°å…³ç³»
        relations_added = LLMGraphUtils.build_relations_from_data(graph, all_relations, entity_mapping)

        # 6. å»ºç«‹æ–°TextChunkä¸å®ä½“å’Œå…³ç³»çš„è¿æ¥
        LLMGraphUtils.link_chunks_to_entities_and_relations(new_text_chunks, graph, all_entities, all_relations)

        # 7. å­˜å‚¨æ–°çš„TextChunk
        chunk_storage_success = LLMGraphUtils.store_text_chunks(graph, new_text_chunks, self.vector_storage)
        if chunk_storage_success:
            logger.info(f"Successfully stored {len(new_text_chunks)} new text chunks to vector storage")
        else:
            logger.warning("Failed to store new text chunks to vector storage")

        # 8. æ›´æ–°åµŒå…¥
        if self.graph_embedding:
            await self._build_embeddings(graph)

            # ä¸ºæ–°çš„text chunksæ„å»ºåµŒå…¥
            if hasattr(self.graph_embedding, "build_chunk_embeddings"):
                await self.graph_embedding.build_chunk_embeddings(new_text_chunks)

        logger.info(
            f"Graph updated with texts: {len(new_entities)} new entities, {relations_added} new relations, {len(new_text_chunks)} new text chunks"
        )
        return graph

    async def _build_embeddings(self, graph: KnowledgeGraph) -> None:
        """æ„å»ºå›¾è°±åµŒå…¥"""
        try:
            await self.graph_embedding.build_text_embeddings(graph)
            self.usage_tracker.track_embedding_call("entity_embedding", success=True)
            self.usage_tracker.track_embedding_call("relation_embedding", success=True)
            self.graph_embedding.save_embeddings()
        except Exception as e:
            logger.error(f"Error building embeddings: {e}")
            self.usage_tracker.track_embedding_call("entity_embedding", success=False, error_msg=str(e))

    def get_usage_statistics(self) -> Dict[str, Any]:
        """è·å–ä½¿ç”¨ç»Ÿè®¡ä¿¡æ¯"""
        return self.usage_tracker.get_usage_statistics()

    def print_usage_summary(self) -> None:
        """æ‰“å°ä½¿ç”¨ç»Ÿè®¡æ‘˜è¦"""
        self.usage_tracker.print_usage_summary()

    def cleanup(self) -> None:
        """æ¸…ç†èµ„æº"""
        self.usage_tracker.cleanup()
        if self.graph_embedding:
            self.graph_embedding.save_embeddings()


class LLMGraphBuilder(
    GraphMergerMixin,
    GraphValidatorMixin,
    GraphExporterMixin,
    GraphStatisticsMixin,
    FullFeaturedGraphBuilder,
):
    """
    å…¨åŠŸèƒ½LLMå›¾æ„å»ºå™¨ - åŒ…å«æ‰€æœ‰åŠŸèƒ½

    åªæœ‰éœ€è¦æ‰€æœ‰åŠŸèƒ½çš„å®¢æˆ·ç«¯æ‰åº”è¯¥ä½¿ç”¨è¿™ä¸ªç±»ã€‚
    å¤§å¤šæ•°å®¢æˆ·ç«¯åº”è¯¥ä½¿ç”¨æ›´ä¸“æ³¨çš„æ¥å£ã€‚
    """

    def __init__(
        self,
        openai_api_key: str,
        openai_api_base: Optional[str] = None,
        llm_model: str = "gpt-4o-mini",
        embedding_model: str = "text-embedding-3-small",
        max_tokens: int = 4000,
        temperature: float = 0.1,
        vector_storage: Optional[VectorStorage] = None,
        entity_extractor: Optional[LLMEntityExtractor] = None,
        relation_extractor: Optional[LLMRelationExtractor] = None,
        max_concurrent: int = 10,
    ):
        """åˆå§‹åŒ–å…¨åŠŸèƒ½LLMå›¾æ„å»ºå™¨"""
        super().__init__()

        # ä½¿ç”¨ç»„åˆè€Œä¸æ˜¯ç»§æ‰¿æ¥å¤ç”¨FlexibleLLMGraphBuilderçš„åŠŸèƒ½
        self.flexible_builder = FlexibleLLMGraphBuilder(
            openai_api_key=openai_api_key,
            openai_api_base=openai_api_base,
            llm_model=llm_model,
            embedding_model=embedding_model,
            max_tokens=max_tokens,
            temperature=temperature,
            vector_storage=vector_storage,
            entity_extractor=entity_extractor,
            relation_extractor=relation_extractor,
            max_concurrent=max_concurrent,
        )

    async def build_graph(
        self,
        texts: Optional[List[str]] = None,
        database_schema: Optional[Dict[str, Any]] = None,
        graph_name: str = "comprehensive_llm_graph",
    ) -> KnowledgeGraph:
        """æ„å»ºå…¨åŠŸèƒ½å›¾è°±"""
        # å§”æ‰˜ç»™çµæ´»æ„å»ºå™¨
        graph = await self.flexible_builder.build_graph(texts, database_schema, graph_name)

        # æ‰§è¡ŒéªŒè¯
        validation_result = await self.validate_graph(graph)
        if not validation_result.get("valid", True):
            logger.warning(f"Graph validation issues: {validation_result.get('issues', [])}")

        # å¯¼å‡ºå›¾è°±å’Œç»Ÿè®¡ä¿¡æ¯
        await self._export_graph_to_file(graph, os.path.join(settings.workdir, "graph.json"))

        # ä¿å­˜å‘é‡å­˜å‚¨
        self.flexible_builder.vector_storage.save()
        return graph

    async def update_graph(
        self,
        graph: KnowledgeGraph,
        new_entities: Optional[List[Entity]] = None,
        new_relations: Optional[List[Relation]] = None,
    ) -> KnowledgeGraph:
        """æ›´æ–°å›¾è°±å¹¶éªŒè¯"""
        # å§”æ‰˜ç»™çµæ´»æ„å»ºå™¨
        updated_graph = await self.flexible_builder.update_graph(graph, new_entities, new_relations)

        # æ‰§è¡ŒéªŒè¯
        validation_result = await self.validate_graph(updated_graph)
        if not validation_result.get("valid", True):
            logger.warning(f"Graph validation issues: {validation_result.get('issues', [])}")

        return updated_graph

    async def _export_graph_to_file(self, graph: KnowledgeGraph, file_path: str) -> None:
        """å¯¼å‡ºå›¾è°±åˆ°æ–‡ä»¶"""
        import asyncio
        import json

        # ä½¿ç”¨ export_to_format æ–¹æ³•è·å– JSON æ ¼å¼çš„æ•°æ®
        graph_data = await self.export_to_format(graph, "json")

        # åœ¨äº‹ä»¶å¾ªç¯ä¸­å¼‚æ­¥å†™å…¥æ–‡ä»¶
        def _write_file():
            with open(file_path, "w", encoding="utf-8") as f:
                json.dump(graph_data, f, ensure_ascii=False, indent=2)

        await asyncio.get_event_loop().run_in_executor(None, _write_file)

    def get_usage_statistics(self) -> Dict[str, Any]:
        """è·å–ä½¿ç”¨ç»Ÿè®¡ä¿¡æ¯"""
        return self.flexible_builder.get_usage_statistics()

    def print_usage_summary(self) -> None:
        """æ‰“å°ä½¿ç”¨ç»Ÿè®¡æ‘˜è¦"""
        self.flexible_builder.print_usage_summary()

    def cleanup(self) -> None:
        """æ¸…ç†èµ„æº"""
        self.flexible_builder.cleanup()


class StreamingLLMGraphBuilder(StreamingGraphBuilder, IncrementalBuilderMixin, GraphStatisticsMixin):
    """
    æµå¼LLMå›¾æ„å»ºå™¨ - é€‚ç”¨äºå®æ—¶å¢é‡æ›´æ–°

    ä¸“ä¸ºéœ€è¦å®æ—¶å¤„ç†æ–‡æ¡£æµè€Œä¸éœ€è¦åˆå¹¶æˆ–éªŒè¯åŠŸèƒ½çš„åº”ç”¨è®¾è®¡ã€‚
    """

    def __init__(
        self,
        openai_api_key: str,
        openai_api_base: Optional[str] = None,
        llm_model: str = "gpt-4o-mini",
        max_tokens: int = 4000,
        temperature: float = 0.1,
        entity_extractor: Optional[LLMEntityExtractor] = None,
        relation_extractor: Optional[LLMRelationExtractor] = None,
        max_concurrent: int = 5,  # æµå¼å¤„ç†ä½¿ç”¨è¾ƒå°çš„å¹¶å‘æ•°
    ):
        """åˆå§‹åŒ–æµå¼LLMå›¾æ„å»ºå™¨"""
        super().__init__()

        self.entity_extractor = entity_extractor or LLMEntityExtractor(
            openai_api_key=openai_api_key,
            openai_api_base=openai_api_base,
            llm_model=llm_model,
            max_tokens=max_tokens,
            temperature=temperature,
        )
        self.relation_extractor = relation_extractor or LLMRelationExtractor(
            openai_api_key=openai_api_key,
            openai_api_base=openai_api_base,
            llm_model=llm_model,
            max_tokens=max_tokens,
            temperature=temperature,
        )

        self.usage_tracker = LLMUsageTracker(llm_model, "")
        self.async_processor = LLMAsyncProcessor(
            self.entity_extractor,
            self.relation_extractor,
            self.usage_tracker,
            max_concurrent,
        )

    async def build_graph(
        self,
        texts: Optional[List[str]] = None,
        database_schema: Optional[Dict[str, Any]] = None,
        graph_name: str = "streaming_llm_graph",
    ) -> KnowledgeGraph:
        """æ„å»ºåˆå§‹å›¾è°±ç”¨äºæµå¼å¤„ç†"""
        if not texts:
            graph = KnowledgeGraph(name=graph_name)
        else:
            # 1. åˆ›å»ºTextChunk
            text_chunks = LLMGraphUtils.create_text_chunks(texts, source=graph_name)
            logger.info(f"Created {len(text_chunks)} text chunks for streaming graph")

            # 2. ä½¿ç”¨æœ€å°æ„å»ºå™¨åˆ›å»ºåˆå§‹å›¾è°±
            minimal_builder = MinimalLLMGraphBuilder(
                openai_api_key=self.entity_extractor.openai_client.api_key,
                openai_api_base=(
                    str(self.entity_extractor.openai_client.base_url)
                    if self.entity_extractor.openai_client.base_url
                    else None
                ),
                llm_model=self.entity_extractor.llm_model,
                entity_extractor=self.entity_extractor,
                relation_extractor=self.relation_extractor,
            )
            graph = await minimal_builder.build_graph(texts, database_schema, graph_name)

        self._current_graph = graph
        return graph

    async def add_documents_async(
        self, documents: List[str], document_ids: Optional[List[str]] = None
    ) -> KnowledgeGraph:
        """å¼‚æ­¥æ·»åŠ æ–‡æ¡£åˆ°æµå¼å›¾è°±"""
        if document_ids is None:
            document_ids = [f"doc_{i}_{datetime.now().timestamp()}" for i in range(len(documents))]

        if len(document_ids) != len(documents):
            raise ValueError("Number of document IDs must match number of documents")

        logger.info(f"Adding {len(documents)} documents to streaming graph")

        try:
            # 1. ç¡®ä¿å½“å‰å›¾è°±å­˜åœ¨
            if self._current_graph is None:
                raise ValueError("Current graph is not initialized. Call build_initial_graph first.")

            # 2. åˆ›å»ºæ–°æ–‡æ¡£çš„TextChunk
            new_text_chunks = LLMGraphUtils.create_text_chunks(
                documents, source=f"{self._current_graph.name}_streaming"
            )
            logger.info(f"Created {len(new_text_chunks)} text chunks for new documents")

            # 3. å¤„ç†æ–°æ–‡æ¡£
            all_entities, all_relations = await self.async_processor.process_texts_async(documents)

            # 4. ç®€å•å»é‡ï¼ˆæµå¼å¤„ç†é¿å…å¤æ‚å»é‡ï¼‰
            simple_dedup_entities = self._simple_entity_dedup(all_entities)

            # 5. æ·»åŠ åˆ°å½“å‰å›¾è°±
            entity_mapping = {entity.name: entity.id for entity in self._current_graph.entities.values()}

            # æ„å»ºæ–°å®ä½“
            new_entities = []
            for entity_data in simple_dedup_entities:
                if entity_data["name"] not in entity_mapping:
                    new_entities.append(entity_data)

            new_entity_mapping = LLMGraphUtils.build_entities_from_data(self._current_graph, new_entities)
            entity_mapping.update(new_entity_mapping)

            # æ„å»ºæ–°å…³ç³»
            relations_added = LLMGraphUtils.build_relations_from_data(
                self._current_graph, all_relations, entity_mapping
            )

            # 6. å»ºç«‹æ–°TextChunkä¸å®ä½“å’Œå…³ç³»çš„è¿æ¥
            LLMGraphUtils.link_chunks_to_entities_and_relations(
                new_text_chunks, self._current_graph, simple_dedup_entities, all_relations
            )

            # 7. å°†æ–°TextChunkæ·»åŠ åˆ°å›¾è°±
            if not hasattr(self._current_graph, "text_chunks"):
                self._current_graph.text_chunks = {}
            for chunk in new_text_chunks:
                self._current_graph.text_chunks[chunk.id] = chunk

            # 8. è®°å½•æ–‡æ¡£æ˜ å°„
            for i, doc_id in enumerate(document_ids):
                chunk_ids = [chunk.id for chunk in new_text_chunks if chunk.source.endswith(f"_doc_{i}")]
                entity_ids = list(new_entity_mapping.values())
                self._document_registry[doc_id] = entity_ids + chunk_ids

            logger.info(
                f"Added {len(new_entities)} entities, {relations_added} relations, and {len(new_text_chunks)} text chunks from streaming documents"
            )
            return self._current_graph

        except Exception as e:
            logger.error(f"Error adding documents to streaming graph: {e}")
            raise

    def _simple_entity_dedup(self, entities: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """ç®€å•çš„å®ä½“å»é‡ - åŸºäºåç§°å’Œç±»å‹"""
        seen = set()
        deduplicated = []

        for entity_data in entities:
            key = (entity_data["name"].lower(), entity_data["type"])
            if key not in seen:
                seen.add(key)
                deduplicated.append(entity_data)

        return deduplicated

    async def add_documents(self, documents: List[str], document_ids: Optional[List[str]] = None) -> KnowledgeGraph:
        """å¼‚æ­¥ç‰ˆæœ¬çš„æ·»åŠ æ–‡æ¡£æ–¹æ³• - å®ç°IncrementalBuilderæ¥å£"""
        return await self.add_documents_async(documents, document_ids)

    async def remove_documents(self, document_ids: List[str]) -> KnowledgeGraph:
        """ç§»é™¤æ–‡æ¡£æ–¹æ³• - å®ç°IncrementalBuilderæ¥å£"""
        if self._current_graph is None:
            raise ValueError("No graph exists to remove documents from")

        try:
            entities_to_remove = set()
            chunks_to_remove = set()

            # æ”¶é›†è¦ç§»é™¤çš„æ–‡æ¡£çš„å®ä½“å’Œchunks
            for doc_id in document_ids:
                if doc_id in self._document_registry:
                    items_to_remove = self._document_registry[doc_id]
                    for item_id in items_to_remove:
                        # åŒºåˆ†å®ä½“IDå’Œchunk ID
                        if item_id.startswith("entity_"):
                            entities_to_remove.add(item_id)
                        else:
                            chunks_to_remove.add(item_id)
                    del self._document_registry[doc_id]

            # ç§»é™¤TextChunk
            if hasattr(self._current_graph, "text_chunks"):
                for chunk_id in chunks_to_remove:
                    if chunk_id in self._current_graph.text_chunks:
                        del self._current_graph.text_chunks[chunk_id]

            # ç§»é™¤å®ä½“ï¼ˆè¿™ä¹Ÿä¼šç§»é™¤ç›¸å…³å…³ç³»ï¼‰
            for entity_id in entities_to_remove:
                if entity_id in self._current_graph.entities:
                    self._current_graph.remove_entity(entity_id)

            logger.info(f"Removed {len(entities_to_remove)} entities and {len(chunks_to_remove)} text chunks")
            return self._current_graph

        except Exception as e:
            logger.error(f"Error removing documents: {e}")
            raise

    def get_usage_statistics(self) -> Dict[str, Any]:
        """è·å–ä½¿ç”¨ç»Ÿè®¡ä¿¡æ¯"""
        return self.usage_tracker.get_usage_statistics()


class BatchLLMGraphBuilder(GraphMergerMixin, BatchGraphBuilder):
    """
    æ‰¹é‡LLMå›¾æ„å»ºå™¨ - ä¼˜åŒ–æ‰¹é‡å¤„ç†å¤šä¸ªæ•°æ®æº

    é€‚ç”¨äºéœ€è¦å¤„ç†å¤šä¸ªæ•°æ®æºå¹¶åˆå¹¶å®ƒä»¬ï¼Œä½†ä¸éœ€è¦å¢é‡æ›´æ–°æˆ–éªŒè¯çš„åœºæ™¯ã€‚
    """

    def __init__(
        self,
        openai_api_key: str,
        openai_api_base: Optional[str] = None,
        llm_model: str = "gpt-4o-mini",
        embedding_model: str = "text-embedding-3-small",
        max_tokens: int = 4000,
        temperature: float = 0.1,
        vector_storage: Optional[VectorStorage] = None,
        entity_extractor: Optional[LLMEntityExtractor] = None,
        relation_extractor: Optional[LLMRelationExtractor] = None,
        max_concurrent: int = 15,  # æ‰¹é‡å¤„ç†ä½¿ç”¨æ›´é«˜çš„å¹¶å‘æ•°
    ):
        """åˆå§‹åŒ–æ‰¹é‡LLMå›¾æ„å»ºå™¨"""
        super().__init__()

        # ä½¿ç”¨ç»„åˆå¤ç”¨FlexibleLLMGraphBuilder
        self.flexible_builder = FlexibleLLMGraphBuilder(
            openai_api_key=openai_api_key,
            openai_api_base=openai_api_base,
            llm_model=llm_model,
            embedding_model=embedding_model,
            max_tokens=max_tokens,
            temperature=temperature,
            vector_storage=vector_storage,
            entity_extractor=entity_extractor,
            relation_extractor=relation_extractor,
            max_concurrent=max_concurrent,
        )

    async def build_graph(
        self,
        texts: Optional[List[str]] = None,
        database_schema: Optional[Dict[str, Any]] = None,
        graph_name: str = "batch_llm_graph",
    ) -> KnowledgeGraph:
        """æ„å»ºæ‰¹é‡å›¾è°±"""
        return await self.flexible_builder.build_graph(texts, database_schema, graph_name)

    async def build_from_multiple_sources(
        self, sources: List[Dict[str, Any]], graph_name: str = "multi_source_batch_llm_graph"
    ) -> KnowledgeGraph:
        """ä»å¤šä¸ªå¼‚æ„æ•°æ®æºæ„å»ºå›¾è°±"""
        sub_graphs = []

        # å¹¶è¡Œå¤„ç†å¤šä¸ªæ•°æ®æº
        tasks = []
        for i, source in enumerate(sources):
            source_type = source.get("type")
            source_data = source.get("data")
            source_name = f"{graph_name}_source_{i}"

            if source_type == "text":
                texts = source_data if isinstance(source_data, list) else [source_data]
                task = self.flexible_builder.build_graph(texts=texts, graph_name=source_name)
                tasks.append(task)
            elif source_type == "mixed":
                if source_data is not None:
                    texts = source_data.get("texts", [])
                else:
                    texts = []
                task = self.flexible_builder.build_graph(texts=texts, graph_name=source_name)
                tasks.append(task)
            else:
                logger.warning(f"Unknown source type: {source_type}")

        if not tasks:
            return KnowledgeGraph(name=graph_name)

        # å¹¶è¡Œæ‰§è¡Œæ‰€æœ‰æ„å»ºä»»åŠ¡
        logger.info(f"Building {len(tasks)} sub-graphs in parallel")
        sub_graphs = await asyncio.gather(*tasks, return_exceptions=True)

        # è¿‡æ»¤å¼‚å¸¸ç»“æœ
        valid_graphs: List[KnowledgeGraph] = []
        for result in sub_graphs:
            if isinstance(result, Exception):
                logger.error(f"Error building sub-graph: {result}")
            elif isinstance(result, KnowledgeGraph):
                valid_graphs.append(result)

        if not valid_graphs:
            return KnowledgeGraph(name=graph_name)

        # åˆå¹¶æ‰€æœ‰å­å›¾
        logger.info(f"Merging {len(valid_graphs)} sub-graphs")
        merged_graph = await self.merge_graphs(valid_graphs)
        merged_graph.name = graph_name
        return merged_graph

    def get_usage_statistics(self) -> Dict[str, Any]:
        """è·å–ä½¿ç”¨ç»Ÿè®¡ä¿¡æ¯"""
        return self.flexible_builder.get_usage_statistics()

    def cleanup(self) -> None:
        """æ¸…ç†èµ„æº"""
        self.flexible_builder.cleanup()


# å¯¼å‡ºæ‰€æœ‰å…¬å…±ç±»å’Œå‡½æ•°
__all__ = [
    "LLMUsageTracker",
    "LLMAsyncProcessor",
    "LLMGraphUtils",
    "MinimalLLMGraphBuilder",
    "FlexibleLLMGraphBuilder",
    "LLMGraphBuilder",
    "StreamingLLMGraphBuilder",
    "BatchLLMGraphBuilder",
]
