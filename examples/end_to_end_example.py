#!/usr/bin/env python3
"""
ç«¯åˆ°ç«¯ç¤ºä¾‹ï¼šä½¿ç”¨ KnowledgeGraphBuilder æ„å»ºçŸ¥è¯†å›¾è°±

æ­¤ç¤ºä¾‹å±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨ agraph åŒ…å¤„ç† examples/documents/ æ–‡ä»¶å¤¹ä¸­çš„æ–‡æ¡£ï¼Œ
æ„å»ºå®Œæ•´çš„çŸ¥è¯†å›¾è°±ï¼Œå¹¶å±•ç¤ºç»“æœã€‚
"""
import os
import sys
from pathlib import Path
import asyncio
from dotenv import load_dotenv

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv(project_root / ".env")

from agraph.builder.builder import KnowledgeGraphBuilder
from agraph.base import KnowledgeGraph
from agraph.config import BuilderConfig, settings
from agraph.vectordb import ChromaVectorStore


def main():
    """ä¸»å‡½æ•°ï¼šæ¼”ç¤ºç«¯åˆ°ç«¯çŸ¥è¯†å›¾è°±æ„å»ºè¿‡ç¨‹"""

    print("ğŸš€ å¯åŠ¨ç«¯åˆ°ç«¯çŸ¥è¯†å›¾è°±æ„å»ºç¤ºä¾‹")
    print("=" * 50)

    # 1. è®¾ç½®æ–‡æ¡£ç›®å½•
    documents_dir = project_root / "examples" / "documents"
    settings.workdir = project_root / "workdir" / "end_to_end" # è®¾ç½®å·¥ä½œç›®å½•
    os.makedirs(settings.workdir, exist_ok=True)

    if not documents_dir.exists():
        print(f"âŒ æ–‡æ¡£ç›®å½•ä¸å­˜åœ¨: {documents_dir}")
        return

    # è·å–æ‰€æœ‰æ–‡æ¡£æ–‡ä»¶
    document_files = list(documents_dir.glob("*"))
    print(f"ğŸ“„ å‘ç° {len(document_files)} ä¸ªæ–‡æ¡£æ–‡ä»¶:")
    for doc in document_files:
        print(f"   - {doc.name}")
    print()

    # 2. é…ç½®æ„å»ºå™¨
    print("âš™ï¸ é…ç½®çŸ¥è¯†å›¾è°±æ„å»ºå™¨...")

    print(f"   ä½¿ç”¨API Base: {settings.openai.api_base}")
    print(f"   ä½¿ç”¨æ¨¡å‹: {settings.llm.model}")

    config = BuilderConfig(
        # åŸºæœ¬é…ç½®
        chunk_size=1000,
        chunk_overlap=200,

        # LLMé…ç½® - ä½¿ç”¨ç¯å¢ƒå˜é‡
        llm_provider="openai",  # ä½¿ç”¨OpenAIå…¼å®¹æ¥å£
        llm_model=settings.llm.model,

        # ç½®ä¿¡åº¦é˜ˆå€¼
        entity_confidence_threshold=0.7,
        relation_confidence_threshold=0.6,

        # èšç±»é…ç½®
        cluster_algorithm="community_detection",
        min_cluster_size=2,

        # ç¼“å­˜é…ç½®
        cache_dir=str(settings.workdir / "cache")
    )

    # 3. åˆ›å»ºæ„å»ºå™¨å®ä¾‹
    builder = KnowledgeGraphBuilder(config=config)

    try:
        # 4. å…ˆæµ‹è¯•æ–‡æ¡£å¤„ç†
        print("ğŸ”§ å¼€å§‹æ„å»ºçŸ¥è¯†å›¾è°±...")
        print("   è¿™ä¸ªè¿‡ç¨‹å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿæ—¶é—´ï¼Œè¯·è€å¿ƒç­‰å¾…...")
        print()

        knowledge_graph = builder.build_from_documents(
            documents=document_files,
            graph_name="å…¬å¸çŸ¥è¯†å›¾è°±",
            graph_description="åŸºäºå…¬å¸æ–‡æ¡£æ„å»ºçš„ç»¼åˆçŸ¥è¯†å›¾è°±",
            use_cache=True
        )

        print("âœ… çŸ¥è¯†å›¾è°±æ„å»ºå®Œæˆï¼")
        print()

        # 5. å±•ç¤ºæ„å»ºç»“æœ
        display_results(knowledge_graph, builder)

        # 6. å±•ç¤ºå¢é‡æ›´æ–°åŠŸèƒ½
        demonstrate_incremental_updates(builder, document_files)

        # 7. ä¿å­˜çŸ¥è¯†å›¾è°±
        asyncio.run(save_knowledge_graph(knowledge_graph))

        print("ğŸ’¾ çŸ¥è¯†å›¾è°±å·²ä¿å­˜åˆ°å·¥ä½œç›®å½•")
        demonstrate_graph_operations(knowledge_graph)

    except Exception as e:
        print(f"âŒ æ„å»ºè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        print("è¯·æ£€æŸ¥:")
        print("1. æ˜¯å¦æ­£ç¡®é…ç½®äº†LLM APIå¯†é’¥")
        print("2. ç½‘ç»œè¿æ¥æ˜¯å¦æ­£å¸¸")
        print("3. æ–‡æ¡£æ–‡ä»¶æ˜¯å¦å¯è¯»")

        # æ˜¾ç¤ºæ„å»ºçŠ¶æ€
        build_status = builder.get_build_status()
        print(f"\nå½“å‰æ„å»ºçŠ¶æ€: {build_status}")


def display_results(kg, builder):
    """å±•ç¤ºçŸ¥è¯†å›¾è°±æ„å»ºç»“æœ"""

    print("ğŸ“Š çŸ¥è¯†å›¾è°±æ„å»ºç»“æœ:")
    print("-" * 30)
    print(f"å›¾è°±åç§°: {kg.name}")
    print(f"å›¾è°±æè¿°: {kg.description}")
    print()

    # ç»Ÿè®¡ä¿¡æ¯ - ç›´æ¥è®¿é—®å­—å…¸å±æ€§
    entities = list(kg.entities.values())
    relations = list(kg.relations.values())
    clusters = list(kg.clusters.values())
    chunks = list(kg.text_chunks.values())

    print(f"ğŸ“‹ ç»Ÿè®¡ä¿¡æ¯:")
    print(f"   - å®ä½“æ•°é‡: {len(entities)}")
    print(f"   - å…³ç³»æ•°é‡: {len(relations)}")
    print(f"   - èšç±»æ•°é‡: {len(clusters)}")
    print(f"   - æ–‡æœ¬å—æ•°é‡: {len(chunks)}")
    print()

    # å±•ç¤ºéƒ¨åˆ†å®ä½“
    if entities:
        print("ğŸ·ï¸  æå–çš„å®ä½“æ ·ä¾‹ (å‰10ä¸ª):")
        for i, entity in enumerate(entities[:10]):
            print(f"   {i+1}. {entity.name} ({entity.entity_type}) - ç½®ä¿¡åº¦: {entity.confidence:.2f}")
        if len(entities) > 10:
            print(f"   ... è¿˜æœ‰ {len(entities) - 10} ä¸ªå®ä½“")
        print()

    # å±•ç¤ºéƒ¨åˆ†å…³ç³»
    if relations:
        print("ğŸ”— æå–çš„å…³ç³»æ ·ä¾‹ (å‰10ä¸ª):")
        for i, relation in enumerate(relations[:10]):
            # ä½¿ç”¨æ­£ç¡®çš„å±æ€§å
            source_name = relation.head_entity.name if relation.head_entity else "æœªçŸ¥"
            target_name = relation.tail_entity.name if relation.tail_entity else "æœªçŸ¥"
            print(f"   {i+1}. {source_name} --[{relation.relation_type}]--> {target_name} "
                  f"(ç½®ä¿¡åº¦: {relation.confidence:.2f})")
        if len(relations) > 10:
            print(f"   ... è¿˜æœ‰ {len(relations) - 10} ä¸ªå…³ç³»")
        print()

    # å±•ç¤ºèšç±»ä¿¡æ¯
    if clusters:
        print("ğŸ¯ èšç±»ä¿¡æ¯:")
        for i, cluster in enumerate(clusters[:5]):
            entities_in_cluster = len(cluster.entities) if hasattr(cluster, 'entities') else 0
            print(f"   èšç±» {i+1}: {cluster.name} (åŒ…å« {entities_in_cluster} ä¸ªå®ä½“)")
        if len(clusters) > 5:
            print(f"   ... è¿˜æœ‰ {len(clusters) - 5} ä¸ªèšç±»")
        print()

    # æ˜¾ç¤ºç¼“å­˜ä¿¡æ¯
    cache_info = builder.get_cache_info()
    backend_info = cache_info.get('backend', {})
    print(f"ğŸ’¾ ç¼“å­˜ä¿¡æ¯:")
    print(f"   - ç¼“å­˜ç›®å½•: {backend_info.get('cache_dir', 'N/A')}")
    print(f"   - ç¼“å­˜æ–‡ä»¶æ•°: {backend_info.get('total_files', 0)}")
    print(f"   - æ–‡æ¡£å¤„ç†ç¼“å­˜: {cache_info.get('document_processing', {}).get('total_documents', 0)} ä¸ªæ–‡æ¡£å·²ç¼“å­˜")


async def save_knowledge_graph(kg: KnowledgeGraph):
    """ä¿å­˜çŸ¥è¯†å›¾è°±åˆ°æ–‡ä»¶"""

    output_dir = settings.workdir / "output"
    output_dir.mkdir(exist_ok=True)
    vectordb = ChromaVectorStore(
        persist_directory= str(output_dir / "vectordb"),
        use_openai_embeddings=True
    )

    try:
        # å¯¼å‡ºä¸ºJSONæ ¼å¼ (å¦‚æœKnowledgeGraphæ”¯æŒçš„è¯)
        if hasattr(kg, 'export_to_json'):
            json_path = output_dir / "knowledge_graph.json"
            kg.export_to_json(json_path)
            print(f"ğŸ’¾ çŸ¥è¯†å›¾è°±å·²ä¿å­˜ä¸ºJSON: {json_path}")

        if hasattr(kg, 'export_to_graphml'):
            graphml_path = output_dir / "knowledge_graph.graphml"
            kg.export_to_graphml(graphml_path)
            print(f"ğŸ’¾ çŸ¥è¯†å›¾è°±å·²ä¿å­˜ä¸ºGraphML: {graphml_path}")

        # å¯¼å‡ºå®ä½“å’Œå…³ç³»ä¿¡æ¯
        entities_path = output_dir / "entities.txt"
        with open(entities_path, 'w', encoding='utf-8') as f:
            f.write("æå–çš„å®ä½“:\n")
            f.write("=" * 50 + "\n")
            for entity in kg.entities.values():
                f.write(f"{entity.name} ({entity.entity_type}) - ç½®ä¿¡åº¦: {entity.confidence:.2f}\n")

        relations_path = output_dir / "relations.txt"
        with open(relations_path, 'w', encoding='utf-8') as f:
            f.write("æå–çš„å…³ç³»:\n")
            f.write("=" * 50 + "\n")
            for relation in kg.relations.values():
                # ä½¿ç”¨æ­£ç¡®çš„å±æ€§å
                source_name = relation.head_entity.name if relation.head_entity else "æœªçŸ¥"
                target_name = relation.tail_entity.name if relation.tail_entity else "æœªçŸ¥"
                f.write(f"{source_name} --[{relation.relation_type}]--> {target_name} "
                       f"(ç½®ä¿¡åº¦: {relation.confidence:.2f})\n")

        print(f"ğŸ“ å®ä½“ä¿¡æ¯å·²ä¿å­˜åˆ°: {entities_path}")
        print(f"ğŸ“ å…³ç³»ä¿¡æ¯å·²ä¿å­˜åˆ°: {relations_path}")
        print()

    except Exception as e:
        print(f"âš ï¸  ä¿å­˜æ–‡ä»¶æ—¶å‡ºç°é”™è¯¯: {e}")

    try:
        # ä¿å­˜å‘é‡æ•°æ®åº“
        await vectordb.initialize()
        await vectordb.batch_add_entities(kg.entities.values())
        await vectordb.batch_add_relations(kg.relations.values())
        await vectordb.batch_add_clusters(kg.clusters.values())
        await vectordb.batch_add_text_chunks(kg.text_chunks.values())
        await vectordb.close()

        print(f"ğŸ”— å‘é‡æ•°æ®åº“å·²ä¿å­˜åˆ°: {vectordb.persist_directory}")
    except Exception as e:
        print(f"âš ï¸ ä¿å­˜å‘é‡æ•°æ®åº“æ—¶å‡ºç°é”™è¯¯: {e}")


def demonstrate_incremental_updates(builder, document_files):
    """æ¼”ç¤ºå¢é‡æ›´æ–°åŠŸèƒ½"""

    print("ğŸ”„ å¢é‡æ›´æ–°åŠŸèƒ½æ¼”ç¤º:")
    print("-" * 30)

    # æ˜¾ç¤ºæ–‡æ¡£å¤„ç†çŠ¶æ€
    print("ğŸ“Š æ–‡æ¡£å¤„ç†çŠ¶æ€æ€»ç»“:")
    doc_status = builder.get_document_processing_status()
    print(f"   - æ€»æ–‡æ¡£æ•°: {doc_status.get('total_documents', 0)}")
    print(f"   - å·²å®Œæˆ: {doc_status.get('completed', 0)}")
    print(f"   - å¤±è´¥: {doc_status.get('failed', 0)}")
    print(f"   - æ€»å¤„ç†æ—¶é—´: {doc_status.get('total_processing_time', 0):.4f} ç§’")
    print()

    # æ˜¾ç¤ºå„ä¸ªæ–‡æ¡£çš„çŠ¶æ€
    print("ğŸ“‹ å„æ–‡æ¡£å¤„ç†çŠ¶æ€:")
    for i, doc_path in enumerate(document_files[:5]):  # åªæ˜¾ç¤ºå‰5ä¸ª
        status = builder.get_document_processing_status(doc_path)
        if status:
            print(f"   {i+1}. {doc_path.name}:")
            print(f"      çŠ¶æ€: {status.get('processing_status', 'unknown')}")
            print(f"      å¤„ç†æ—¶é—´: {status.get('processing_time', 0):.4f}s")
            print(f"      æ–‡ä»¶å“ˆå¸Œ: {status.get('file_hash', 'N/A')[:16]}...")

    if len(document_files) > 5:
        print(f"   ... è¿˜æœ‰ {len(document_files) - 5} ä¸ªæ–‡æ¡£")
    print()

    # è·å–ç¼“å­˜ä¿¡æ¯
    cache_info = builder.get_cache_info()
    print("ğŸ’¾ è¯¦ç»†ç¼“å­˜ä¿¡æ¯:")
    backend_info = cache_info.get('backend', {})
    print(f"   - ç¼“å­˜ç›®å½•: {backend_info.get('cache_dir', 'N/A')}")
    print(f"   - æ€»ç¼“å­˜æ–‡ä»¶: {backend_info.get('total_files', 0)}")
    print(f"   - ç¼“å­˜æ€»å¤§å°: {backend_info.get('total_size', 0)} å­—èŠ‚")

    doc_processing_info = cache_info.get('document_processing', {})
    if doc_processing_info:
        print(f"   - æ–‡æ¡£ç¼“å­˜ç»Ÿè®¡: {doc_processing_info}")
    print()

    print("ğŸ’¡ æç¤º:")
    print("   - å†æ¬¡è¿è¡Œæ­¤è„šæœ¬æ—¶ï¼Œå·²å¤„ç†çš„æ–‡æ¡£å°†ä½¿ç”¨ç¼“å­˜ï¼Œå¤§å¤§æé«˜é€Ÿåº¦")
    print("   - å¦‚æœæ–‡æ¡£è¢«ä¿®æ”¹ï¼Œç³»ç»Ÿä¼šè‡ªåŠ¨æ£€æµ‹å¹¶é‡æ–°å¤„ç†")
    print("   - ä½¿ç”¨ builder.force_reprocess_document(path) å¯å¼ºåˆ¶é‡æ–°å¤„ç†ç‰¹å®šæ–‡æ¡£")
    print("   - ä½¿ç”¨ builder.clear_document_cache() å¯æ¸…é™¤æ‰€æœ‰æ–‡æ¡£ç¼“å­˜")
    print()


def demonstrate_graph_operations(kg: KnowledgeGraph):
    """æ¼”ç¤ºçŸ¥è¯†å›¾è°±çš„åŸºæœ¬æ“ä½œ"""

    print("ğŸ” çŸ¥è¯†å›¾è°±æ“ä½œæ¼”ç¤º:")
    print("-" * 30)

    # æœç´¢å®ä½“
    if hasattr(kg, 'search_entities'):
        print("æœç´¢åŒ…å«'å…¬å¸'çš„å®ä½“:")
        company_entities = kg.search_entities("å…¬å¸")
        for entity in company_entities[:5]:
            print(f"   - {entity.name}")
        print()

    vectordb = ChromaVectorStore(
        persist_directory=str(settings.workdir / "output" / "vectordb"),
        use_openai_embeddings=True
    )
    try:
        asyncio.run(vectordb.initialize())
        print("ğŸ”— å‘é‡æ•°æ®åº“å·²åˆå§‹åŒ–")
        results = asyncio.run(vectordb.search_entities("å…¬å¸", top_k=5))
        print("æœç´¢å‘é‡æ•°æ®åº“ä¸­çš„å®ä½“:")
        for entity, score in results:
            print(f"   - {entity.name} ({entity.entity_type}) - ç½®ä¿¡åº¦: {score:.2f}")
        results = asyncio.run(vectordb.search_relations("å…¬å¸", top_k=5))
        print("æœç´¢å‘é‡æ•°æ®åº“ä¸­çš„å…³ç³»:")
        for relation, score in results:
            source_name = relation.head_entity.name if relation.head_entity else "æœªçŸ¥"
            target_name = relation.tail_entity.name if relation.tail_entity else "æœªçŸ¥"
            print(f"   - {source_name} --[{relation.relation_type}]--> {target_name} "
                  f"(ç½®ä¿¡åº¦: {score:.2f})")
        results = asyncio.run(vectordb.search_text_chunks("å…¬å¸", top_k=5))
        print("æœç´¢å‘é‡æ•°æ®åº“ä¸­çš„æ–‡æœ¬å—:")
        for chunk, score in results:
            print(f"   - {chunk.content[:50]}... (ç½®ä¿¡åº¦: {score:.2f})")

        asyncio.run(vectordb.close())
    except Exception as e:
        print(f"âš ï¸ å‘é‡æ•°æ®åº“åˆå§‹åŒ–å¤±è´¥: {e}")

if __name__ == "__main__":
    main()
