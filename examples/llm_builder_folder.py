"""
æ–‡ä»¶å¤¹çŸ¥è¯†åº“æ„å»ºå™¨ç¤ºä¾‹

å±•ç¤ºå¦‚ä½•è¯»å–æ–‡ä»¶å¤¹ä¸‹çš„å„ç§æ–‡ä»¶ç±»å‹å¹¶æ„å»ºçŸ¥è¯†åº“ï¼Œ
æ”¯æŒPDFã€Wordã€æ–‡æœ¬ã€HTMLã€Excelã€JSONã€å›¾ç‰‡ç­‰å¤šç§æ ¼å¼
"""

import asyncio
import logging
from pathlib import Path
from typing import Dict, List, Optional, Tuple

from agraph.builders import (
    BatchLLMGraphBuilder,
    FlexibleLLMGraphBuilder,
    LLMGraphBuilder,
    MinimalLLMGraphBuilder,
)
from agraph.config import Settings
from agraph.embeddings import JsonVectorStorage
from agraph.processer import (
    DocumentProcessorManager,
    can_process,
    get_supported_extensions,
    process_document,
)

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class FolderKnowledgeBaseBuilder:
    """æ–‡ä»¶å¤¹çŸ¥è¯†åº“æ„å»ºå™¨"""

    def __init__(self, builder_type: str = "flexible", output_dir: str = "workdir"):
        """
        åˆå§‹åŒ–æ–‡ä»¶å¤¹çŸ¥è¯†åº“æ„å»ºå™¨

        Args:
            builder_type: æ„å»ºå™¨ç±»å‹ ("minimal", "flexible", "full", "batch")
            output_dir: è¾“å‡ºç›®å½•
        """
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)

        # æ–‡æ¡£å¤„ç†å™¨
        self.doc_processor = DocumentProcessorManager()

        # åˆ›å»ºå¯¹åº”çš„LLMæ„å»ºå™¨
        self.builder = self._create_builder(builder_type)

        # å¤„ç†ç»Ÿè®¡
        self.processing_stats = {
            "total_files": 0,
            "processed_files": 0,
            "failed_files": 0,
            "supported_files": 0,
            "unsupported_files": 0,
            "file_types": {},
            "errors": [],
        }

    def _create_builder(self, builder_type: str):
        """åˆ›å»ºLLMæ„å»ºå™¨"""
        if builder_type == "minimal":
            return MinimalLLMGraphBuilder(
                openai_api_key=Settings.OPENAI_API_KEY,
                openai_api_base=Settings.OPENAI_API_BASE,
                llm_model=Settings.LLM_MODEL,
                temperature=0.1,
            )
        elif builder_type == "flexible":
            return FlexibleLLMGraphBuilder(
                openai_api_key=Settings.OPENAI_API_KEY,
                openai_api_base=Settings.OPENAI_API_BASE,
                llm_model=Settings.LLM_MODEL,
                embedding_model=Settings.EMBEDDING_MODEL,
                vector_storage=JsonVectorStorage(file_path=str(self.output_dir / "vector_store.json")),
            )
        elif builder_type == "batch":
            return BatchLLMGraphBuilder(
                openai_api_key=Settings.OPENAI_API_KEY,
                openai_api_base=Settings.OPENAI_API_BASE,
                llm_model=Settings.LLM_MODEL,
                embedding_model=Settings.EMBEDDING_MODEL,
                max_concurrent=8,
            )
        elif builder_type == "full":
            return LLMGraphBuilder(
                openai_api_key=Settings.OPENAI_API_KEY,
                openai_api_base=Settings.OPENAI_API_BASE,
                llm_model=Settings.LLM_MODEL,
                embedding_model=Settings.EMBEDDING_MODEL,
                max_concurrent=10,
                vector_storage=JsonVectorStorage(file_path=str(self.output_dir / "vector_store.json")),
            )
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ„å»ºå™¨ç±»å‹: {builder_type}")

    def scan_folder(
        self, folder_path: str, recursive: bool = True, file_patterns: Optional[List[str]] = None
    ) -> List[Path]:
        """
        æ‰«ææ–‡ä»¶å¤¹è·å–æ‰€æœ‰å¯å¤„ç†çš„æ–‡ä»¶

        Args:
            folder_path: æ–‡ä»¶å¤¹è·¯å¾„
            recursive: æ˜¯å¦é€’å½’æ‰«æå­æ–‡ä»¶å¤¹
            file_patterns: æ–‡ä»¶æ¨¡å¼è¿‡æ»¤ï¼ˆå¦‚ ["*.pdf", "*.txt"]ï¼‰

        Returns:
            æ–‡ä»¶è·¯å¾„åˆ—è¡¨
        """
        folder = Path(folder_path)
        if not folder.exists():
            raise FileNotFoundError(f"æ–‡ä»¶å¤¹ä¸å­˜åœ¨: {folder_path}")

        if not folder.is_dir():
            raise ValueError(f"è·¯å¾„ä¸æ˜¯æ–‡ä»¶å¤¹: {folder_path}")

        files = []

        # è·å–æ”¯æŒçš„æ‰©å±•å
        supported_extensions = get_supported_extensions()
        logger.info(f"æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {supported_extensions}")

        # æ‰«ææ–‡ä»¶
        if recursive:
            pattern = "**/*"
        else:
            pattern = "*"

        for file_path in folder.glob(pattern):
            if file_path.is_file():
                self.processing_stats["total_files"] += 1

                # æ£€æŸ¥æ–‡ä»¶æ‰©å±•å
                ext = file_path.suffix.lower()
                self.processing_stats["file_types"][ext] = self.processing_stats["file_types"].get(ext, 0) + 1

                # æ£€æŸ¥æ˜¯å¦å¯ä»¥å¤„ç†
                if can_process(file_path):
                    self.processing_stats["supported_files"] += 1

                    # åº”ç”¨æ–‡ä»¶æ¨¡å¼è¿‡æ»¤
                    if file_patterns:
                        if any(file_path.match(pattern) for pattern in file_patterns):
                            files.append(file_path)
                    else:
                        files.append(file_path)
                else:
                    self.processing_stats["unsupported_files"] += 1
                    logger.debug(f"ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {file_path}")

        logger.info(
            f"æ‰«æå®Œæˆ: æ€»æ–‡ä»¶ {self.processing_stats['total_files']} ä¸ªï¼Œ"
            f"æ”¯æŒå¤„ç† {self.processing_stats['supported_files']} ä¸ªï¼Œ"
            f"å°†å¤„ç† {len(files)} ä¸ª"
        )

        return files

    def process_files(self, file_paths: List[Path]) -> Tuple[List[str], Dict[str, str]]:
        """
        å¤„ç†æ–‡ä»¶åˆ—è¡¨ï¼Œæå–æ–‡æœ¬å†…å®¹

        Args:
            file_paths: æ–‡ä»¶è·¯å¾„åˆ—è¡¨

        Returns:
            (æˆåŠŸæå–çš„æ–‡æœ¬åˆ—è¡¨, æ–‡ä»¶è·¯å¾„åˆ°æ–‡æœ¬çš„æ˜ å°„)
        """
        texts = []
        file_text_mapping = {}

        logger.info(f"å¼€å§‹å¤„ç† {len(file_paths)} ä¸ªæ–‡ä»¶...")

        for i, file_path in enumerate(file_paths, 1):
            try:
                logger.info(f"å¤„ç†æ–‡ä»¶ {i}/{len(file_paths)}: {file_path.name}")

                # å¤„ç†æ–‡ä»¶
                text_content = process_document(file_path)

                if text_content and text_content.strip():
                    # æ·»åŠ æ–‡ä»¶ä¿¡æ¯åˆ°æ–‡æœ¬å†…å®¹
                    enhanced_content = f"æ–‡ä»¶: {file_path.name}\nè·¯å¾„: {file_path}\n\n{text_content}"
                    texts.append(enhanced_content)
                    file_text_mapping[str(file_path)] = enhanced_content

                    self.processing_stats["processed_files"] += 1
                    logger.debug(f"æˆåŠŸå¤„ç†: {file_path.name}, å†…å®¹é•¿åº¦: {len(text_content)}")
                else:
                    logger.warning(f"æ–‡ä»¶å†…å®¹ä¸ºç©º: {file_path}")

            except Exception as e:
                self.processing_stats["failed_files"] += 1
                self.processing_stats["errors"].append(f"{file_path}: {str(e)}")
                logger.error(f"å¤„ç†æ–‡ä»¶å¤±è´¥ {file_path}: {e}")

        logger.info(
            f"æ–‡ä»¶å¤„ç†å®Œæˆ: æˆåŠŸ {self.processing_stats['processed_files']} ä¸ªï¼Œ"
            f"å¤±è´¥ {self.processing_stats['failed_files']} ä¸ª"
        )

        return texts, file_text_mapping

    async def build_knowledge_base(
        self,
        folder_path: str,
        graph_name: str,
        recursive: bool = True,
        file_patterns: Optional[List[str]] = None,
        chunk_size: Optional[int] = None,
    ) -> Dict:
        """
        ä»æ–‡ä»¶å¤¹æ„å»ºçŸ¥è¯†åº“

        Args:
            folder_path: æ–‡ä»¶å¤¹è·¯å¾„
            graph_name: å›¾è°±åç§°
            recursive: æ˜¯å¦é€’å½’æ‰«æ
            file_patterns: æ–‡ä»¶æ¨¡å¼è¿‡æ»¤
            chunk_size: æ–‡æœ¬åˆ†å—å¤§å°ï¼ˆå¯é€‰ï¼‰

        Returns:
            åŒ…å«å›¾è°±å’Œç»Ÿè®¡ä¿¡æ¯çš„å­—å…¸
        """
        logger.info(f"å¼€å§‹æ„å»ºçŸ¥è¯†åº“: {graph_name}")
        logger.info(f"æºæ–‡ä»¶å¤¹: {folder_path}")

        try:
            # 1. æ‰«ææ–‡ä»¶å¤¹
            file_paths = self.scan_folder(folder_path, recursive, file_patterns)

            if not file_paths:
                logger.warning("æ²¡æœ‰æ‰¾åˆ°å¯å¤„ç†çš„æ–‡ä»¶")
                return {"graph": None, "stats": self.processing_stats, "message": "æ²¡æœ‰æ‰¾åˆ°å¯å¤„ç†çš„æ–‡ä»¶"}

            # 2. å¤„ç†æ–‡ä»¶
            texts, file_mapping = self.process_files(file_paths)

            if not texts:
                logger.warning("æ²¡æœ‰æˆåŠŸæå–åˆ°æ–‡æœ¬å†…å®¹")
                return {
                    "graph": None,
                    "stats": self.processing_stats,
                    "file_mapping": file_mapping,
                    "message": "æ²¡æœ‰æˆåŠŸæå–åˆ°æ–‡æœ¬å†…å®¹",
                }

            # 3. æ–‡æœ¬åˆ†å—ï¼ˆå¦‚æœéœ€è¦ï¼‰
            if chunk_size:
                texts = self._chunk_texts(texts, chunk_size)

            # 4. æ„å»ºå›¾è°±
            logger.info(f"ä½¿ç”¨ {len(texts)} ä¸ªæ–‡æœ¬å—æ„å»ºå›¾è°±...")
            graph = await self.builder.build_graph(texts=texts, graph_name=graph_name)

            # 5. ä¿å­˜ç»“æœ
            await self._save_results(graph, file_mapping, graph_name)

            logger.info(f"çŸ¥è¯†åº“æ„å»ºå®Œæˆ: {graph_name}")

            return {
                "graph": graph,
                "stats": self.processing_stats,
                "file_mapping": file_mapping,
                "message": "çŸ¥è¯†åº“æ„å»ºæˆåŠŸ",
            }

        except Exception as e:
            logger.error(f"æ„å»ºçŸ¥è¯†åº“å¤±è´¥: {e}")
            self.processing_stats["errors"].append(f"æ„å»ºå¤±è´¥: {str(e)}")
            return {"graph": None, "stats": self.processing_stats, "message": f"æ„å»ºå¤±è´¥: {str(e)}"}

    def _chunk_texts(self, texts: List[str], chunk_size: int) -> List[str]:
        """å°†é•¿æ–‡æœ¬åˆ†å—"""
        chunked_texts = []

        for text in texts:
            if len(text) <= chunk_size:
                chunked_texts.append(text)
            else:
                # ç®€å•çš„æŒ‰å­—ç¬¦åˆ†å—ï¼ˆå¯ä»¥æ”¹è¿›ä¸ºæŒ‰æ®µè½æˆ–å¥å­åˆ†å—ï¼‰
                for i in range(0, len(text), chunk_size):
                    chunk = text[i : i + chunk_size]
                    if chunk.strip():
                        chunked_texts.append(chunk)

        logger.info(f"æ–‡æœ¬åˆ†å—å®Œæˆ: {len(texts)} -> {len(chunked_texts)}")
        return chunked_texts

    async def _save_results(self, graph, file_mapping: Dict[str, str], graph_name: str):
        """ä¿å­˜æ„å»ºç»“æœ"""
        try:
            # ä¿å­˜å›¾è°±ä¿¡æ¯
            graph_info = {
                "name": graph.name,
                "entities_count": len(graph.entities),
                "relations_count": len(graph.relations),
                "processing_stats": self.processing_stats,
                "file_mapping": {path: len(content) for path, content in file_mapping.items()},
            }

            import json

            info_file = self.output_dir / f"{graph_name}_info.json"
            with open(info_file, "w", encoding="utf-8") as f:
                json.dump(graph_info, f, ensure_ascii=False, indent=2)

            # å¦‚æœæ˜¯å…¨åŠŸèƒ½æ„å»ºå™¨ï¼Œå¯¼å‡ºå›¾è°±
            if hasattr(self.builder, "export_to_format"):
                exported_data = await self.builder.export_to_format(graph, "json")
                export_file = self.output_dir / f"{graph_name}_graph.json"
                with open(export_file, "w", encoding="utf-8") as f:
                    json.dump(exported_data, f, ensure_ascii=False, indent=2)
                logger.info(f"å›¾è°±å·²å¯¼å‡ºåˆ°: {export_file}")

            logger.info(f"æ„å»ºä¿¡æ¯å·²ä¿å­˜åˆ°: {info_file}")

        except Exception as e:
            logger.error(f"ä¿å­˜ç»“æœå¤±è´¥: {e}")

    def print_statistics(self):
        """æ‰“å°å¤„ç†ç»Ÿè®¡ä¿¡æ¯"""
        stats = self.processing_stats

        print("\n" + "=" * 60)
        print("ğŸ“Š æ–‡ä»¶å¤¹å¤„ç†ç»Ÿè®¡ä¿¡æ¯")
        print("=" * 60)

        print(f"æ€»æ–‡ä»¶æ•°: {stats['total_files']}")
        print(f"æ”¯æŒçš„æ–‡ä»¶æ•°: {stats['supported_files']}")
        print(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ•°: {stats['unsupported_files']}")
        print(f"æˆåŠŸå¤„ç†: {stats['processed_files']}")
        print(f"å¤„ç†å¤±è´¥: {stats['failed_files']}")

        print("\nğŸ“ æ–‡ä»¶ç±»å‹åˆ†å¸ƒ:")
        for ext, count in stats["file_types"].items():
            print(f"  {ext or '(æ— æ‰©å±•å)'}: {count} ä¸ª")

        if stats["errors"]:
            print("\nâŒ é”™è¯¯ä¿¡æ¯:")
            for error in stats["errors"][:5]:  # åªæ˜¾ç¤ºå‰5ä¸ªé”™è¯¯
                print(f"  - {error}")
            if len(stats["errors"]) > 5:
                print(f"  ... è¿˜æœ‰ {len(stats['errors']) - 5} ä¸ªé”™è¯¯")


async def example_simple_folder():
    """ç¤ºä¾‹1: ç®€å•æ–‡ä»¶å¤¹å¤„ç†"""
    print("\n" + "=" * 50)
    print("ğŸ“ ç¤ºä¾‹1: ç®€å•æ–‡ä»¶å¤¹çŸ¥è¯†åº“æ„å»º")
    print("=" * 50)

    # åˆ›å»ºç®€å•æ„å»ºå™¨
    builder = FolderKnowledgeBaseBuilder(builder_type="minimal", output_dir="workdir/simple_kb")

    # æ„å»ºçŸ¥è¯†åº“
    result = await builder.build_knowledge_base(
        folder_path="examples/documents", graph_name="simple_folder_kb", recursive=True  # å‡è®¾æœ‰ä¸€ä¸ªæ–‡æ¡£æ–‡ä»¶å¤¹
    )

    if result["graph"]:
        graph = result["graph"]
        print("âœ… ç®€å•çŸ¥è¯†åº“æ„å»ºæˆåŠŸ:")
        print(f"   - å®ä½“æ•°é‡: {len(graph.entities)}")
        print(f"   - å…³ç³»æ•°é‡: {len(graph.relations)}")
    else:
        print(f"âŒ æ„å»ºå¤±è´¥: {result['message']}")

    # æ‰“å°ç»Ÿè®¡
    builder.print_statistics()


async def example_advanced_folder():
    """ç¤ºä¾‹2: é«˜çº§æ–‡ä»¶å¤¹å¤„ç†"""
    print("\n" + "=" * 50)
    print("ğŸš€ ç¤ºä¾‹2: é«˜çº§æ–‡ä»¶å¤¹çŸ¥è¯†åº“æ„å»º")
    print("=" * 50)

    # åˆ›å»ºé«˜çº§æ„å»ºå™¨
    builder = FolderKnowledgeBaseBuilder(builder_type="full", output_dir="workdir/advanced_kb")

    # æŒ‡å®šæ–‡ä»¶ç±»å‹è¿‡æ»¤
    file_patterns = ["*.pdf", "*.txt", "*.md", "*.docx"]

    # æ„å»ºçŸ¥è¯†åº“
    result = await builder.build_knowledge_base(
        folder_path="examples/documents",
        graph_name="advanced_folder_kb",
        recursive=True,
        file_patterns=file_patterns,
        chunk_size=2000,  # 2000å­—ç¬¦åˆ†å—
    )

    if result["graph"]:
        graph = result["graph"]
        print("âœ… é«˜çº§çŸ¥è¯†åº“æ„å»ºæˆåŠŸ:")
        print(f"   - å®ä½“æ•°é‡: {len(graph.entities)}")
        print(f"   - å…³ç³»æ•°é‡: {len(graph.relations)}")

        # æ˜¾ç¤ºä¸€äº›å®ä½“
        print("\nğŸ“‹ å®ä½“ç¤ºä¾‹:")
        for i, (entity_id, entity) in enumerate(list(graph.entities.items())[:5]):
            print(f"   {i+1}. {entity.name} ({entity.entity_type.value})")

        # å¦‚æœæ˜¯å…¨åŠŸèƒ½æ„å»ºå™¨ï¼Œè¿›è¡ŒéªŒè¯
        if hasattr(builder.builder, "validate_graph"):
            validation_result = await builder.builder.validate_graph(graph)
            print("\nğŸ” å›¾è°±éªŒè¯:")
            print(f"   - éªŒè¯é€šè¿‡: {validation_result.get('valid', False)}")
    else:
        print(f"âŒ æ„å»ºå¤±è´¥: {result['message']}")

    # æ‰“å°ç»Ÿè®¡
    builder.print_statistics()


async def example_batch_processing():
    """ç¤ºä¾‹3: æ‰¹é‡å¤„ç†å¤§æ–‡ä»¶å¤¹"""
    print("\n" + "=" * 50)
    print("ğŸ“¦ ç¤ºä¾‹3: æ‰¹é‡å¤„ç†å¤§æ–‡ä»¶å¤¹")
    print("=" * 50)

    # åˆ›å»ºæ‰¹é‡æ„å»ºå™¨
    builder = FolderKnowledgeBaseBuilder(builder_type="batch", output_dir="workdir/batch_kb")

    # æ„å»ºçŸ¥è¯†åº“
    result = await builder.build_knowledge_base(
        folder_path="examples/documents",
        graph_name="batch_folder_kb",
        recursive=True,
        chunk_size=1500,  # è¾ƒå°çš„åˆ†å—ä»¥æé«˜å¹¶è¡Œåº¦
    )

    if result["graph"]:
        graph = result["graph"]
        print("âœ… æ‰¹é‡çŸ¥è¯†åº“æ„å»ºæˆåŠŸ:")
        print(f"   - å®ä½“æ•°é‡: {len(graph.entities)}")
        print(f"   - å…³ç³»æ•°é‡: {len(graph.relations)}")
    else:
        print(f"âŒ æ„å»ºå¤±è´¥: {result['message']}")

    # æ‰“å°ç»Ÿè®¡
    builder.print_statistics()


async def example_create_sample_documents():
    """åˆ›å»ºç¤ºä¾‹æ–‡æ¡£æ–‡ä»¶å¤¹"""
    docs_dir = Path("examples/documents")
    docs_dir.mkdir(parents=True, exist_ok=True)

    # åˆ›å»ºä¸€äº›ç¤ºä¾‹æ–‡ä»¶
    sample_files = {
        "å…¬å¸ä»‹ç».txt": """
è…¾è®¯æ§è‚¡æœ‰é™å…¬å¸æ˜¯ä¸€å®¶ä¸­å›½è·¨å›½æŠ€æœ¯é›†å›¢å…¬å¸ã€‚
å…¬å¸æˆç«‹äº1998å¹´ï¼Œæ€»éƒ¨ä½äºæ·±åœ³ã€‚
ä¸»è¦ä¸šåŠ¡åŒ…æ‹¬ç¤¾äº¤ç½‘ç»œã€ç½‘ç»œæ¸¸æˆã€ç§»åŠ¨æ”¯ä»˜ã€äº‘è®¡ç®—ç­‰ã€‚
åˆ›å§‹äººåŒ…æ‹¬é©¬åŒ–è…¾ã€å¼ å¿—ä¸œã€è®¸æ™¨æ™”ã€é™ˆä¸€ä¸¹ã€æ›¾æé’ã€‚
        """,
        "äº§å“ä¿¡æ¯.md": """
# è…¾è®¯ä¸»è¦äº§å“

## å³æ—¶é€šè®¯
- å¾®ä¿¡ï¼šç§»åŠ¨å³æ—¶é€šè®¯åº”ç”¨
- QQï¼šæ¡Œé¢å’Œç§»åŠ¨å³æ—¶é€šè®¯è½¯ä»¶

## æ¸¸æˆ
- ç‹è€…è£è€€ï¼šç§»åŠ¨MOBAæ¸¸æˆ
- å’Œå¹³ç²¾è‹±ï¼šå¤§é€ƒæ€ç±»æ‰‹æ¸¸

## é‡‘èç§‘æŠ€
- å¾®ä¿¡æ”¯ä»˜ï¼šç§»åŠ¨æ”¯ä»˜å¹³å°
- QQé’±åŒ…ï¼šæ•°å­—é’±åŒ…æœåŠ¡
        """,
        "äººç‰©ç®€ä»‹.txt": """
é©¬åŒ–è…¾ï¼Œè…¾è®¯å…¬å¸ä¸»è¦åˆ›åŠäººä¹‹ä¸€ï¼Œç°ä»»è…¾è®¯å…¬å¸è‘£äº‹ä¼šä¸»å¸­å…¼é¦–å¸­æ‰§è¡Œå®˜ã€‚
1971å¹´å‡ºç”Ÿäºå¹¿ä¸œçœæ±•å¤´å¸‚ã€‚
1993å¹´æ¯•ä¸šäºæ·±åœ³å¤§å­¦è®¡ç®—æœºç³»ã€‚
1998å¹´ä¸åŒå­¦å¼ å¿—ä¸œç­‰äººåˆ›ç«‹è…¾è®¯å…¬å¸ã€‚
        """,
        "æŠ€æœ¯ä¿¡æ¯.txt": """
è…¾è®¯äº‘æ˜¯è…¾è®¯å€¾åŠ›æ‰“é€ çš„é¢å‘äº‘æ—¶ä»£çš„æ™ºèƒ½äº§ä¸šå…±åŒä½“ã€‚
æä¾›äº‘è®¡ç®—ã€å¤§æ•°æ®ã€äººå·¥æ™ºèƒ½ç­‰æŠ€æœ¯æœåŠ¡ã€‚
æœåŠ¡èŒƒå›´æ¶µç›–æ¸¸æˆã€è§†é¢‘ã€é‡‘èã€é›¶å”®ã€æ•™è‚²ç­‰å¤šä¸ªè¡Œä¸šã€‚
åœ¨å…¨çƒ27ä¸ªåœ°ç†åŒºåŸŸå†…è¿è¥ç€70ä¸ªå¯ç”¨åŒºã€‚
        """,
    }

    for filename, content in sample_files.items():
        file_path = docs_dir / filename
        with open(file_path, "w", encoding="utf-8") as f:
            f.write(content.strip())

    print(f"âœ… ç¤ºä¾‹æ–‡æ¡£å·²åˆ›å»ºåœ¨: {docs_dir}")
    print(f"   åˆ›å»ºäº† {len(sample_files)} ä¸ªç¤ºä¾‹æ–‡ä»¶")


async def main():
    """è¿è¡Œæ‰€æœ‰ç¤ºä¾‹"""
    print("ğŸ¯ æ–‡ä»¶å¤¹çŸ¥è¯†åº“æ„å»ºç¤ºä¾‹")
    print("æ¼”ç¤ºå¦‚ä½•ä»æ–‡ä»¶å¤¹æ„å»ºçŸ¥è¯†åº“")

    # é¦–å…ˆåˆ›å»ºç¤ºä¾‹æ–‡æ¡£
    await example_create_sample_documents()

    # è¿è¡Œå„ä¸ªç¤ºä¾‹
    await example_simple_folder()
    await example_advanced_folder()
    await example_batch_processing()

    print("\nğŸ‰ æ‰€æœ‰ç¤ºä¾‹è¿è¡Œå®Œæˆ!")

    # æ˜¾ç¤ºæ”¯æŒçš„æ–‡ä»¶ç±»å‹
    print("\nğŸ“‹ æ”¯æŒçš„æ–‡ä»¶ç±»å‹:")
    extensions = get_supported_extensions()
    for ext in sorted(extensions):
        print(f"   - {ext}")


if __name__ == "__main__":
    asyncio.run(main())
