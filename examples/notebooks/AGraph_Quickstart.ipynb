{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# AGraph å¿«é€Ÿå…¥é—¨æŒ‡å—\n\næœ¬æ•™ç¨‹å±•ç¤ºå¦‚ä½•ä½¿ç”¨AGraphæ„å»ºå’Œæ“ä½œçŸ¥è¯†å›¾è°±ã€‚\n\n## ç›®å½•\n1. [åŸºæœ¬è®¾ç½®](#åŸºæœ¬è®¾ç½®)\n2. [åˆ›å»ºçŸ¥è¯†å›¾è°±](#åˆ›å»ºçŸ¥è¯†å›¾è°±)\n3. [æ·»åŠ å®ä½“](#æ·»åŠ å®ä½“)\n4. [å»ºç«‹å…³ç³»](#å»ºç«‹å…³ç³»)\n5. [æŸ¥è¯¢å›¾è°±](#æŸ¥è¯¢å›¾è°±)\n6. [æ–‡æ¡£å¤„ç†](#æ–‡æ¡£å¤„ç†)\n7. [å®ä½“ä½ç½®è¿½è¸ª](#å®ä½“ä½ç½®è¿½è¸ª) â­ æ–°å¢\n8. [ä½ç½®å¯è§†åŒ–](#ä½ç½®å¯è§†åŒ–) â­ æ–°å¢\n9. [æ€§èƒ½ä¼˜åŒ–](#æ€§èƒ½ä¼˜åŒ–)\n10. [å®é™…åº”ç”¨ç¤ºä¾‹](#å®é™…åº”ç”¨ç¤ºä¾‹)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## åŸºæœ¬è®¾ç½®\n",
    "\n",
    "é¦–å…ˆå¯¼å…¥å¿…è¦çš„æ¨¡å—å¹¶é…ç½®ç¯å¢ƒï¼š"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-01T09:58:20.326317Z",
     "start_time": "2025-09-01T09:58:19.531137Z"
    }
   },
   "source": [
    "import os\n",
    "from agraph import AGraph, KnowledgeGraphBuilder\n",
    "from agraph.base.graphs.optimized import OptimizedKnowledgeGraph\n",
    "from agraph.base.models.entities import Entity\n",
    "from agraph.base.models.relations import Relation\n",
    "from agraph.base.models.text import TextChunk\n",
    "from agraph.base.core.types import EntityType, RelationType\n",
    "from agraph.config import get_settings\n",
    "\n",
    "# è®¾ç½®OpenAI API Keyï¼ˆå¦‚æœä½¿ç”¨LLMåŠŸèƒ½ï¼‰\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"your-api-key-here\"\n",
    "\n",
    "print(\"AGraphæ¨¡å—å¯¼å…¥æˆåŠŸï¼\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AGraphæ¨¡å—å¯¼å…¥æˆåŠŸï¼\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## åˆ›å»ºçŸ¥è¯†å›¾è°±\n",
    "\n",
    "ä½¿ç”¨ä¼˜åŒ–çš„çŸ¥è¯†å›¾è°±å®ç°ï¼Œè·å¾—æœ€ä½³æ€§èƒ½ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-01T09:58:27.728480Z",
     "start_time": "2025-09-01T09:58:27.723837Z"
    }
   },
   "source": [
    "# åˆ›å»ºä¼˜åŒ–çš„çŸ¥è¯†å›¾è°±å®ä¾‹\n",
    "kg = OptimizedKnowledgeGraph()\n",
    "\n",
    "# æŸ¥çœ‹å›¾è°±åˆå§‹çŠ¶æ€\n",
    "print(f\"å›¾è°±åˆå§‹åŒ–å®Œæˆ\")\n",
    "print(f\"å®ä½“æ•°é‡: {len(kg.entities)}\")\n",
    "print(f\"å…³ç³»æ•°é‡: {len(kg.relations)}\")\n",
    "print(f\"ç¼“å­˜çŠ¶æ€: {'å¯ç”¨' if kg.cache_manager else 'ç¦ç”¨'}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å›¾è°±åˆå§‹åŒ–å®Œæˆ\n",
      "å®ä½“æ•°é‡: 0\n",
      "å…³ç³»æ•°é‡: 0\n",
      "ç¼“å­˜çŠ¶æ€: å¯ç”¨\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## æ·»åŠ å®ä½“\n",
    "\n",
    "åˆ›å»ºä¸åŒç±»å‹çš„å®ä½“å¹¶æ·»åŠ åˆ°å›¾è°±ä¸­ï¼š"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-01T09:59:39.615370Z",
     "start_time": "2025-09-01T09:59:39.599480Z"
    }
   },
   "cell_type": "code",
   "source": "# åˆ›å»ºäººç‰©å®ä½“\nperson1 = Entity(\n    name=\"å¼ ä¸‰\",\n    entity_type=EntityType.PERSON,\n    description=\"è½¯ä»¶å·¥ç¨‹å¸ˆ\",\n    properties={\"èŒä½\": \"é«˜çº§å¼€å‘å·¥ç¨‹å¸ˆ\", \"æŠ€èƒ½\": [\"Python\", \"æœºå™¨å­¦ä¹ \"]}\n)\n\nperson2 = Entity(\n    name=\"æå››\",\n    entity_type=EntityType.PERSON,\n    description=\"æ•°æ®ç§‘å­¦å®¶\",\n    properties={\"èŒä½\": \"æ•°æ®ç§‘å­¦å®¶\", \"ä¸“é•¿\": [\"ç»Ÿè®¡å­¦\", \"æ·±åº¦å­¦ä¹ \"]}\n)\n\n# åˆ›å»ºç»„ç»‡å®ä½“\ncompany = Entity(\n    name=\"AIç§‘æŠ€å…¬å¸\",\n    entity_type=EntityType.ORGANIZATION,\n    description=\"ä¸“æ³¨äºäººå·¥æ™ºèƒ½æŠ€æœ¯çš„ç§‘æŠ€å…¬å¸\",\n    properties={\"è¡Œä¸š\": \"äººå·¥æ™ºèƒ½\", \"æˆç«‹å¹´ä»½\": 2020}\n)\n\n# åˆ›å»ºæ¦‚å¿µå®ä½“\nconcept = Entity(\n    name=\"çŸ¥è¯†å›¾è°±\",\n    entity_type=EntityType.CONCEPT,\n    description=\"ç”¨äºè¡¨ç¤ºçŸ¥è¯†çš„å›¾å½¢åŒ–æ•°æ®ç»“æ„\",\n    properties={\"é¢†åŸŸ\": \"äººå·¥æ™ºèƒ½\", \"åº”ç”¨åœºæ™¯\": [\"æœç´¢\", \"æ¨è\", \"é—®ç­”\"]}\n)\n\n# æ·»åŠ å®ä½“åˆ°å›¾è°±\nentities = [person1, person2, company, concept]\nfor entity in entities:\n    kg.add_entity(entity)\n\nprint(f\"å·²æ·»åŠ  {len(entities)} ä¸ªå®ä½“\")\nprint(f\"å½“å‰å›¾è°±åŒ…å« {len(kg.entities)} ä¸ªå®ä½“\")\n\n# æ˜¾ç¤ºæ·»åŠ çš„å®ä½“ - ä¿®å¤ï¼šæ­£ç¡®è¿­ä»£å®ä½“å­—å…¸çš„å€¼\nfor entity in kg.entities.values():  # ä½¿ç”¨ .values() è·å–å®ä½“å¯¹è±¡\n    print(f\"- {entity.name} ({entity.entity_type})\")",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å·²æ·»åŠ  4 ä¸ªå®ä½“\n",
      "å½“å‰å›¾è°±åŒ…å« 4 ä¸ªå®ä½“\n",
      "- å¼ ä¸‰ (person)\n",
      "- æå›› (person)\n",
      "- AIç§‘æŠ€å…¬å¸ (organization)\n",
      "- çŸ¥è¯†å›¾è°± (concept)\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## å»ºç«‹å…³ç³»\n",
    "\n",
    "åœ¨å®ä½“ä¹‹é—´åˆ›å»ºæœ‰æ„ä¹‰çš„å…³ç³»ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-01T10:02:53.616244Z",
     "start_time": "2025-09-01T10:02:53.553959Z"
    }
   },
   "source": [
    "# åˆ›å»ºå·¥ä½œå…³ç³»\n",
    "works_for_1 = Relation(\n",
    "    head_entity=person1,\n",
    "    tail_entity=company,\n",
    "    relation_type=RelationType.WORKS_FOR,\n",
    "    description=\"å¼ ä¸‰åœ¨AIç§‘æŠ€å…¬å¸å·¥ä½œ\",\n",
    "    properties={\"éƒ¨é—¨\": \"ç ”å‘éƒ¨\", \"å…¥èŒæ—¶é—´\": \"2021-03-01\"}\n",
    ")\n",
    "\n",
    "works_for_2 = Relation(\n",
    "    head_entity=person2,\n",
    "    tail_entity=company,\n",
    "    relation_type=RelationType.WORKS_FOR,\n",
    "    description=\"æå››åœ¨AIç§‘æŠ€å…¬å¸å·¥ä½œ\",\n",
    "    properties={\"éƒ¨é—¨\": \"æ•°æ®éƒ¨\", \"å…¥èŒæ—¶é—´\": \"2021-06-15\"}\n",
    ")\n",
    "\n",
    "# åˆ›å»ºæ¦‚å¿µå…³ç³»\n",
    "works_with = Relation(\n",
    "    head_entity=person1,\n",
    "    tail_entity=concept,\n",
    "    relation_type=custom_relation_type4.WORKS_WITH,\n",
    "    description=\"å¼ ä¸‰ä»äº‹çŸ¥è¯†å›¾è°±ç›¸å…³å·¥ä½œ\"\n",
    ")\n",
    "\n",
    "specializes_in = Relation(\n",
    "    head_entity=person2,\n",
    "    tail_entity=concept,\n",
    "    relation_type=custom_relation_type4.SPECIALIZES_IN,\n",
    "    description=\"æå››ä¸“é—¨ç ”ç©¶çŸ¥è¯†å›¾è°±\"\n",
    ")\n",
    "\n",
    "# åˆ›å»ºåˆä½œå…³ç³»\n",
    "collaborates = Relation(\n",
    "    head_entity=person1,\n",
    "    tail_entity=person2,\n",
    "    relation_type=custom_relation_type4.COLLABORATES_WITH,\n",
    "    description=\"å¼ ä¸‰å’Œæå››ç»å¸¸åˆä½œ\",\n",
    "    properties={\"é¡¹ç›®\": [\"æ™ºèƒ½é—®ç­”ç³»ç»Ÿ\", \"æ¨èå¼•æ“\"]}\n",
    ")\n",
    "\n",
    "# æ·»åŠ å…³ç³»åˆ°å›¾è°±\n",
    "relations = [works_for_1, works_for_2, works_with, specializes_in, collaborates]\n",
    "for relation in relations:\n",
    "    kg.add_relation(relation)\n",
    "\n",
    "print(f\"å·²æ·»åŠ  {len(relations)} ä¸ªå…³ç³»\")\n",
    "print(f\"å½“å‰å›¾è°±åŒ…å« {len(kg.relations)} ä¸ªå…³ç³»\")\n",
    "\n",
    "# æ˜¾ç¤ºå…³ç³»\n",
    "for relation in kg.relations:\n",
    "    head_name = relation.head_entity.name if relation.head_entity else \"æœªçŸ¥\"\n",
    "    tail_name = relation.tail_entity.name if relation.tail_entity else \"æœªçŸ¥\"\n",
    "    print(f\"- {head_name} --[{relation.relation_type}]--> {tail_name}\")"
   ],
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "<enum 'WORKS_FOR'> cannot extend <enum 'RelationType'>",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mTypeError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[5]\u001B[39m\u001B[32m, line 1\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m1\u001B[39m custom_relation_type = \u001B[43mRelationType\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mWORKS_FOR\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43må·¥ä½œäº\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m      2\u001B[39m custom_relation_type2 = RelationType(\u001B[33m\"\u001B[39m\u001B[33mWORKS_WITH\u001B[39m\u001B[33m\"\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33mä»äº‹äº\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m      3\u001B[39m custom_relation_type3 = RelationType(\u001B[33m\"\u001B[39m\u001B[33mSPECIALIZES_IN\u001B[39m\u001B[33m\"\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33mä¸“é—¨ç ”ç©¶\u001B[39m\u001B[33m\"\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/homebrew/Caskroom/miniconda/base/lib/python3.11/enum.py:713\u001B[39m, in \u001B[36mEnumType.__call__\u001B[39m\u001B[34m(cls, value, names, module, qualname, type, start, boundary)\u001B[39m\n\u001B[32m    711\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mcls\u001B[39m.\u001B[34m__new__\u001B[39m(\u001B[38;5;28mcls\u001B[39m, value)\n\u001B[32m    712\u001B[39m \u001B[38;5;66;03m# otherwise, functional API: we're creating a new Enum type\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m713\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mcls\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_create_\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    714\u001B[39m \u001B[43m        \u001B[49m\u001B[43mvalue\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    715\u001B[39m \u001B[43m        \u001B[49m\u001B[43mnames\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    716\u001B[39m \u001B[43m        \u001B[49m\u001B[43mmodule\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmodule\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    717\u001B[39m \u001B[43m        \u001B[49m\u001B[43mqualname\u001B[49m\u001B[43m=\u001B[49m\u001B[43mqualname\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    718\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43mtype\u001B[39;49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mtype\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m    719\u001B[39m \u001B[43m        \u001B[49m\u001B[43mstart\u001B[49m\u001B[43m=\u001B[49m\u001B[43mstart\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    720\u001B[39m \u001B[43m        \u001B[49m\u001B[43mboundary\u001B[49m\u001B[43m=\u001B[49m\u001B[43mboundary\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    721\u001B[39m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/homebrew/Caskroom/miniconda/base/lib/python3.11/enum.py:852\u001B[39m, in \u001B[36mEnumType._create_\u001B[39m\u001B[34m(cls, class_name, names, module, qualname, type, start, boundary)\u001B[39m\n\u001B[32m    850\u001B[39m metacls = \u001B[38;5;28mcls\u001B[39m.\u001B[34m__class__\u001B[39m\n\u001B[32m    851\u001B[39m bases = (\u001B[38;5;28mcls\u001B[39m, ) \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mtype\u001B[39m \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m (\u001B[38;5;28mtype\u001B[39m, \u001B[38;5;28mcls\u001B[39m)\n\u001B[32m--> \u001B[39m\u001B[32m852\u001B[39m _, first_enum = \u001B[38;5;28;43mcls\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_get_mixins_\u001B[49m\u001B[43m(\u001B[49m\u001B[43mclass_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbases\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    853\u001B[39m classdict = metacls.\u001B[34m__prepare__\u001B[39m(class_name, bases)\n\u001B[32m    855\u001B[39m \u001B[38;5;66;03m# special processing needed for names?\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/homebrew/Caskroom/miniconda/base/lib/python3.11/enum.py:950\u001B[39m, in \u001B[36mEnumType._get_mixins_\u001B[39m\u001B[34m(mcls, class_name, bases)\u001B[39m\n\u001B[32m    947\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m bases:\n\u001B[32m    948\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mobject\u001B[39m, Enum\n\u001B[32m--> \u001B[39m\u001B[32m950\u001B[39m \u001B[43mmcls\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_check_for_existing_members_\u001B[49m\u001B[43m(\u001B[49m\u001B[43mclass_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbases\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    952\u001B[39m \u001B[38;5;66;03m# ensure final parent class is an Enum derivative, find any concrete\u001B[39;00m\n\u001B[32m    953\u001B[39m \u001B[38;5;66;03m# data type, and check that Enum has no members\u001B[39;00m\n\u001B[32m    954\u001B[39m first_enum = bases[-\u001B[32m1\u001B[39m]\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/homebrew/Caskroom/miniconda/base/lib/python3.11/enum.py:934\u001B[39m, in \u001B[36mEnumType._check_for_existing_members_\u001B[39m\u001B[34m(mcls, class_name, bases)\u001B[39m\n\u001B[32m    932\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m base \u001B[38;5;129;01min\u001B[39;00m chain.\u001B[34m__mro__\u001B[39m:\n\u001B[32m    933\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(base, EnumType) \u001B[38;5;129;01mand\u001B[39;00m base._member_names_:\n\u001B[32m--> \u001B[39m\u001B[32m934\u001B[39m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\n\u001B[32m    935\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33m<enum \u001B[39m\u001B[38;5;132;01m%r\u001B[39;00m\u001B[33m> cannot extend \u001B[39m\u001B[38;5;132;01m%r\u001B[39;00m\u001B[33m\"\u001B[39m\n\u001B[32m    936\u001B[39m                 % (class_name, base)\n\u001B[32m    937\u001B[39m                 )\n",
      "\u001B[31mTypeError\u001B[39m: <enum 'WORKS_FOR'> cannot extend <enum 'RelationType'>"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## æŸ¥è¯¢å›¾è°±\n",
    "\n",
    "ä½¿ç”¨ä¸åŒçš„æŸ¥è¯¢æ–¹æ³•æ£€ç´¢ä¿¡æ¯ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. æŒ‰åç§°æŸ¥æ‰¾å®ä½“\n",
    "print(\"=== æŒ‰åç§°æŸ¥æ‰¾å®ä½“ ===\")\n",
    "zhang_san = kg.get_entity_by_name(\"å¼ ä¸‰\")\n",
    "if zhang_san:\n",
    "    print(f\"æ‰¾åˆ°å®ä½“: {zhang_san.name} - {zhang_san.description}\")\n",
    "    print(f\"å±æ€§: {zhang_san.properties}\")\n",
    "\n",
    "# 2. æŒ‰ç±»å‹æŸ¥æ‰¾å®ä½“\n",
    "print(\"\\n=== æŒ‰ç±»å‹æŸ¥æ‰¾å®ä½“ ===\")\n",
    "persons = kg.get_entities_by_type(EntityType.PERSON)\n",
    "print(f\"æ‰¾åˆ° {len(persons)} ä¸ªäººç‰©å®ä½“:\")\n",
    "for person in persons:\n",
    "    print(f\"  - {person.name}: {person.description}\")\n",
    "\n",
    "# 3. æŸ¥æ‰¾å®ä½“çš„å…³ç³»\n",
    "print(\"\\n=== æŸ¥æ‰¾å®ä½“å…³ç³» ===\")\n",
    "if zhang_san:\n",
    "    relations = kg.get_entity_relations(zhang_san.id)\n",
    "    print(f\"{zhang_san.name} çš„å…³ç³»:\")\n",
    "    for rel in relations:\n",
    "        if rel.head_entity and rel.tail_entity:\n",
    "            other_entity = rel.tail_entity if rel.head_entity.id == zhang_san.id else rel.head_entity\n",
    "            print(f\"  - ä¸ {other_entity.name} çš„å…³ç³»: {rel.relation_type}\")\n",
    "\n",
    "# 4. æŒ‰å…³ç³»ç±»å‹æŸ¥æ‰¾\n",
    "print(\"\\n=== æŒ‰å…³ç³»ç±»å‹æŸ¥æ‰¾ ===\")\n",
    "work_relations = kg.get_relations_by_type(RelationType.WORKS_FOR)\n",
    "print(f\"æ‰¾åˆ° {len(work_relations)} ä¸ªå·¥ä½œå…³ç³»:\")\n",
    "for rel in work_relations:\n",
    "    if rel.head_entity and rel.tail_entity:\n",
    "        print(f\"  - {rel.head_entity.name} åœ¨ {rel.tail_entity.name} å·¥ä½œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## æ–‡æ¡£å¤„ç†\n",
    "\n",
    "ä½¿ç”¨AGraphè‡ªåŠ¨ä»æ–‡æ¡£ä¸­æå–å®ä½“å’Œå…³ç³»ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å‡†å¤‡ç¤ºä¾‹æ–‡æ¡£\n",
    "sample_documents = [\n",
    "    \"\"\"è‹¹æœå…¬å¸æ˜¯ä¸€å®¶ç¾å›½è·¨å›½ç§‘æŠ€å…¬å¸ï¼Œæ€»éƒ¨ä½äºåŠ åˆ©ç¦å°¼äºšå·åº“æ¯”è’‚è¯ºã€‚\n",
    "    è’‚å§†Â·åº“å…‹æ˜¯è‹¹æœå…¬å¸çš„é¦–å¸­æ‰§è¡Œå®˜ï¼Œä»–åœ¨2011å¹´æ¥æ›¿å²è’‚å¤«Â·ä¹”å¸ƒæ–¯æˆä¸ºCEOã€‚\n",
    "    è‹¹æœå…¬å¸ä¸»è¦è®¾è®¡ã€å¼€å‘å’Œé”€å”®æ¶ˆè´¹ç”µå­äº§å“ã€è®¡ç®—æœºè½¯ä»¶å’Œåœ¨çº¿æœåŠ¡ã€‚\"\"\",\n",
    "    \n",
    "    \"\"\"å¾®è½¯å…¬å¸æˆç«‹äº1975å¹´ï¼Œç”±æ¯”å°”Â·ç›–èŒ¨å’Œä¿ç½—Â·è‰¾ä¼¦åˆ›ç«‹ã€‚\n",
    "    è¨è’‚äºšÂ·çº³å¾·æ‹‰ç°ä»»å¾®è½¯å…¬å¸é¦–å¸­æ‰§è¡Œå®˜ï¼Œäº2014å¹´ä¸Šä»»ã€‚\n",
    "    å¾®è½¯ä¸“æ³¨äºå¼€å‘æ“ä½œç³»ç»Ÿã€åŠå…¬è½¯ä»¶å’Œäº‘è®¡ç®—æœåŠ¡ã€‚\"\"\",\n",
    "    \n",
    "    \"\"\"äººå·¥æ™ºèƒ½æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œæ—¨åœ¨åˆ›å»ºèƒ½å¤Ÿæ‰§è¡Œé€šå¸¸éœ€è¦äººç±»æ™ºèƒ½çš„ä»»åŠ¡çš„ç³»ç»Ÿã€‚\n",
    "    æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªå­é›†ï¼Œä¸“æ³¨äºç®—æ³•å’Œç»Ÿè®¡æ¨¡å‹çš„åº”ç”¨ã€‚\n",
    "    æ·±åº¦å­¦ä¹ åˆæ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªå­é›†ï¼Œä½¿ç”¨ç¥ç»ç½‘ç»œæ¥æ¨¡æ‹Ÿäººè„‘çš„å­¦ä¹ è¿‡ç¨‹ã€‚\"\"\"\n",
    "]\n",
    "\n",
    "# åˆ›å»ºæ–‡æœ¬å—\n",
    "text_chunks = []\n",
    "for i, doc in enumerate(sample_documents):\n",
    "    chunk = TextChunk(\n",
    "        content=doc,\n",
    "        source=f\"document_{i+1}\",\n",
    "        metadata={\"document_type\": \"sample\", \"language\": \"zh\"}\n",
    "    )\n",
    "    text_chunks.append(chunk)\n",
    "\n",
    "print(f\"å‡†å¤‡äº† {len(text_chunks)} ä¸ªæ–‡æ¡£å—\")\n",
    "for i, chunk in enumerate(text_chunks):\n",
    "    print(f\"æ–‡æ¡£ {i+1}: {chunk.content[:50]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "source": "# ä½ç½®è¿½è¸ªçš„å®ç”¨å·¥å…·å‡½æ•°\nprint(\"=== å®ç”¨ä½ç½®å·¥å…·æ¼”ç¤º ===\")\n\ndef find_entities_near_position(entities, target_pos, radius=50):\n    \"\"\"æŸ¥æ‰¾æŒ‡å®šä½ç½®é™„è¿‘çš„å®ä½“\"\"\"\n    nearby_entities = []\n    for entity in entities:\n        char_pos = entity.get_char_position()\n        if char_pos:\n            start_pos, end_pos = char_pos\n            center_pos = (start_pos + end_pos) // 2\n            if abs(center_pos - target_pos) <= radius:\n                distance = abs(center_pos - target_pos)\n                nearby_entities.append((entity, distance))\n    \n    # æŒ‰è·ç¦»æ’åº\n    nearby_entities.sort(key=lambda x: x[1])\n    return nearby_entities\n\ndef create_entity_timeline(entities, text):\n    \"\"\"åˆ›å»ºå®ä½“åœ¨æ–‡æ¡£ä¸­çš„æ—¶é—´çº¿è§†å›¾\"\"\"\n    positioned = [(e, e.get_char_position()) for e in entities if e.has_position()]\n    positioned.sort(key=lambda x: x[1][0])  # æŒ‰å¼€å§‹ä½ç½®æ’åº\n    \n    timeline = []\n    for entity, (start_pos, end_pos) in positioned:\n        # è·å–ä¸Šä¸‹æ–‡\n        context_start = max(0, start_pos - 20)\n        context_end = min(len(text), end_pos + 20)\n        context = text[context_start:context_end].replace('\\n', ' ')\n        \n        timeline.append({\n            'entity': entity.name,\n            'type': entity.entity_type.value,\n            'position': (start_pos, end_pos),\n            'context': context,\n            'confidence': entity.get_position_confidence()\n        })\n    \n    return timeline\n\n# 1. æŸ¥æ‰¾ç‰¹å®šä½ç½®é™„è¿‘çš„å®ä½“\nprint(\"1. ä½ç½®é™„è¿‘å®ä½“æŸ¥è¯¢:\")\ntarget_position = 50  # ç¬¬50ä¸ªå­—ç¬¦é™„è¿‘\nnearby = find_entities_near_position(positioned_entities, target_position, radius=30)\n\nprint(f\"   ä½ç½® {target_position} é™„è¿‘30ä¸ªå­—ç¬¦å†…çš„å®ä½“:\")\nfor entity, distance in nearby:\n    print(f\"     - {entity.name}: è·ç¦» {distance} å­—ç¬¦\")\n\nif not nearby:\n    print(f\"     (æ— å®ä½“åœ¨æŒ‡å®šèŒƒå›´å†…)\")\n\n# 2. åˆ›å»ºå®ä½“æ—¶é—´çº¿\nprint(f\"\\n2. å®ä½“æ–‡æ¡£æ—¶é—´çº¿:\")\ntimeline = create_entity_timeline(positioned_entities, test_document)\n\nfor i, item in enumerate(timeline, 1):\n    print(f\"   {i}. ã€{item['type']}ã€‘{item['entity']}\")\n    print(f\"      ä½ç½®: {item['position']}\")\n    print(f\"      ç½®ä¿¡åº¦: {item['confidence']:.2f}\")\n    print(f\"      ä¸Šä¸‹æ–‡: ...{item['context']}...\")\n    print()\n\n# 3. ä½ç½®è´¨é‡è¯„ä¼°\nprint(\"3. ä½ç½®è´¨é‡è¯„ä¼°:\")\nalignment_counts = {}\ntotal_confidence = 0\nconfidence_count = 0\n\nfor entity in positioned_entities:\n    if entity.has_position():\n        status = entity.get_alignment_status()\n        alignment_counts[status] = alignment_counts.get(status, 0) + 1\n        \n        confidence = entity.get_position_confidence()\n        total_confidence += confidence\n        confidence_count += 1\n\nprint(f\"   å¯¹é½çŠ¶æ€åˆ†å¸ƒ:\")\nfor status, count in alignment_counts.items():\n    print(f\"     - {status.value}: {count} ä¸ªå®ä½“\")\n\nif confidence_count > 0:\n    avg_confidence = total_confidence / confidence_count\n    print(f\"   å¹³å‡ç½®ä¿¡åº¦: {avg_confidence:.3f}\")\n    \nprint(f\"\\nâœ… å®ä½“ä½ç½®è¿½è¸ªåŠŸèƒ½æ¼”ç¤ºå®Œæˆï¼\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# ä½¿ç”¨AGraphè‡ªåŠ¨æå–å’Œå®šä½ (å¦‚æœLLMå¯ç”¨)\nprint(\"=== AGraphè‡ªåŠ¨æå–å’Œå®šä½æ¼”ç¤º ===\")\n\ntry:\n    # åˆ›å»ºAGraphå®ä¾‹è¿›è¡Œè‡ªåŠ¨æå–\n    agraph = AGraph(\n        collection_name=\"positioning_demo\",\n        persist_directory=\"./temp_agraph\",\n        enable_knowledge_graph=True\n    )\n    \n    await agraph.initialize()\n    \n    print(\"1. ä½¿ç”¨AGraphè‡ªåŠ¨ä»æ–‡æ¡£æå–å®ä½“:\")\n    print(f\"   æ–‡æ¡£: æµ‹è¯•æ–‡æ¡£ ({len(test_document)} å­—ç¬¦)\")\n    \n    # ä»æ–‡æ¡£æ„å»ºçŸ¥è¯†å›¾è°±ï¼ˆè¿™ä¼šè‡ªåŠ¨æå–å®ä½“å¹¶è®¾ç½®ä½ç½®ï¼‰\n    auto_kg = await agraph.build_from_texts(\n        texts=[test_document],\n        graph_name=\"è‡ªåŠ¨å®šä½æ¼”ç¤º\",\n        use_cache=False,\n        save_to_vector_store=False\n    )\n    \n    print(f\"   è‡ªåŠ¨æå–ç»“æœ:\")\n    print(f\"     å®ä½“æ•°: {len(auto_kg.entities)}\")\n    print(f\"     å…³ç³»æ•°: {len(auto_kg.relations)}\")\n    print(f\"     æ–‡æœ¬å—æ•°: {len(auto_kg.text_chunks)}\")\n    \n    # æ£€æŸ¥è‡ªåŠ¨æå–çš„å®ä½“æ˜¯å¦æœ‰ä½ç½®ä¿¡æ¯\n    auto_positioned_entities = [e for e in auto_kg.entities.values() if e.has_position()]\n    print(f\"     æœ‰ä½ç½®ä¿¡æ¯çš„å®ä½“: {len(auto_positioned_entities)}\")\n    \n    if auto_positioned_entities:\n        print(f\"\\n2. è‡ªåŠ¨å®šä½çš„å®ä½“è¯¦æƒ…:\")\n        for i, entity in enumerate(auto_positioned_entities[:5], 1):\n            char_pos = entity.get_char_position()\n            if char_pos:\n                start_pos, end_pos = char_pos\n                actual_text = test_document[start_pos:end_pos]\n                print(f\"   {i}. {entity.name} ({entity.entity_type.value})\")\n                print(f\"      ä½ç½®: [{start_pos}-{end_pos}]\")\n                print(f\"      æ–‡æ¡£æ–‡æœ¬: '{actual_text}'\")\n                print(f\"      å¯¹é½çŠ¶æ€: {entity.get_alignment_status().value}\")\n                print()\n    \n    await agraph.close()\n    \nexcept Exception as e:\n    print(f\"è‡ªåŠ¨æå–åŠŸèƒ½æš‚ä¸å¯ç”¨: {e}\")\n    print(\"è¿™é€šå¸¸æ˜¯å› ä¸ºæœªé…ç½®LLM APIå¯†é’¥\")\n    print(\"æ‚¨å¯ä»¥è®¾ç½®ç¯å¢ƒå˜é‡ OPENAI_API_KEY æ¥å¯ç”¨è‡ªåŠ¨æå–åŠŸèƒ½\")\n    print(\"\\nç»§ç»­ä½¿ç”¨æ‰‹åŠ¨å®šä½çš„æ¼”ç¤ºç»“æœ...\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# åŸºäºä½ç½®çš„é«˜çº§åˆ†æ\nprint(\"=== åŸºäºä½ç½®çš„é«˜çº§åˆ†æ ===\")\n\n# 1. ä½ç½®æ¥è¿‘åº¦åˆ†æ\ndef calculate_proximity_score(entity1, entity2, max_distance=100):\n    \"\"\"è®¡ç®—ä¸¤ä¸ªå®ä½“çš„ä½ç½®æ¥è¿‘åº¦åˆ†æ•°\"\"\"\n    pos1 = entity1.get_char_position()\n    pos2 = entity2.get_char_position()\n    \n    if not (pos1 and pos2):\n        return 0.0\n    \n    # è®¡ç®—å®ä½“é—´çš„æœ€å°è·ç¦»\n    start1, end1 = pos1\n    start2, end2 = pos2\n    \n    # å¦‚æœé‡å ï¼Œè·ç¦»ä¸º0\n    if start1 < end2 and start2 < end1:\n        distance = 0\n    else:\n        # è®¡ç®—éé‡å å®ä½“é—´çš„è·ç¦»\n        distance = min(abs(start1 - end2), abs(start2 - end1))\n    \n    # è½¬æ¢ä¸ºæ¥è¿‘åº¦åˆ†æ•° (è·ç¦»è¶Šè¿‘åˆ†æ•°è¶Šé«˜)\n    proximity = max(0, (max_distance - distance) / max_distance)\n    return proximity\n\nprint(\"1. å®ä½“æ¥è¿‘åº¦åˆ†æ:\")\nproximity_pairs = []\n\nfor i in range(len(positioned_entities)):\n    for j in range(i+1, len(positioned_entities)):\n        entity1, entity2 = positioned_entities[i], positioned_entities[j]\n        proximity = calculate_proximity_score(entity1, entity2)\n        \n        if proximity > 0.1:  # åªæ˜¾ç¤ºæ¥è¿‘åº¦ > 0.1 çš„å®ä½“å¯¹\n            proximity_pairs.append((entity1, entity2, proximity))\n\n# æŒ‰æ¥è¿‘åº¦æ’åº\nproximity_pairs.sort(key=lambda x: x[2], reverse=True)\n\nfor entity1, entity2, proximity in proximity_pairs[:5]:  # æ˜¾ç¤ºå‰5ä¸ªæœ€æ¥è¿‘çš„\n    pos1 = entity1.get_char_position()\n    pos2 = entity2.get_char_position()\n    print(f\"  â€¢ {entity1.name} â†” {entity2.name}\")\n    print(f\"    ä½ç½®: {pos1} â†” {pos2}\")\n    print(f\"    æ¥è¿‘åº¦: {proximity:.3f}\")\n    print()\n\n# 2. æ–‡æ¡£æ®µè½å®ä½“å¯†åº¦åˆ†æ  \nprint(\"2. æ–‡æ¡£æ®µè½å®ä½“å¯†åº¦:\")\nparagraphs = test_document.split('\\n\\n')\ncurrent_pos = 0\n\nfor i, paragraph in enumerate(paragraphs):\n    if paragraph.strip():  # å¿½ç•¥ç©ºæ®µè½\n        para_start = current_pos\n        para_end = current_pos + len(paragraph)\n        \n        # ç»Ÿè®¡è¯¥æ®µè½ä¸­çš„å®ä½“\n        entities_in_para = []\n        for entity in positioned_entities:\n            char_pos = entity.get_char_position()\n            if char_pos:\n                start_pos, end_pos = char_pos\n                # æ£€æŸ¥å®ä½“æ˜¯å¦åœ¨å½“å‰æ®µè½èŒƒå›´å†…\n                if start_pos >= para_start and end_pos <= para_end:\n                    entities_in_para.append(entity)\n        \n        entity_density = len(entities_in_para) / len(paragraph) if paragraph else 0\n        \n        print(f\"  æ®µè½ {i+1}: ä½ç½® [{para_start}-{para_end}]\")\n        print(f\"    é•¿åº¦: {len(paragraph)} å­—ç¬¦\")\n        print(f\"    å®ä½“æ•°: {len(entities_in_para)}\")\n        print(f\"    å®ä½“å¯†åº¦: {entity_density:.4f} (å®ä½“/å­—ç¬¦)\")\n        \n        if entities_in_para:\n            entity_names = [e.name for e in entities_in_para]\n            print(f\"    åŒ…å«å®ä½“: {', '.join(entity_names)}\")\n        print()\n        \n        current_pos += len(paragraph) + 2  # +2 for '\\n\\n'",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# äº¤äº’å¼ä½ç½®å¯¼èˆª\nprint(\"\\n=== äº¤äº’å¼ä½ç½®å¯¼èˆªæ¼”ç¤º ===\")\n\ndef show_entity_context(text, entity, context_chars=50):\n    \"\"\"æ˜¾ç¤ºå®ä½“åŠå…¶ä¸Šä¸‹æ–‡\"\"\"\n    if not entity.has_position():\n        return f\"å®ä½“ '{entity.name}' æ— ä½ç½®ä¿¡æ¯\"\n    \n    char_pos = entity.get_char_position()\n    if not char_pos:\n        return f\"å®ä½“ '{entity.name}' ä½ç½®ä¿¡æ¯æ— æ•ˆ\"\n    \n    start_pos, end_pos = char_pos\n    \n    # è®¡ç®—ä¸Šä¸‹æ–‡èŒƒå›´\n    context_start = max(0, start_pos - context_chars)\n    context_end = min(len(text), end_pos + context_chars)\n    \n    # æå–ä¸Šä¸‹æ–‡\n    before_text = text[context_start:start_pos]\n    entity_text = text[start_pos:end_pos]\n    after_text = text[end_pos:context_end]\n    \n    return {\n        'before': before_text,\n        'entity': entity_text,\n        'after': after_text,\n        'position': char_pos,\n        'alignment': entity.get_alignment_status().value,\n        'confidence': entity.get_position_confidence()\n    }\n\n# ä¸ºæ¯ä¸ªå®šä½å®ä½“æ˜¾ç¤ºä¸Šä¸‹æ–‡\nprint(\"2. å®ä½“ä¸Šä¸‹æ–‡å¯¼èˆª:\")\nfor i, entity in enumerate(positioned_entities[:5], 1):  # æ˜¾ç¤ºå‰5ä¸ª\n    context = show_entity_context(test_document, entity, context_chars=30)\n    \n    if isinstance(context, dict):\n        print(f\"\\n  {i}. ğŸ“ {entity.name} ({entity.entity_type.value})\")\n        print(f\"     ä½ç½®: {context['position']}\")\n        print(f\"     å¯¹é½: {context['alignment']} (ç½®ä¿¡åº¦: {context['confidence']:.2f})\")\n        print(f\"     ä¸Šä¸‹æ–‡: ...{context['before']}ã€{context['entity']}ã€‘{context['after']}...\")\n    else:\n        print(f\"\\n  {i}. âŒ {entity.name}: {context}\")\n\n# 3. ä½ç½®åˆ†å¸ƒå¯è§†åŒ–\nprint(f\"\\n3. å®ä½“ä½ç½®åˆ†å¸ƒ:\")\nprint(\"   æ–‡æ¡£ç»“æ„æ˜ å°„ (æ¯10ä¸ªå­—ç¬¦ä¸€ä¸ªå•ä½):\")\n\n# åˆ›å»ºç®€å•çš„ä½ç½®åˆ†å¸ƒå›¾\ndoc_length = len(test_document)\nunits = doc_length // 10 + 1\nposition_map = [' ' for _ in range(units)]\n\nfor entity in positioned_entities:\n    char_pos = entity.get_char_position()\n    if char_pos:\n        start_pos, end_pos = char_pos\n        unit_pos = start_pos // 10\n        if unit_pos < len(position_map):\n            # ä½¿ç”¨å®ä½“ç±»å‹çš„é¦–å­—æ¯æ ‡è®°\n            type_markers = {\n                EntityType.PERSON: 'P',\n                EntityType.PRODUCT: 'R', \n                EntityType.SOFTWARE: 'S',\n                EntityType.CONCEPT: 'C'\n            }\n            marker = type_markers.get(entity.entity_type, 'X')\n            position_map[unit_pos] = marker\n\n# æ˜¾ç¤ºä½ç½®åˆ†å¸ƒ\nprint(\"   \" + \"\".join(position_map))\nprint(\"   \" + \"\".join([str(i) if i % 5 == 0 else 'Â·' for i in range(units)]))\nprint(\"   å›¾ä¾‹: P=äººç‰©, R=äº§å“, S=è½¯ä»¶, C=æ¦‚å¿µ\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# æ–‡æœ¬é«˜äº®å¯è§†åŒ–\nprint(\"=== æ–‡æœ¬é«˜äº®å¯è§†åŒ– ===\")\n\ndef create_highlighted_text(text, entities, show_html=False):\n    \"\"\"åˆ›å»ºå¸¦æœ‰å®ä½“é«˜äº®çš„æ–‡æœ¬æ˜¾ç¤º\"\"\"\n    # æŒ‰å®ä½“ç±»å‹åˆ†é…é¢œè‰²\n    entity_colors = {\n        EntityType.PERSON: \"ğŸŸ¦\",      # è“è‰²ä»£è¡¨äººç‰©\n        EntityType.PRODUCT: \"ğŸŸ©\",     # ç»¿è‰²ä»£è¡¨äº§å“\n        EntityType.SOFTWARE: \"ğŸŸ¨\",    # é»„è‰²ä»£è¡¨è½¯ä»¶\n        EntityType.CONCEPT: \"ğŸŸª\",     # ç´«è‰²ä»£è¡¨æ¦‚å¿µ\n        EntityType.ORGANIZATION: \"ğŸŸ§\" # æ©™è‰²ä»£è¡¨ç»„ç»‡\n    }\n    \n    # åˆ›å»ºä½ç½®ç‚¹åˆ—è¡¨\n    position_points = []\n    for i, entity in enumerate(entities):\n        if entity.has_position():\n            char_pos = entity.get_char_position()\n            if char_pos:\n                start_pos, end_pos = char_pos\n                position_points.append({\n                    'pos': start_pos,\n                    'type': 'start',\n                    'entity': entity,\n                    'index': i\n                })\n                position_points.append({\n                    'pos': end_pos,\n                    'type': 'end',\n                    'entity': entity,\n                    'index': i\n                })\n    \n    # æŒ‰ä½ç½®æ’åº\n    position_points.sort(key=lambda x: (x['pos'], x['type'] == 'start'))\n    \n    # ç”Ÿæˆé«˜äº®æ–‡æœ¬\n    highlighted_parts = []\n    last_pos = 0\n    \n    for point in position_points:\n        pos = point['pos']\n        \n        # æ·»åŠ ç‚¹ä¹‹å‰çš„æ–‡æœ¬\n        if pos > last_pos:\n            highlighted_parts.append(text[last_pos:pos])\n        \n        # æ·»åŠ æ ‡è®°\n        if point['type'] == 'start':\n            entity = point['entity']\n            color = entity_colors.get(entity.entity_type, \"â¬œ\")\n            if show_html:\n                highlighted_parts.append(f'<mark style=\"background-color: {color};\">')\n            else:\n                highlighted_parts.append(f'{color}[')\n        else:\n            if show_html:\n                highlighted_parts.append('</mark>')\n            else:\n                highlighted_parts.append(']')\n        \n        last_pos = pos\n    \n    # æ·»åŠ å‰©ä½™æ–‡æœ¬\n    if last_pos < len(text):\n        highlighted_parts.append(text[last_pos:])\n    \n    return ''.join(highlighted_parts)\n\n# åˆ›å»ºé«˜äº®æ–‡æœ¬\nhighlighted_text = create_highlighted_text(test_document, positioned_entities)\n\nprint(\"1. æ–‡æœ¬é«˜äº®å±•ç¤º (ä½¿ç”¨è¡¨æƒ…ç¬¦å·æ ‡è®°):\")\nprint(\"   å›¾ä¾‹: ğŸŸ¦äººç‰© ğŸŸ©äº§å“ ğŸŸ¨è½¯ä»¶ ğŸŸªæ¦‚å¿µ\")\nprint(\"-\" * 60)\nprint(highlighted_text)\nprint(\"-\" * 60)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## ä½ç½®å¯è§†åŒ–\n\nåŸºäºå®ä½“çš„ç²¾ç¡®ä½ç½®ä¿¡æ¯ï¼Œæˆ‘ä»¬å¯ä»¥åˆ›å»ºå¯Œæœ‰æ´å¯ŸåŠ›çš„å¯è§†åŒ–å±•ç¤ºã€‚",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# ä½ç½®ä¿¡æ¯çš„åºåˆ—åŒ–å’ŒæŒä¹…åŒ–\nprint(\"=== ä½ç½®åºåˆ—åŒ–æ¼”ç¤º ===\")\n\n# é€‰æ‹©ä¸€ä¸ªå®ä½“è¿›è¡Œè¯¦ç»†æ¼”ç¤º\nif positioned_entities:\n    demo_entity = positioned_entities[0]\n    print(f\"æ¼”ç¤ºå®ä½“: {demo_entity.name}\")\n    \n    # 1. åºåˆ—åŒ–å®ä½“ï¼ˆåŒ…å«ä½ç½®ä¿¡æ¯ï¼‰\n    entity_dict = demo_entity.to_dict()\n    print(f\"\\n1. å®ä½“åºåˆ—åŒ–:\")\n    print(f\"   åŒ…å«ä½ç½®ä¿¡æ¯: {'position' in entity_dict}\")\n    \n    if 'position' in entity_dict:\n        position_data = entity_dict['position']\n        print(f\"   ä½ç½®æ•°æ®: {position_data}\")\n        \n        # 2. ååºåˆ—åŒ–éªŒè¯\n        restored_entity = Entity.from_dict(entity_dict)\n        print(f\"\\n2. ååºåˆ—åŒ–éªŒè¯:\")\n        print(f\"   ä½ç½®æ¢å¤æˆåŠŸ: {restored_entity.has_position()}\")\n        print(f\"   ä½ç½®æ•°æ®ä¸€è‡´: {restored_entity.get_char_position() == demo_entity.get_char_position()}\")\n        \n        # 3. JSONæ ¼å¼å±•ç¤º\n        import json\n        position_json = json.dumps(position_data, ensure_ascii=False, indent=2)\n        print(f\"\\n3. JSONæ ¼å¼ä½ç½®æ•°æ®:\")\n        print(position_json)\n\n# 4. æ‰¹é‡ä½ç½®ç»Ÿè®¡\nprint(f\"\\n=== ä½ç½®ç»Ÿè®¡ä¿¡æ¯ ===\")\ntotal_positioned = sum(1 for e in positioned_entities if e.has_position())\nprecise_positioned = sum(1 for e in positioned_entities if e.has_precise_position())\n\nprint(f\"æ€»å®ä½“æ•°: {len(positioned_entities)}\")\nprint(f\"æœ‰ä½ç½®ä¿¡æ¯çš„å®ä½“: {total_positioned}\")\nprint(f\"ç²¾ç¡®å®šä½çš„å®ä½“: {precise_positioned}\")\nprint(f\"å®šä½è¦†ç›–ç‡: {total_positioned/len(positioned_entities)*100:.1f}%\")\nprint(f\"ç²¾ç¡®å®šä½ç‡: {precise_positioned/total_positioned*100:.1f}%\" if total_positioned > 0 else \"ç²¾ç¡®å®šä½ç‡: N/A\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# ä½ç½®ä¿¡æ¯åˆ†æå’ŒæŸ¥è¯¢\nprint(\"=== ä½ç½®ä¿¡æ¯åˆ†æ ===\")\n\n# 1. æŒ‰ä½ç½®æ’åºæ˜¾ç¤ºå®ä½“\nprint(\"\\n1. æŒ‰æ–‡æ¡£ä½ç½®é¡ºåºæ˜¾ç¤ºå®ä½“:\")\nsorted_entities = sorted(positioned_entities, key=lambda e: e.get_char_position()[0] if e.get_char_position() else 0)\n\nfor i, entity in enumerate(sorted_entities, 1):\n    char_pos = entity.get_char_position()\n    alignment_status = entity.get_alignment_status()\n    confidence = entity.get_position_confidence()\n    \n    print(f\"  {i}. {entity.name} ({entity.entity_type.value})\")\n    print(f\"     ä½ç½®: {char_pos}\")\n    print(f\"     å¯¹é½çŠ¶æ€: {alignment_status.value}\")\n    print(f\"     ç½®ä¿¡åº¦: {confidence:.2f}\")\n    print()\n\n# 2. æŸ¥æ‰¾ç‰¹å®šèŒƒå›´å†…çš„å®ä½“\nprint(\"2. èŒƒå›´æŸ¥è¯¢ - å‰100ä¸ªå­—ç¬¦å†…çš„å®ä½“:\")\nentities_in_range = []\nfor entity in positioned_entities:\n    char_pos = entity.get_char_position()\n    if char_pos and char_pos[0] < 100:  # å®ä½“å¼€å§‹ä½ç½®åœ¨å‰100å­—ç¬¦å†…\n        entities_in_range.append(entity)\n\nfor entity in entities_in_range:\n    char_pos = entity.get_char_position()\n    print(f\"  - {entity.name}: ä½ç½® {char_pos}\")\n\n# 3. é‡å æ£€æµ‹\nprint(f\"\\n3. å®ä½“é‡å æ£€æµ‹:\")\noverlap_count = 0\nfor i in range(len(positioned_entities)):\n    for j in range(i+1, len(positioned_entities)):\n        if positioned_entities[i].overlaps_with(positioned_entities[j]):\n            overlap_count += 1\n            pos1 = positioned_entities[i].get_char_position()\n            pos2 = positioned_entities[j].get_char_position()\n            print(f\"  é‡å : {positioned_entities[i].name} {pos1} â†” {positioned_entities[j].name} {pos2}\")\n\nif overlap_count == 0:\n    print(\"  æ— å®ä½“é‡å \")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# æ‰‹åŠ¨åˆ›å»ºå¸¦ä½ç½®ä¿¡æ¯çš„å®ä½“\nprint(\"=== æ‰‹åŠ¨å®ä½“å®šä½æ¼”ç¤º ===\")\n\n# åœ¨æ–‡æ¡£ä¸­æŸ¥æ‰¾å®ä½“ä½ç½®\nentities_to_locate = [\n    (\"æ™ºèƒ½æ¨èç³»ç»Ÿ\", EntityType.PRODUCT),\n    (\"å¼ ä¸‰åšå£«\", EntityType.PERSON), \n    (\"æå››\", EntityType.PERSON),\n    (\"ç‹äº”\", EntityType.PERSON),\n    (\"Python\", EntityType.SOFTWARE),\n    (\"TensorFlow\", EntityType.SOFTWARE),\n    (\"æ·±åº¦å­¦ä¹ \", EntityType.CONCEPT)\n]\n\npositioned_entities = []\n\nprint(\"åœ¨æ–‡æ¡£ä¸­å®šä½å®ä½“:\")\nfor entity_name, entity_type in entities_to_locate:\n    # æŸ¥æ‰¾å®ä½“åœ¨æ–‡æ¡£ä¸­çš„ä½ç½®\n    start_pos = test_document.find(entity_name)\n    if start_pos != -1:\n        end_pos = start_pos + len(entity_name)\n        \n        # åˆ›å»ºå¸¦ä½ç½®ä¿¡æ¯çš„å®ä½“\n        entity = Entity(\n            name=entity_name,\n            entity_type=entity_type,\n            description=f\"ä»æ–‡æ¡£ä¸­æå–çš„{entity_type.value}\"\n        )\n        \n        # è®¾ç½®ç²¾ç¡®ä½ç½®\n        entity.set_char_position(\n            start_pos, \n            end_pos, \n            AlignmentStatus.MATCH_EXACT, \n            confidence=1.0\n        )\n        \n        positioned_entities.append(entity)\n        \n        # éªŒè¯ä½ç½®å‡†ç¡®æ€§\n        actual_text = test_document[start_pos:end_pos]\n        print(f\"  âœ“ {entity_name}: ä½ç½® [{start_pos}-{end_pos}], æ–‡æœ¬ '{actual_text}'\")\n    else:\n        print(f\"  âœ— {entity_name}: æœªåœ¨æ–‡æ¡£ä¸­æ‰¾åˆ°\")\n\nprint(f\"\\næˆåŠŸå®šä½ {len(positioned_entities)} ä¸ªå®ä½“\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# æ¼”ç¤ºå®ä½“ä½ç½®è¿½è¸ªåŠŸèƒ½\nfrom agraph.base.models.positioning import Position, CharInterval, AlignmentStatus\nfrom agraph.chunker import TokenChunker\n\nprint(\"=== å®ä½“ä½ç½®è¿½è¸ªæ¼”ç¤º ===\")\n\n# åˆ›å»ºåŒ…å«æ˜ç¡®å®ä½“çš„æµ‹è¯•æ–‡æ¡£\ntest_document = \"\"\"\næ•°æ®ç§‘å­¦é¡¹ç›®æŠ¥å‘Š\n\né¡¹ç›®åç§°ï¼šæ™ºèƒ½æ¨èç³»ç»Ÿ\nè´Ÿè´£äººï¼šå¼ ä¸‰åšå£«\nå›¢é˜Ÿæˆå‘˜ï¼šæå››ï¼ˆæœºå™¨å­¦ä¹ å·¥ç¨‹å¸ˆï¼‰ã€ç‹äº”ï¼ˆæ•°æ®å·¥ç¨‹å¸ˆï¼‰\né¡¹ç›®çŠ¶æ€ï¼šè¿›è¡Œä¸­\næŠ€æœ¯æ ˆï¼šPythonã€TensorFlowã€Redis\n\né¡¹ç›®æè¿°ï¼š\næœ¬é¡¹ç›®æ—¨åœ¨ä¸ºç”µå•†å¹³å°å¼€å‘æ™ºèƒ½æ¨èç®—æ³•ã€‚ç³»ç»Ÿä½¿ç”¨æ·±åº¦å­¦ä¹ æ¨¡å‹åˆ†æç”¨æˆ·è¡Œä¸ºæ•°æ®ï¼Œ\næä¾›ä¸ªæ€§åŒ–å•†å“æ¨èã€‚ç›®å‰å·²å®Œæˆæ•°æ®é¢„å¤„ç†æ¨¡å—å’Œæ¨¡å‹è®­ç»ƒæ¡†æ¶ã€‚\n\nä¸‹ä¸€æ­¥è®¡åˆ’ï¼š\n1. å®Œå–„æ¨èç®—æ³•ä¼˜åŒ–\n2. éƒ¨ç½²åˆ°ç”Ÿäº§ç¯å¢ƒ\n3. è¿›è¡ŒA/Bæµ‹è¯•éªŒè¯æ•ˆæœ\n\"\"\"\n\nprint(f\"æµ‹è¯•æ–‡æ¡£é•¿åº¦: {len(test_document)} å­—ç¬¦\")\nprint(f\"æ–‡æ¡£å†…å®¹é¢„è§ˆ: {test_document[:100]}...\")\n\n# ä½¿ç”¨åˆ†å—å™¨è·å–ä½ç½®ä¿¡æ¯\nchunker = TokenChunker(chunk_size=200, chunk_overlap=50)\nchunks_with_positions = chunker.split_text_with_positions(test_document)\n\nprint(f\"\\næ–‡æ¡£åˆ†å—ç»“æœ: {len(chunks_with_positions)} ä¸ªå—\")\nfor i, (chunk_text, start_pos, end_pos) in enumerate(chunks_with_positions):\n    print(f\"  å— {i+1}: ä½ç½® [{start_pos}-{end_pos}], é•¿åº¦ {end_pos-start_pos}\")\n    print(f\"    å†…å®¹é¢„è§ˆ: {chunk_text[:60]}...\")\n    print()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## å®ä½“ä½ç½®è¿½è¸ª\n\nAGraph æ”¯æŒç²¾ç¡®è¿½è¸ªæ¯ä¸ªå®ä½“åœ¨æºæ–‡æ¡£ä¸­çš„ä½ç½®ï¼Œè¿™å¯¹äºå¯è§†åŒ–ã€éªŒè¯å’Œäº¤äº’å¼åˆ†æéå¸¸æœ‰ç”¨ã€‚",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä½¿ç”¨KnowledgeGraphBuilderè‡ªåŠ¨æ„å»ºå›¾è°±\n",
    "# æ³¨æ„ï¼šè¿™éœ€è¦é…ç½®OpenAI APIå¯†é’¥\n",
    "try:\n",
    "    builder = KnowledgeGraphBuilder()\n",
    "    \n",
    "    # æ£€æŸ¥æ˜¯å¦é…ç½®äº†APIå¯†é’¥\n",
    "    settings = get_settings()\n",
    "    if not settings.openai_api_key:\n",
    "        print(\"âš ï¸  æœªé…ç½®OpenAI APIå¯†é’¥ï¼Œè·³è¿‡è‡ªåŠ¨æŠ½å–æ¼”ç¤º\")\n",
    "        print(\"è¦å¯ç”¨è‡ªåŠ¨æŠ½å–åŠŸèƒ½ï¼Œè¯·è®¾ç½®ç¯å¢ƒå˜é‡ OPENAI_API_KEY\")\n",
    "    else:\n",
    "        print(\"å¼€å§‹è‡ªåŠ¨å®ä½“å’Œå…³ç³»æŠ½å–...\")\n",
    "        \n",
    "        # æ„å»ºçŸ¥è¯†å›¾è°±\n",
    "        built_kg = await builder.build_from_chunks(text_chunks)\n",
    "        \n",
    "        print(f\"è‡ªåŠ¨æŠ½å–å®Œæˆï¼\")\n",
    "        print(f\"æŠ½å–çš„å®ä½“æ•°é‡: {len(built_kg.entities)}\")\n",
    "        print(f\"æŠ½å–çš„å…³ç³»æ•°é‡: {len(built_kg.relations)}\")\n",
    "        \n",
    "        # æ˜¾ç¤ºæŠ½å–çš„å®ä½“\n",
    "        print(\"\\næŠ½å–çš„å®ä½“:\")\n",
    "        for entity in built_kg.entities[:10]:  # æ˜¾ç¤ºå‰10ä¸ª\n",
    "            print(f\"  - {entity.name} ({entity.entity_type})\")\n",
    "        \n",
    "        # æ˜¾ç¤ºæŠ½å–çš„å…³ç³»\n",
    "        print(\"\\næŠ½å–çš„å…³ç³»:\")\n",
    "        for relation in built_kg.relations[:5]:  # æ˜¾ç¤ºå‰5ä¸ª\n",
    "            if relation.head_entity and relation.tail_entity:\n",
    "                print(f\"  - {relation.head_entity.name} --[{relation.relation_type}]--> {relation.tail_entity.name}\")\n",
    "        \n",
    "        # åˆå¹¶åˆ°ç°æœ‰å›¾è°±\n",
    "        kg.merge_graph(built_kg)\n",
    "        print(f\"\\nåˆå¹¶åå›¾è°±ç»Ÿè®¡:\")\n",
    "        print(f\"æ€»å®ä½“æ•°: {len(kg.entities)}\")\n",
    "        print(f\"æ€»å…³ç³»æ•°: {len(kg.relations)}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"è‡ªåŠ¨æŠ½å–è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}\")\n",
    "    print(\"ç»§ç»­ä½¿ç”¨æ‰‹åŠ¨åˆ›å»ºçš„å®ä½“å’Œå…³ç³»...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## é«˜çº§æŸ¥è¯¢å’Œåˆ†æ\n",
    "\n",
    "æ¼”ç¤ºå›¾è°±çš„é«˜çº§æŸ¥è¯¢åŠŸèƒ½ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. é‚»å±…æŸ¥è¯¢\n",
    "print(\"=== é‚»å±…æŸ¥è¯¢ ===\")\n",
    "if zhang_san:\n",
    "    neighbors = kg.get_neighbors(zhang_san.id)\n",
    "    print(f\"{zhang_san.name} çš„é‚»å±…å®ä½“:\")\n",
    "    for neighbor in neighbors:\n",
    "        print(f\"  - {neighbor.name} ({neighbor.entity_type})\")\n",
    "\n",
    "# 2. è·¯å¾„æŸ¥è¯¢\n",
    "print(\"\\n=== è·¯å¾„æŸ¥è¯¢ ===\")\n",
    "if zhang_san and person2:\n",
    "    paths = kg.find_shortest_path(zhang_san.id, person2.id)\n",
    "    if paths:\n",
    "        print(f\"ä» {zhang_san.name} åˆ° {person2.name} çš„æœ€çŸ­è·¯å¾„:\")\n",
    "        for i, entity_id in enumerate(paths):\n",
    "            entity = kg.get_entity(entity_id)\n",
    "            if entity:\n",
    "                print(f\"  {i+1}. {entity.name}\")\n",
    "    else:\n",
    "        print(f\"æœªæ‰¾åˆ°ä» {zhang_san.name} åˆ° {person2.name} çš„è·¯å¾„\")\n",
    "\n",
    "# 3. å±æ€§è¿‡æ»¤æŸ¥è¯¢\n",
    "print(\"\\n=== å±æ€§è¿‡æ»¤æŸ¥è¯¢ ===\")\n",
    "def find_entities_by_property(kg, property_key, property_value):\n",
    "    \"\"\"æ ¹æ®å±æ€§æŸ¥æ‰¾å®ä½“\"\"\"\n",
    "    matching_entities = []\n",
    "    for entity in kg.entities:\n",
    "        if property_key in entity.properties:\n",
    "            prop_value = entity.properties[property_key]\n",
    "            if (isinstance(prop_value, list) and property_value in prop_value) or prop_value == property_value:\n",
    "                matching_entities.append(entity)\n",
    "    return matching_entities\n",
    "\n",
    "# æŸ¥æ‰¾å…·æœ‰PythonæŠ€èƒ½çš„å®ä½“\n",
    "python_experts = find_entities_by_property(kg, \"æŠ€èƒ½\", \"Python\")\n",
    "print(f\"Pythonä¸“å®¶:\")\n",
    "for expert in python_experts:\n",
    "    print(f\"  - {expert.name}\")\n",
    "\n",
    "# 4. å›¾è°±ç»Ÿè®¡\n",
    "print(\"\\n=== å›¾è°±ç»Ÿè®¡ä¿¡æ¯ ===\")\n",
    "print(f\"å®ä½“ç»Ÿè®¡:\")\n",
    "entity_types = {}\n",
    "for entity in kg.entities:\n",
    "    entity_type = entity.entity_type\n",
    "    entity_types[entity_type] = entity_types.get(entity_type, 0) + 1\n",
    "\n",
    "for entity_type, count in entity_types.items():\n",
    "    print(f\"  - {entity_type}: {count}ä¸ª\")\n",
    "\n",
    "print(f\"\\nå…³ç³»ç»Ÿè®¡:\")\n",
    "relation_types = {}\n",
    "for relation in kg.relations:\n",
    "    relation_type = relation.relation_type\n",
    "    relation_types[relation_type] = relation_types.get(relation_type, 0) + 1\n",
    "\n",
    "for relation_type, count in relation_types.items():\n",
    "    print(f\"  - {relation_type}: {count}ä¸ª\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## æ€§èƒ½ä¼˜åŒ–\n",
    "\n",
    "å±•ç¤ºAGraphçš„æ€§èƒ½ä¼˜åŒ–ç‰¹æ€§ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from agraph.base.infrastructure.indexes import IndexManager\n",
    "\n",
    "# 1. ç´¢å¼•æ€§èƒ½æµ‹è¯•\n",
    "print(\"=== ç´¢å¼•æ€§èƒ½æµ‹è¯• ===\")\n",
    "\n",
    "# åˆ›å»ºå¤§é‡æµ‹è¯•å®ä½“\n",
    "print(\"åˆ›å»ºæµ‹è¯•å®ä½“...\")\n",
    "test_entities = []\n",
    "for i in range(1000):\n",
    "    entity = Entity(\n",
    "        name=f\"æµ‹è¯•å®ä½“_{i}\",\n",
    "        entity_type=EntityType.CONCEPT,\n",
    "        properties={\"æµ‹è¯•ID\": i, \"åˆ†ç±»\": f\"ç±»åˆ«_{i % 10}\"}\n",
    "    )\n",
    "    test_entities.append(entity)\n",
    "    kg.add_entity(entity)\n",
    "\n",
    "print(f\"å·²æ·»åŠ  {len(test_entities)} ä¸ªæµ‹è¯•å®ä½“\")\n",
    "print(f\"å›¾è°±ç°åœ¨åŒ…å« {len(kg.entities)} ä¸ªå®ä½“\")\n",
    "\n",
    "# æµ‹è¯•æŸ¥è¯¢æ€§èƒ½\n",
    "print(\"\\næµ‹è¯•æŸ¥è¯¢æ€§èƒ½...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# æ‰§è¡Œå¤šæ¬¡æŸ¥è¯¢\n",
    "for i in range(100):\n",
    "    entity = kg.get_entity_by_name(f\"æµ‹è¯•å®ä½“_{i * 10}\")\n",
    "    \n",
    "end_time = time.time()\n",
    "query_time = end_time - start_time\n",
    "\n",
    "print(f\"100æ¬¡æŸ¥è¯¢è€—æ—¶: {query_time:.4f}ç§’\")\n",
    "print(f\"å¹³å‡æ¯æ¬¡æŸ¥è¯¢: {query_time/100*1000:.2f}æ¯«ç§’\")\n",
    "\n",
    "# 2. ç¼“å­˜æ•ˆæœæµ‹è¯•\n",
    "print(\"\\n=== ç¼“å­˜æ•ˆæœæµ‹è¯• ===\")\n",
    "if kg.cache_manager:\n",
    "    cache_stats = kg.cache_manager.get_cache_stats()\n",
    "    print(f\"ç¼“å­˜å‘½ä¸­ç‡: {cache_stats.get('hit_rate', 0):.1%}\")\n",
    "    print(f\"ç¼“å­˜å¤§å°: {cache_stats.get('size', 0)}\")\n",
    "    print(f\"ç¼“å­˜å‘½ä¸­æ¬¡æ•°: {cache_stats.get('hits', 0)}\")\n",
    "    print(f\"ç¼“å­˜æœªå‘½ä¸­æ¬¡æ•°: {cache_stats.get('misses', 0)}\")\n",
    "else:\n",
    "    print(\"ç¼“å­˜ç®¡ç†å™¨æœªå¯ç”¨\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## å›¾è°±åºåˆ—åŒ–å’ŒæŒä¹…åŒ–\n",
    "\n",
    "æ¼”ç¤ºå¦‚ä½•ä¿å­˜å’ŒåŠ è½½çŸ¥è¯†å›¾è°±ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import tempfile\n",
    "import os\n",
    "\n",
    "# 1. åºåˆ—åŒ–å›¾è°±\n",
    "print(\"=== å›¾è°±åºåˆ—åŒ– ===\")\n",
    "graph_data = kg.to_dict()\n",
    "\n",
    "print(f\"åºåˆ—åŒ–æ•°æ®åŒ…å«:\")\n",
    "print(f\"  - å®ä½“: {len(graph_data.get('entities', []))}\")\n",
    "print(f\"  - å…³ç³»: {len(graph_data.get('relations', []))}\")\n",
    "print(f\"  - å…ƒæ•°æ®: {list(graph_data.get('metadata', {}).keys())}\")\n",
    "\n",
    "# 2. ä¿å­˜åˆ°æ–‡ä»¶\n",
    "with tempfile.NamedTemporaryFile(mode='w', suffix='.json', delete=False) as f:\n",
    "    json.dump(graph_data, f, ensure_ascii=False, indent=2)\n",
    "    temp_file = f.name\n",
    "\n",
    "print(f\"\\nå›¾è°±å·²ä¿å­˜åˆ°: {temp_file}\")\n",
    "print(f\"æ–‡ä»¶å¤§å°: {os.path.getsize(temp_file) / 1024:.1f} KB\")\n",
    "\n",
    "# 3. ä»æ–‡ä»¶åŠ è½½æ–°å›¾è°±\n",
    "print(\"\\n=== å›¾è°±åŠ è½½ ===\")\n",
    "new_kg = OptimizedKnowledgeGraph()\n",
    "\n",
    "with open(temp_file, 'r', encoding='utf-8') as f:\n",
    "    loaded_data = json.load(f)\n",
    "\n",
    "new_kg.from_dict(loaded_data)\n",
    "\n",
    "print(f\"ä»æ–‡ä»¶åŠ è½½çš„å›¾è°±:\")\n",
    "print(f\"  - å®ä½“æ•°é‡: {len(new_kg.entities)}\")\n",
    "print(f\"  - å…³ç³»æ•°é‡: {len(new_kg.relations)}\")\n",
    "\n",
    "# éªŒè¯æ•°æ®å®Œæ•´æ€§\n",
    "original_entity_names = {e.name for e in kg.entities}\n",
    "loaded_entity_names = {e.name for e in new_kg.entities}\n",
    "print(f\"  - æ•°æ®å®Œæ•´æ€§: {'âœ“' if original_entity_names == loaded_entity_names else 'âœ—'}\")\n",
    "\n",
    "# æ¸…ç†ä¸´æ—¶æ–‡ä»¶\n",
    "os.unlink(temp_file)\n",
    "print(f\"å·²æ¸…ç†ä¸´æ—¶æ–‡ä»¶\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## å®é™…åº”ç”¨ç¤ºä¾‹ï¼šæ„å»ºä¼ä¸šçŸ¥è¯†å›¾è°±\n",
    "\n",
    "å±•ç¤ºä¸€ä¸ªå®Œæ•´çš„ä¼ä¸šçŸ¥è¯†å›¾è°±æ„å»ºæµç¨‹ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆ›å»ºä¼ä¸šçŸ¥è¯†å›¾è°±\n",
    "enterprise_kg = OptimizedKnowledgeGraph()\n",
    "\n",
    "# å®šä¹‰ä¼ä¸šç»“æ„\n",
    "company_data = {\n",
    "    \"departments\": [\n",
    "        {\"name\": \"æŠ€æœ¯éƒ¨\", \"type\": \"éƒ¨é—¨\", \"è´Ÿè´£äºº\": \"ç‹ç»ç†\"},\n",
    "        {\"name\": \"äº§å“éƒ¨\", \"type\": \"éƒ¨é—¨\", \"è´Ÿè´£äºº\": \"æ—ç»ç†\"},\n",
    "        {\"name\": \"å¸‚åœºéƒ¨\", \"type\": \"éƒ¨é—¨\", \"è´Ÿè´£äºº\": \"é™ˆç»ç†\"}\n",
    "    ],\n",
    "    \"employees\": [\n",
    "        {\"name\": \"ç‹ç»ç†\", \"éƒ¨é—¨\": \"æŠ€æœ¯éƒ¨\", \"èŒä½\": \"æŠ€æœ¯æ€»ç›‘\", \"æŠ€èƒ½\": [\"æ¶æ„è®¾è®¡\", \"å›¢é˜Ÿç®¡ç†\"]},\n",
    "        {\"name\": \"æ—ç»ç†\", \"éƒ¨é—¨\": \"äº§å“éƒ¨\", \"èŒä½\": \"äº§å“æ€»ç›‘\", \"æŠ€èƒ½\": [\"äº§å“è§„åˆ’\", \"ç”¨æˆ·ç ”ç©¶\"]},\n",
    "        {\"name\": \"é™ˆç»ç†\", \"éƒ¨é—¨\": \"å¸‚åœºéƒ¨\", \"èŒä½\": \"å¸‚åœºæ€»ç›‘\", \"æŠ€èƒ½\": [\"å¸‚åœºåˆ†æ\", \"å“ç‰Œæ¨å¹¿\"]},\n",
    "        {\"name\": \"å°å¼ \", \"éƒ¨é—¨\": \"æŠ€æœ¯éƒ¨\", \"èŒä½\": \"åç«¯å·¥ç¨‹å¸ˆ\", \"æŠ€èƒ½\": [\"Python\", \"æ•°æ®åº“\"]},\n",
    "        {\"name\": \"å°æ\", \"éƒ¨é—¨\": \"æŠ€æœ¯éƒ¨\", \"èŒä½\": \"å‰ç«¯å·¥ç¨‹å¸ˆ\", \"æŠ€èƒ½\": [\"React\", \"Vue.js\"]}\n",
    "    ],\n",
    "    \"projects\": [\n",
    "        {\"name\": \"æ™ºèƒ½æ¨èç³»ç»Ÿ\", \"çŠ¶æ€\": \"è¿›è¡Œä¸­\", \"è´Ÿè´£éƒ¨é—¨\": \"æŠ€æœ¯éƒ¨\"},\n",
    "        {\"name\": \"ç”¨æˆ·ä½“éªŒä¼˜åŒ–\", \"çŠ¶æ€\": \"è§„åˆ’ä¸­\", \"è´Ÿè´£éƒ¨é—¨\": \"äº§å“éƒ¨\"},\n",
    "        {\"name\": \"å“ç‰Œé‡å¡‘\", \"çŠ¶æ€\": \"å·²å®Œæˆ\", \"è´Ÿè´£éƒ¨é—¨\": \"å¸‚åœºéƒ¨\"}\n",
    "    ]\n",
    "}\n",
    "\n",
    "# åˆ›å»ºå®ä½“\n",
    "entities_map = {}\n",
    "\n",
    "# åˆ›å»ºéƒ¨é—¨å®ä½“\n",
    "for dept_info in company_data[\"departments\"]:\n",
    "    dept = Entity(\n",
    "        name=dept_info[\"name\"],\n",
    "        entity_type=EntityType.ORGANIZATION,\n",
    "        description=f\"ä¼ä¸š{dept_info['type']}\",\n",
    "        properties={\"ç±»å‹\": dept_info[\"type\"], \"è´Ÿè´£äºº\": dept_info[\"è´Ÿè´£äºº\"]}\n",
    "    )\n",
    "    enterprise_kg.add_entity(dept)\n",
    "    entities_map[dept.name] = dept\n",
    "\n",
    "# åˆ›å»ºå‘˜å·¥å®ä½“\n",
    "for emp_info in company_data[\"employees\"]:\n",
    "    emp = Entity(\n",
    "        name=emp_info[\"name\"],\n",
    "        entity_type=EntityType.PERSON,\n",
    "        description=f\"{emp_info['èŒä½']}\",\n",
    "        properties={\n",
    "            \"éƒ¨é—¨\": emp_info[\"éƒ¨é—¨\"],\n",
    "            \"èŒä½\": emp_info[\"èŒä½\"],\n",
    "            \"æŠ€èƒ½\": emp_info[\"æŠ€èƒ½\"]\n",
    "        }\n",
    "    )\n",
    "    enterprise_kg.add_entity(emp)\n",
    "    entities_map[emp.name] = emp\n",
    "\n",
    "# åˆ›å»ºé¡¹ç›®å®ä½“\n",
    "for proj_info in company_data[\"projects\"]:\n",
    "    proj = Entity(\n",
    "        name=proj_info[\"name\"],\n",
    "        entity_type=EntityType.PROJECT,\n",
    "        description=f\"ä¼ä¸šé¡¹ç›®\",\n",
    "        properties={\n",
    "            \"çŠ¶æ€\": proj_info[\"çŠ¶æ€\"],\n",
    "            \"è´Ÿè´£éƒ¨é—¨\": proj_info[\"è´Ÿè´£éƒ¨é—¨\"]\n",
    "        }\n",
    "    )\n",
    "    enterprise_kg.add_entity(proj)\n",
    "    entities_map[proj.name] = proj\n",
    "\n",
    "print(f\"åˆ›å»ºäº† {len(entities_map)} ä¸ªä¼ä¸šå®ä½“\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å»ºç«‹ä¼ä¸šå…³ç³»\n",
    "print(\"=== å»ºç«‹ä¼ä¸šå…³ç³» ===\")\n",
    "\n",
    "# å‘˜å·¥ä¸éƒ¨é—¨çš„å…³ç³»\n",
    "for emp_info in company_data[\"employees\"]:\n",
    "    emp_entity = entities_map[emp_info[\"name\"]]\n",
    "    dept_entity = entities_map[emp_info[\"éƒ¨é—¨\"]]\n",
    "    \n",
    "    belongs_to = Relation(\n",
    "        head_entity=emp_entity,\n",
    "        tail_entity=dept_entity,\n",
    "        relation_type=RelationType.BELONGS_TO,\n",
    "        description=f\"{emp_info['name']}å±äº{emp_info['éƒ¨é—¨']}\",\n",
    "        properties={\"èŒä½\": emp_info[\"èŒä½\"]}\n",
    "    )\n",
    "    enterprise_kg.add_relation(belongs_to)\n",
    "\n",
    "# é¡¹ç›®ä¸éƒ¨é—¨çš„å…³ç³»\n",
    "for proj_info in company_data[\"projects\"]:\n",
    "    proj_entity = entities_map[proj_info[\"name\"]]\n",
    "    dept_entity = entities_map[proj_info[\"è´Ÿè´£éƒ¨é—¨\"]]\n",
    "    \n",
    "    managed_by = Relation(\n",
    "        head_entity=proj_entity,\n",
    "        tail_entity=dept_entity,\n",
    "        relation_type=RelationType.MANAGED_BY,\n",
    "        description=f\"{proj_info['name']}ç”±{proj_info['è´Ÿè´£éƒ¨é—¨']}è´Ÿè´£\",\n",
    "        properties={\"çŠ¶æ€\": proj_info[\"çŠ¶æ€\"]}\n",
    "    )\n",
    "    enterprise_kg.add_relation(managed_by)\n",
    "\n",
    "# ç®¡ç†å…³ç³»\n",
    "manager_relations = [\n",
    "    (\"ç‹ç»ç†\", \"å°å¼ \", \"ç®¡ç†\"),\n",
    "    (\"ç‹ç»ç†\", \"å°æ\", \"ç®¡ç†\")\n",
    "]\n",
    "\n",
    "for manager_name, employee_name, relation_desc in manager_relations:\n",
    "    if manager_name in entities_map and employee_name in entities_map:\n",
    "        manages = Relation(\n",
    "            head_entity=entities_map[manager_name],\n",
    "            tail_entity=entities_map[employee_name],\n",
    "            relation_type=RelationType.MANAGES,\n",
    "            description=f\"{manager_name}{relation_desc}{employee_name}\"\n",
    "        )\n",
    "        enterprise_kg.add_relation(manages)\n",
    "\n",
    "print(f\"ä¼ä¸šçŸ¥è¯†å›¾è°±æ„å»ºå®Œæˆï¼\")\n",
    "print(f\"æ€»å®ä½“æ•°: {len(enterprise_kg.entities)}\")\n",
    "print(f\"æ€»å…³ç³»æ•°: {len(enterprise_kg.relations)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ä¼ä¸šå›¾è°±åˆ†æ\n",
    "\n",
    "å¯¹æ„å»ºçš„ä¼ä¸šçŸ¥è¯†å›¾è°±è¿›è¡Œåˆ†æï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. ç»„ç»‡ç»“æ„åˆ†æ\n",
    "print(\"=== ç»„ç»‡ç»“æ„åˆ†æ ===\")\n",
    "\n",
    "# æŸ¥æ‰¾æ‰€æœ‰éƒ¨é—¨\n",
    "departments = [e for e in enterprise_kg.entities if e.entity_type == EntityType.ORGANIZATION and \"éƒ¨é—¨\" in e.name]\n",
    "print(f\"éƒ¨é—¨åˆ—è¡¨:\")\n",
    "for dept in departments:\n",
    "    print(f\"  - {dept.name}\")\n",
    "    \n",
    "    # æŸ¥æ‰¾éƒ¨é—¨å‘˜å·¥\n",
    "    dept_relations = enterprise_kg.get_entity_relations(dept.id)\n",
    "    employees_in_dept = []\n",
    "    for rel in dept_relations:\n",
    "        if rel.relation_type == RelationType.BELONGS_TO and rel.head_entity:\n",
    "            if rel.head_entity.entity_type == EntityType.PERSON:\n",
    "                employees_in_dept.append(rel.head_entity)\n",
    "    \n",
    "    print(f\"    å‘˜å·¥æ•°é‡: {len(employees_in_dept)}\")\n",
    "    for emp in employees_in_dept:\n",
    "        position = emp.properties.get(\"èŒä½\", \"æœªçŸ¥\")\n",
    "        print(f\"      - {emp.name} ({position})\")\n",
    "\n",
    "# 2. é¡¹ç›®åˆ†æ\n",
    "print(\"\\n=== é¡¹ç›®åˆ†æ ===\")\n",
    "projects = [e for e in enterprise_kg.entities if e.entity_type == EntityType.PROJECT]\n",
    "print(f\"é¡¹ç›®ç»Ÿè®¡:\")\n",
    "\n",
    "project_status = {}\n",
    "for proj in projects:\n",
    "    status = proj.properties.get(\"çŠ¶æ€\", \"æœªçŸ¥\")\n",
    "    project_status[status] = project_status.get(status, 0) + 1\n",
    "    print(f\"  - {proj.name}: {status}\")\n",
    "\n",
    "print(f\"\\né¡¹ç›®çŠ¶æ€ç»Ÿè®¡:\")\n",
    "for status, count in project_status.items():\n",
    "    print(f\"  - {status}: {count}ä¸ªé¡¹ç›®\")\n",
    "\n",
    "# 3. æŠ€èƒ½åˆ†æ\n",
    "print(\"\\n=== æŠ€èƒ½åˆ†æ ===\")\n",
    "all_skills = {}\n",
    "for entity in enterprise_kg.entities:\n",
    "    if entity.entity_type == EntityType.PERSON and \"æŠ€èƒ½\" in entity.properties:\n",
    "        skills = entity.properties[\"æŠ€èƒ½\"]\n",
    "        if isinstance(skills, list):\n",
    "            for skill in skills:\n",
    "                if skill not in all_skills:\n",
    "                    all_skills[skill] = []\n",
    "                all_skills[skill].append(entity.name)\n",
    "\n",
    "print(f\"æŠ€èƒ½åˆ†å¸ƒ:\")\n",
    "for skill, people in all_skills.items():\n",
    "    print(f\"  - {skill}: {', '.join(people)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## å›¾è°±å¯è§†åŒ–\n",
    "\n",
    "ä½¿ç”¨ç®€å•çš„æ–‡æœ¬æ ¼å¼å¯è§†åŒ–å›¾è°±ç»“æ„ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_graph_structure(kg, max_entities=20):\n",
    "    \"\"\"ç®€å•çš„å›¾è°±ç»“æ„å¯è§†åŒ–\"\"\"\n",
    "    print(\"=== å›¾è°±ç»“æ„å¯è§†åŒ– ===\")\n",
    "    \n",
    "    entities_to_show = list(kg.entities)[:max_entities]\n",
    "    \n",
    "    for entity in entities_to_show:\n",
    "        print(f\"\\nğŸ“ {entity.name} ({entity.entity_type})\")\n",
    "        \n",
    "        # æŸ¥æ‰¾ç›¸å…³å…³ç³»\n",
    "        entity_relations = kg.get_entity_relations(entity.id)\n",
    "        \n",
    "        # åˆ†ç±»æ˜¾ç¤ºå…³ç³»\n",
    "        outgoing = []\n",
    "        incoming = []\n",
    "        \n",
    "        for rel in entity_relations:\n",
    "            if rel.head_entity and rel.tail_entity:\n",
    "                if rel.head_entity.id == entity.id:\n",
    "                    outgoing.append((rel.relation_type, rel.tail_entity.name))\n",
    "                else:\n",
    "                    incoming.append((rel.relation_type, rel.head_entity.name))\n",
    "        \n",
    "        # æ˜¾ç¤ºå‡ºè¾¹\n",
    "        if outgoing:\n",
    "            print(f\"  å‡ºè¾¹:\")\n",
    "            for rel_type, target_name in outgoing[:3]:  # é™åˆ¶æ˜¾ç¤ºæ•°é‡\n",
    "                print(f\"    â””â”€[{rel_type}]â†’ {target_name}\")\n",
    "        \n",
    "        # æ˜¾ç¤ºå…¥è¾¹\n",
    "        if incoming:\n",
    "            print(f\"  å…¥è¾¹:\")\n",
    "            for rel_type, source_name in incoming[:3]:  # é™åˆ¶æ˜¾ç¤ºæ•°é‡\n",
    "                print(f\"    â†â”€[{rel_type}]â”€â”˜ {source_name}\")\n",
    "        \n",
    "        if not outgoing and not incoming:\n",
    "            print(f\"  (æ— å…³ç³»)\")\n",
    "\n",
    "# å¯è§†åŒ–ä¼ä¸šå›¾è°±\n",
    "visualize_graph_structure(enterprise_kg, max_entities=10)\n",
    "\n",
    "# å›¾è°±åº¦é‡ç»Ÿè®¡\n",
    "print(f\"\\n=== å›¾è°±åº¦é‡ ===\")\n",
    "total_entities = len(enterprise_kg.entities)\n",
    "total_relations = len(enterprise_kg.relations)\n",
    "avg_degree = (2 * total_relations) / total_entities if total_entities > 0 else 0\n",
    "\n",
    "print(f\"èŠ‚ç‚¹æ•°: {total_entities}\")\n",
    "print(f\"è¾¹æ•°: {total_relations}\")\n",
    "print(f\"å¹³å‡åº¦æ•°: {avg_degree:.2f}\")\n",
    "print(f\"å›¾å¯†åº¦: {2 * total_relations / (total_entities * (total_entities - 1)):.4f}\" if total_entities > 1 else \"å›¾å¯†åº¦: N/A\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## æ€»ç»“\n\næœ¬æ•™ç¨‹å±•ç¤ºäº†AGraphçš„æ ¸å¿ƒåŠŸèƒ½ï¼š\n\nâœ… **åŸºæœ¬æ“ä½œ**ï¼šåˆ›å»ºå®ä½“ã€å»ºç«‹å…³ç³»ã€æŸ¥è¯¢å›¾è°±  \nâœ… **é«˜çº§æŸ¥è¯¢**ï¼šé‚»å±…æŸ¥è¯¢ã€è·¯å¾„æŸ¥è¯¢ã€å±æ€§è¿‡æ»¤  \nâœ… **å®ä½“å®šä½**ï¼šç²¾ç¡®è¿½è¸ªå®ä½“åœ¨æºæ–‡æ¡£ä¸­çš„ä½ç½® â­ æ–°å¢  \nâœ… **ä½ç½®å¯è§†åŒ–**ï¼šæ–‡æœ¬é«˜äº®ã€ä½ç½®åˆ†å¸ƒã€ä¸Šä¸‹æ–‡å¯¼èˆª â­ æ–°å¢  \nâœ… **æ€§èƒ½ä¼˜åŒ–**ï¼šç´¢å¼•åŠ é€Ÿã€ç¼“å­˜æœºåˆ¶  \nâœ… **æ•°æ®æŒä¹…åŒ–**ï¼šåºåˆ—åŒ–å’Œååºåˆ—åŒ–ï¼ˆå«ä½ç½®ä¿¡æ¯ï¼‰  \nâœ… **å®é™…åº”ç”¨**ï¼šä¼ä¸šçŸ¥è¯†å›¾è°±æ„å»ºå’Œåˆ†æ  \n\n### å®ä½“å®šä½åŠŸèƒ½äº®ç‚¹\n\nğŸ¯ **ç²¾ç¡®å®šä½**ï¼šæ”¯æŒå­—ç¬¦çº§å’ŒTokençº§åŒé‡å®šä½  \nğŸ¯ **å¯¹é½çŠ¶æ€**ï¼šç²¾ç¡®åŒ¹é…ã€æ¨¡ç³ŠåŒ¹é…ã€éƒ¨åˆ†åŒ¹é…ç­‰çŠ¶æ€è¿½è¸ª  \nğŸ¯ **ä½ç½®æŸ¥è¯¢**ï¼šæŒ‰ä½ç½®èŒƒå›´æŸ¥æ‰¾å®ä½“ã€æ¥è¿‘åº¦åˆ†æ  \nğŸ¯ **å¯è§†åŒ–æ”¯æŒ**ï¼šæ–‡æœ¬é«˜äº®ã€å®ä½“åˆ†å¸ƒå›¾ã€ä¸Šä¸‹æ–‡å¯¼èˆª  \nğŸ¯ **æŒä¹…åŒ–**ï¼šä½ç½®ä¿¡æ¯å®Œæ•´çš„åºåˆ—åŒ–å’Œååºåˆ—åŒ–  \n\n### ä¸‹ä¸€æ­¥\n\n- æ¢ç´¢æ›´å¤šé«˜çº§åŠŸèƒ½ï¼šèšç±»åˆ†æã€å‘é‡æ£€ç´¢ã€å¯¹è¯ç³»ç»Ÿ\n- æŸ¥çœ‹ `AGraph_Advanced_Features.ipynb` äº†è§£é«˜çº§ç‰¹æ€§\n- å‚è€ƒ API æ–‡æ¡£äº†è§£æ›´å¤šé…ç½®é€‰é¡¹\n- åœ¨å®é™…é¡¹ç›®ä¸­åº”ç”¨ AGraph æ„å»ºä¸“ä¸šçŸ¥è¯†å›¾è°±\n\n### æ€§èƒ½æç¤º\n\n1. **ä½¿ç”¨ OptimizedKnowledgeGraph**ï¼šè·å¾—æœ€ä½³æŸ¥è¯¢æ€§èƒ½\n2. **å¯ç”¨ç¼“å­˜**ï¼šé¢‘ç¹æŸ¥è¯¢æ—¶æ˜¾è‘—æå‡æ€§èƒ½\n3. **æ‰¹é‡æ“ä½œ**ï¼šä¸€æ¬¡æ·»åŠ å¤šä¸ªå®ä½“æ¯”é€ä¸ªæ·»åŠ æ›´é«˜æ•ˆ\n4. **åˆç†ç´¢å¼•**ï¼šå……åˆ†åˆ©ç”¨å†…ç½®çš„7ç§ç´¢å¼•ç±»å‹\n5. **ä½ç½®ä¼˜åŒ–**ï¼šä½¿ç”¨ä½ç½®ä¿¡æ¯åŠ é€ŸèŒƒå›´æŸ¥è¯¢å’Œç›¸å…³æ€§åˆ†æ â­ æ–°å¢"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æœ€ç»ˆçš„å›¾è°±çŠ¶æ€\n",
    "print(\"=== æœ€ç»ˆå›¾è°±çŠ¶æ€ ===\")\n",
    "print(f\"ä¸»å›¾è°±å®ä½“æ•°: {len(kg.entities)}\")\n",
    "print(f\"ä¸»å›¾è°±å…³ç³»æ•°: {len(kg.relations)}\")\n",
    "print(f\"ä¼ä¸šå›¾è°±å®ä½“æ•°: {len(enterprise_kg.entities)}\")\n",
    "print(f\"ä¼ä¸šå›¾è°±å…³ç³»æ•°: {len(enterprise_kg.relations)}\")\n",
    "print(\"\\nğŸ‰ AGraphå¿«é€Ÿå…¥é—¨æ•™ç¨‹å®Œæˆï¼\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
