{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AGraph Advanced Features Demo\n",
    "\n",
    "This notebook demonstrates the advanced features of AGraph including:\n",
    "\n",
    "- 📊 **Knowledge Graph Construction** - Building graphs from text documents\n",
    "- 🎯 **Entity Positioning** - Precise location tracking in source documents  \n",
    "- 🔍 **Document Processing** - Multi-format document support\n",
    "- ⚡ **Performance Optimization** - Caching and indexing capabilities\n",
    "- 🔗 **Relationship Extraction** - Finding connections between entities\n",
    "- 📈 **Graph Analysis** - Clustering and community detection\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Make sure you have AGraph installed and configured:\n",
    "\n",
    "```bash\n",
    "pip install -e .\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 🚀 Basic Setup and Initialization"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-31T15:31:34.069256Z",
     "start_time": "2025-08-31T15:31:33.468852Z"
    }
   },
   "source": [
    "# Import AGraph and related modules\n",
    "from agraph import AGraph\n",
    "from agraph.base.models.entities import Entity\n",
    "from agraph.base.models.relations import Relation\n",
    "from agraph.base.models.text import TextChunk\n",
    "from agraph.base.models.positioning import Position, CharInterval, AlignmentStatus\n",
    "from agraph.base.core.types import EntityType, RelationType\n",
    "from agraph.processor.factory import DocumentProcessorFactory\n",
    "\n",
    "import tempfile\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"📦 AGraph modules imported successfully!\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📦 AGraph modules imported successfully!\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-31T15:31:34.698083Z",
     "start_time": "2025-08-31T15:31:34.693182Z"
    }
   },
   "source": "# Initialize AGraph with optimized performance settings\nagraph = AGraph(\n    persist_directory=\"./workdir\",\n    enable_cache=True,\n    cache_ttl=3600,  # 1 hour cache\n    enable_performance_mode=True\n)\n\nprint(f\"✅ AGraph initialized successfully!\")\nprint(f\"📁 Persist directory: {agraph.persist_directory}\")\nprint(f\"⚙️ Configuration: {agraph.config.cache_dir if agraph.config else 'Default config'}\")\nprint(f\"🔧 KG enabled: {agraph.enable_knowledge_graph}\")",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m2025-08-31 23:31:34.695\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36magraph.agraph\u001B[0m:\u001B[36m__init__\u001B[0m:\u001B[36m99\u001B[0m - \u001B[1mAGraph initialization completed, collection: agraph_knowledge, persist_dir: ./workdir, enable_kg: True\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ AGraph initialized successfully!\n",
      "📁 Persist directory: ./workdir\n",
      "⚙️ Configuration: ./workdir/cache\n",
      "🔧 KG enabled: True\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 🎯 Entity Positioning Features\n",
    "\n",
    "AGraph now supports precise entity positioning within source documents, enabling you to track exactly where each entity appears in the original text."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-31T15:31:36.422307Z",
     "start_time": "2025-08-31T15:31:36.419371Z"
    }
   },
   "source": "# Create a sample text chunk for processing\nsample_text = \"Apple Inc is a major technology company founded by Steve Jobs in California.\"\n\nprint(f\"📝 Sample text: {sample_text}\")\nprint(f\"📄 Text length: {len(sample_text)} characters\")\n\n# This text will be processed by AGraph to extract entities and relations\nprint(\"\\n🔍 This text contains entities like:\")\nprint(\"  • Apple Inc (Organization)\")\nprint(\"  • Steve Jobs (Person)\")  \nprint(\"  • California (Location)\")\nprint(\"  • And relations like 'founded by' and 'located in'\")",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📝 Sample text: Apple Inc is a major technology company founded by Steve Jobs in California.\n",
      "📄 Text length: 76 characters\n",
      "\n",
      "🔍 This text contains entities like:\n",
      "  • Apple Inc (Organization)\n",
      "  • Steve Jobs (Person)\n",
      "  • California (Location)\n",
      "  • And relations like 'founded by' and 'located in'\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-31T15:31:37.311302Z",
     "start_time": "2025-08-31T15:31:37.302746Z"
    }
   },
   "source": "# Build knowledge graph from the sample text\nprint(\"🏗️ Building knowledge graph from sample text...\")\n\n# Note: This requires OpenAI API key for automatic entity/relation extraction\ntry:\n    # Build knowledge graph from the text\n    knowledge_graph = await agraph.build_from_texts([sample_text])\n    \n    print(f\"✅ Knowledge graph built successfully!\")\n    print(f\"📊 Extracted entities: {len(knowledge_graph.entities)}\")\n    print(f\"📊 Extracted relations: {len(knowledge_graph.relations)}\")\n    \n    # Show extracted entities with positioning info\n    print(f\"\\n🎯 Extracted entities with positioning:\")\n    for entity in knowledge_graph.entities[:5]:  # Show first 5\n        print(f\"  • {entity.name} ({entity.entity_type})\")\n        if hasattr(entity, 'text_chunks') and entity.text_chunks:\n            print(f\"    Connected to {len(entity.text_chunks)} text chunks\")\n        if hasattr(entity, 'position') and entity.position:\n            print(f\"    Position info available: {entity.has_position()}\")\n            \nexcept Exception as e:\n    print(f\"⚠️ Automatic extraction failed: {e}\")\n    print(\"💡 This usually means OpenAI API key is not configured\")\n    print(\"   Set OPENAI_API_KEY environment variable to enable this feature\")\n    \n    # Create a manual demonstration instead\n    print(f\"\\n🔧 Creating manual demonstration...\")\n    \n    # For demo purposes, we'll work with OptimizedKnowledgeGraph directly\n    from agraph.base.graphs.optimized import OptimizedKnowledgeGraph\n    kg = OptimizedKnowledgeGraph()\n    \n    # Create manual entities to show positioning concepts\n    apple = Entity(\n        name=\"Apple Inc\",\n        entity_type=EntityType.ORGANIZATION,\n        description=\"Technology company\"\n    )\n    kg.add_entity(apple)\n    \n    print(f\"📝 Manual demo entity created: {apple.name}\")",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏗️ Building knowledge graph from sample text...\n",
      "⚠️ Automatic extraction failed: AGraph not initialized, please call initialize() first\n",
      "💡 This usually means OpenAI API key is not configured\n",
      "   Set OPENAI_API_KEY environment variable to enable this feature\n",
      "\n",
      "🔧 Creating manual demonstration...\n",
      "📝 Manual demo entity created: Apple Inc\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-31T15:31:38.506470Z",
     "start_time": "2025-08-31T15:31:38.324064Z"
    }
   },
   "source": "# Continue with the knowledge graph (either extracted or manual demo)\nif 'knowledge_graph' in locals():\n    # Use extracted knowledge graph\n    working_kg = knowledge_graph\n    print(f\"📊 Using extracted knowledge graph\")\nelse:\n    # Use manual demo knowledge graph\n    working_kg = kg\n    print(f\"📊 Using manual demo knowledge graph\")\n\nprint(f\"\\n📈 Current graph statistics:\")\nprint(f\"  • Entities: {len(working_kg.entities)}\")\nprint(f\"  • Relations: {len(working_kg.relations)}\")\n\n# Show sample entities\nprint(f\"\\n📋 Sample entities:\")\nfor entity in list(working_kg.entities)[:3]:\n    print(f\"  • {entity.name} ({entity.entity_type})\")\n    if hasattr(entity, 'description') and entity.description:\n        print(f\"    Description: {entity.description}\")\n    if hasattr(entity, 'properties') and entity.properties:\n        print(f\"    Properties: {entity.properties}\")",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Using manual demo knowledge graph\n",
      "\n",
      "📈 Current graph statistics:\n",
      "  • Entities: 1\n",
      "  • Relations: 0\n",
      "\n",
      "📋 Sample entities:\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'name'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mAttributeError\u001B[39m                            Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[5]\u001B[39m\u001B[32m, line 18\u001B[39m\n\u001B[32m     16\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m📋 Sample entities:\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m     17\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m entity \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mlist\u001B[39m(working_kg.entities)[:\u001B[32m3\u001B[39m]:\n\u001B[32m---> \u001B[39m\u001B[32m18\u001B[39m     \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m  • \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[43mentity\u001B[49m\u001B[43m.\u001B[49m\u001B[43mname\u001B[49m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m (\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mentity.entity_type\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m)\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m     19\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(entity, \u001B[33m'\u001B[39m\u001B[33mdescription\u001B[39m\u001B[33m'\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m entity.description:\n\u001B[32m     20\u001B[39m         \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m    Description: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mentity.description\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n",
      "\u001B[31mAttributeError\u001B[39m: 'str' object has no attribute 'name'"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 🔗 Relationship Creation with Positioning\n",
    "\n",
    "Create relationships between entities and track their positions in the source text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Demonstrate positioning concepts (for manual demo case)\nif 'kg' in locals():\n    print(\"🔗 Demonstrating positioning concepts:\")\n    \n    # Create additional entities to show relationships\n    steve_jobs = Entity(\n        name=\"Steve Jobs\",\n        entity_type=EntityType.PERSON,\n        description=\"Co-founder of Apple Inc\"\n    )\n    kg.add_entity(steve_jobs)\n    \n    california = Entity(\n        name=\"California\", \n        entity_type=EntityType.LOCATION,\n        description=\"US State\"\n    )\n    kg.add_entity(california)\n    \n    # Create relations\n    founded_relation = Relation(\n        head_entity=steve_jobs,\n        tail_entity=apple,\n        relation_type=RelationType.FOUNDED_BY,\n        description=\"Steve Jobs founded Apple Inc\"\n    )\n    kg.add_relation(founded_relation)\n    \n    located_relation = Relation(\n        head_entity=apple,\n        tail_entity=california,\n        relation_type=RelationType.LOCATED_IN, \n        description=\"Apple Inc is located in California\"\n    )\n    kg.add_relation(located_relation)\n    \n    print(f\"🔗 Relations created:\")\n    for relation in [founded_relation, located_relation]:\n        print(f\"  • {relation.head_entity.name} -> {relation.relation_type} -> {relation.tail_entity.name}\")\n        \nelse:\n    print(\"🔗 Using extracted relations from knowledge graph:\")\n    for relation in list(working_kg.relations)[:3]:\n        if relation.head_entity and relation.tail_entity:\n            print(f\"  • {relation.head_entity.name} -> {relation.relation_type} -> {relation.tail_entity.name}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 🔍 Document Processing Features\n",
    "\n",
    "Demonstrate multi-format document processing capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize document processor factory\n",
    "processor_factory = DocumentProcessorFactory()\n",
    "\n",
    "print(\"📄 Document Processing Capabilities:\")\n",
    "print(f\"Supported extensions: {processor_factory.get_supported_extensions()}\")\n",
    "print(f\"Total processors: {len(processor_factory._processors)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample documents for processing\n",
    "with tempfile.TemporaryDirectory() as temp_dir:\n",
    "    temp_path = Path(temp_dir)\n",
    "    \n",
    "    # Create a text file\n",
    "    text_file = temp_path / \"company_info.txt\"\n",
    "    text_content = \"\"\"\n",
    "Microsoft Corporation is a multinational technology company.\n",
    "Founded in 1975 by Bill Gates and Paul Allen in Albuquerque, New Mexico.\n",
    "The company is now headquartered in Redmond, Washington.\n",
    "Microsoft develops software, hardware, and cloud services.\n",
    "\"\"\".strip()\n",
    "    text_file.write_text(text_content)\n",
    "    \n",
    "    # Create a JSON file\n",
    "    json_file = temp_path / \"product_data.json\"\n",
    "    json_data = {\n",
    "        \"company\": \"Microsoft\",\n",
    "        \"products\": [\n",
    "            {\"name\": \"Windows\", \"type\": \"Operating System\", \"launched\": 1985},\n",
    "            {\"name\": \"Office\", \"type\": \"Productivity Suite\", \"launched\": 1990},\n",
    "            {\"name\": \"Azure\", \"type\": \"Cloud Platform\", \"launched\": 2010}\n",
    "        ],\n",
    "        \"headquarters\": \"Redmond, Washington\"\n",
    "    }\n",
    "    json_file.write_text(json.dumps(json_data, indent=2))\n",
    "    \n",
    "    # Process documents\n",
    "    print(\"\\n📄 Processing documents:\")\n",
    "    \n",
    "    # Process text file\n",
    "    text_processor = processor_factory.get_processor(text_file)\n",
    "    text_result = text_processor.process(text_file)\n",
    "    text_metadata = text_processor.extract_metadata(text_file)\n",
    "    \n",
    "    print(f\"\\n📝 Text file processed:\")\n",
    "    print(f\"  • Content length: {len(text_result)} characters\")\n",
    "    print(f\"  • Word count: {text_metadata.get('word_count', 'N/A')}\")\n",
    "    print(f\"  • Line count: {text_metadata.get('line_count', 'N/A')}\")\n",
    "    \n",
    "    # Process JSON file\n",
    "    json_processor = processor_factory.get_processor(json_file)\n",
    "    json_result = json_processor.process(json_file, pretty_print=True)\n",
    "    json_metadata = json_processor.extract_metadata(json_file)\n",
    "    \n",
    "    print(f\"\\n📊 JSON file processed:\")\n",
    "    print(f\"  • Data type: {json_metadata.get('data_type', 'N/A')}\")\n",
    "    print(f\"  • Key count: {json_metadata.get('key_count', 'N/A')}\")\n",
    "    print(f\"  • Max depth: {json_metadata.get('max_depth', 'N/A')}\")\n",
    "    \n",
    "    print(f\"\\n✨ Sample JSON content preview:\")\n",
    "    print(json_result[:200] + \"...\" if len(json_result) > 200 else json_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 🏗️ Automated Knowledge Graph Building\n",
    "\n",
    "Use AGraph's builder to automatically extract entities and relations from documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample documents for graph building\n",
    "documents = [\n",
    "    {\n",
    "        \"content\": \"Tesla Inc is an electric vehicle company founded by Elon Musk. The company is headquartered in Austin, Texas.\",\n",
    "        \"source\": \"tesla_info.txt\"\n",
    "    },\n",
    "    {\n",
    "        \"content\": \"SpaceX, also founded by Elon Musk, develops spacecraft and rocket technology. The company is based in Hawthorne, California.\",\n",
    "        \"source\": \"spacex_info.txt\"\n",
    "    },\n",
    "    {\n",
    "        \"content\": \"Elon Musk is a South African entrepreneur and business magnate. He moved to California in the 1990s.\",\n",
    "        \"source\": \"elon_bio.txt\"\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"📚 Prepared {len(documents)} sample documents for processing\")\n",
    "for i, doc in enumerate(documents, 1):\n",
    "    print(f\"  {i}. {doc['source']}: {len(doc['content'])} characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build knowledge graph from documents\n",
    "print(\"🏗️ Building knowledge graph...\")\n",
    "\n",
    "# Add documents to AGraph\n",
    "for doc in documents:\n",
    "    # Create text chunks with positioning info\n",
    "    chunk = TextChunk(\n",
    "        content=doc[\"content\"],\n",
    "        source=doc[\"source\"],\n",
    "        start_index=0,\n",
    "        end_index=len(doc[\"content\"])\n",
    "    )\n",
    "    agraph.add_text_chunk(chunk)\n",
    "\n",
    "# Get updated stats\n",
    "stats = agraph.get_stats()\n",
    "print(f\"\\n📊 Graph statistics after document processing:\")\n",
    "print(f\"  • Total text chunks: {stats['text_chunk_count']}\")\n",
    "print(f\"  • Total entities: {stats['entity_count']}\")\n",
    "print(f\"  • Total relations: {stats['relation_count']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 🔍 Entity Search and Positioning Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Search and analyze entities using AGraph's search functionality\nprint(\"🔍 Searching entities using AGraph search:\")\n\ntry:\n    # Use AGraph's async search methods\n    import asyncio\n    \n    # Search for organization entities using text search\n    org_search_results = await agraph.search_entities(\"organization company\", top_k=5)\n    print(f\"\\n🏢 Organization search results: {len(org_search_results)}\")\n    \n    for entity, score in org_search_results:\n        print(f\"\\n📍 Entity: {entity.name} (Score: {score:.3f})\")\n        print(f\"  • Type: {entity.entity_type}\")\n        print(f\"  • Description: {entity.description}\")\n        \n        # Check for positioning info if available\n        if hasattr(entity, 'has_position') and entity.has_position():\n            char_pos = entity.get_char_position()\n            print(f\"  • Character position: {char_pos}\")\n            print(f\"  • Position confidence: {entity.get_position_confidence():.2f}\")\n        else:\n            print(f\"  • Position: Not available in this demo\")\n            \nexcept Exception as e:\n    print(f\"⚠️ Search failed: {e}\")\n    print(\"💡 This may be due to vector store not being initialized\")\n    \n    # Fallback to manual knowledge graph operations\n    if 'working_kg' in locals():\n        print(f\"\\n🔍 Using manual graph for demonstration:\")\n        all_entities = list(working_kg.entities)\n        print(f\"📋 Available entities:\")\n        for entity in all_entities:\n            print(f\"  • {entity.name} ({entity.entity_type})\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. ⚡ Performance Features\n",
    "\n",
    "Demonstrate AGraph's performance optimization capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Demonstrate performance features\nimport time\n\nprint(\"⚡ Performance optimization demo:\")\n\n# Test AGraph statistics\ntry:\n    stats = await agraph.get_stats()\n    print(f\"\\n📊 AGraph statistics:\")\n    for key, value in stats.items():\n        print(f\"  • {key}: {value}\")\n        \nexcept Exception as e:\n    print(f\"⚠️ Stats access failed: {e}\")\n\n# Test search performance if possible\nprint(f\"\\n🔍 Testing search performance:\")\nstart_time = time.time()\n\ntry:\n    # Try to search for any text\n    search_results = await agraph.search_entities(\"Apple\", top_k=3)\n    search_time = time.time() - start_time\n    \n    print(f\"  • Search time: {search_time*1000:.2f}ms\")\n    print(f\"  • Results found: {len(search_results)}\")\n    \n    for entity, score in search_results:\n        print(f\"    - {entity.name}: {score:.3f}\")\n        \nexcept Exception as e:\n    search_time = time.time() - start_time\n    print(f\"  • Search test completed in {search_time*1000:.2f}ms\")\n    print(f\"  • Search capability: {type(e).__name__}\")\n\n# Show configuration info\nprint(f\"\\n⚙️ Configuration info:\")\nprint(f\"  • Persist directory: {agraph.persist_directory}\")\nprint(f\"  • KG enabled: {agraph.enable_knowledge_graph}\")\nif agraph.config:\n    print(f\"  • Cache directory: {agraph.config.cache_dir}\")\n    print(f\"  • Chunk size: {agraph.config.chunk_size}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 📊 Graph Analysis and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Analyze graph structure\nprint(\"📊 Graph analysis:\")\n\n# Use the working knowledge graph\nif 'working_kg' in locals():\n    all_entities = list(working_kg.entities)\n    all_relations = list(working_kg.relations)\n    \n    print(f\"\\n🌐 Graph overview:\")\n    print(f\"  • Total entities: {len(all_entities)}\")\n    print(f\"  • Total relations: {len(all_relations)}\")\n    \n    # Analyze entity types distribution\n    entity_types = {}\n    positioned_entities = 0\n    \n    for entity in all_entities:\n        entity_type = entity.entity_type\n        entity_types[entity_type] = entity_types.get(entity_type, 0) + 1\n        if hasattr(entity, 'has_position') and entity.has_position():\n            positioned_entities += 1\n    \n    print(f\"\\n📈 Entity type distribution:\")\n    for entity_type, count in entity_types.items():\n        print(f\"  • {entity_type}: {count}\")\n    \n    print(f\"\\n🎯 Positioning coverage:\")\n    if all_entities:\n        print(f\"  • Entities with positions: {positioned_entities}/{len(all_entities)} ({positioned_entities/len(all_entities)*100:.1f}%)\")\n    else:\n        print(f\"  • No entities available for analysis\")\n    \n    # Show relation types\n    relation_types = {}\n    positioned_relations = 0\n    \n    for relation in all_relations:\n        rel_type = relation.relation_type\n        relation_types[rel_type] = relation_types.get(rel_type, 0) + 1\n        if hasattr(relation, 'has_position') and relation.has_position():\n            positioned_relations += 1\n    \n    if relation_types:\n        print(f\"\\n🔗 Relation type distribution:\")\n        for rel_type, count in relation_types.items():\n            print(f\"  • {rel_type}: {count}\")\n        \n        print(f\"\\n🎯 Relation positioning coverage:\")\n        print(f\"  • Relations with positions: {positioned_relations}/{len(all_relations)} ({positioned_relations/len(all_relations)*100:.1f}%)\")\n    else:\n        print(f\"\\n🔗 No relations found for analysis\")\n        \nelse:\n    print(\"No working knowledge graph available for analysis\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. 💾 Serialization and Persistence\n",
    "\n",
    "Demonstrate how positioning information is preserved through serialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Test serialization with positioning\nprint(\"💾 Testing serialization with positioning data:\")\n\n# Use available entities from working knowledge graph\nif 'working_kg' in locals() and working_kg.entities:\n    # Get first entity for serialization demo\n    demo_entity = list(working_kg.entities)[0]\n    \n    # Serialize the entity\n    entity_dict = demo_entity.to_dict()\n    print(f\"\\n📝 {demo_entity.name} serialization:\")\n    print(f\"  • Entity ID: {entity_dict.get('id')}\")\n    print(f\"  • Entity type: {entity_dict.get('entity_type')}\")\n    print(f\"  • Contains position data: {'position' in entity_dict}\")\n    \n    if 'position' in entity_dict and entity_dict['position']:\n        pos_data = entity_dict['position']\n        print(f\"  • Character interval: {pos_data.get('char_interval')}\")\n        print(f\"  • Alignment status: {pos_data.get('alignment_status')}\")\n        print(f\"  • Confidence: {pos_data.get('confidence')}\")\n    else:\n        print(f\"  • Position data: Not set for this entity\")\n    \n    # Test deserialization\n    try:\n        restored_entity = Entity.from_dict(entity_dict)\n        print(f\"\\n🔄 Deserialization verification:\")\n        print(f\"  • Name preserved: {restored_entity.name == demo_entity.name}\")\n        print(f\"  • Type preserved: {restored_entity.entity_type == demo_entity.entity_type}\")\n        print(f\"  • ID preserved: {restored_entity.id == demo_entity.id}\")\n        \n    except Exception as e:\n        print(f\"⚠️ Deserialization failed: {e}\")\n        \nelse:\n    print(\"No entities available for serialization demo\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Export graph data with positioning information\nprint(\"📤 Exporting complete graph data:\")\n\nif 'working_kg' in locals():\n    try:\n        # Export using knowledge graph's to_dict method\n        graph_data = working_kg.to_dict()\n        \n        print(f\"✅ Graph data exported:\")\n        print(f\"  • Entities: {len(graph_data.get('entities', []))}\")\n        print(f\"  • Relations: {len(graph_data.get('relations', []))}\")\n        print(f\"  • Metadata keys: {list(graph_data.get('metadata', {}).keys())}\")\n        \n        # Calculate data size\n        import json\n        data_size = len(json.dumps(graph_data, ensure_ascii=False))\n        print(f\"  • Data size: ~{data_size / 1024:.1f} KB\")\n        \n        # Show sample entity data structure\n        if graph_data.get('entities'):\n            sample_entity = graph_data['entities'][0]\n            print(f\"\\n📋 Sample entity structure:\")\n            for key in ['id', 'name', 'entity_type', 'description']:\n                if key in sample_entity:\n                    print(f\"  • {key}: {sample_entity[key]}\")\n            \n            if 'position' in sample_entity:\n                print(f\"  • position: Available ✓\")\n            else:\n                print(f\"  • position: Not set\")\n                \n    except Exception as e:\n        print(f\"⚠️ Export failed: {e}\")\n        \nelse:\n    print(\"No knowledge graph available for export\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. 🎯 Advanced Positioning Queries\n",
    "\n",
    "Demonstrate advanced querying capabilities with positioning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Advanced positioning analysis\nprint(\"🎯 Advanced positioning analysis:\")\n\nif 'working_kg' in locals():\n    all_entities = list(working_kg.entities)\n    \n    # Find entities by position characteristics\n    precisely_positioned = []\n    high_confidence = []\n    \n    for entity in all_entities:\n        if hasattr(entity, 'has_precise_position') and entity.has_precise_position():\n            precisely_positioned.append(entity)\n        if hasattr(entity, 'get_position_confidence') and entity.get_position_confidence() > 0.8:\n            high_confidence.append(entity)\n    \n    print(f\"\\n📍 Positioning quality metrics:\")\n    print(f\"  • Precisely aligned entities: {len(precisely_positioned)}\")\n    print(f\"  • High confidence positions: {len(high_confidence)}\")\n    \n    # Find overlapping entities (entities that share text positions)\n    overlapping_pairs = []\n    for i, entity1 in enumerate(all_entities):\n        for entity2 in all_entities[i+1:]:\n            if hasattr(entity1, 'overlaps_with') and entity1.overlaps_with(entity2):\n                overlapping_pairs.append((entity1, entity2))\n    \n    print(f\"\\n🔄 Position overlaps:\")\n    print(f\"  • Overlapping entity pairs: {len(overlapping_pairs)}\")\n    for entity1, entity2 in overlapping_pairs[:3]:  # Show first 3\n        print(f\"    - {entity1.name} ↔ {entity2.name}\")\n        \n    # Show positioning demonstration\n    print(f\"\\n💡 Positioning features demonstration:\")\n    for entity in all_entities[:3]:\n        print(f\"  • {entity.name}:\")\n        print(f\"    - Has position method: {hasattr(entity, 'has_position')}\")\n        print(f\"    - Has char position method: {hasattr(entity, 'get_char_position')}\")\n        print(f\"    - Has positioning: {entity.has_position() if hasattr(entity, 'has_position') else 'Unknown'}\")\n        \nelse:\n    print(\"No knowledge graph available for positioning analysis\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Position-based text highlighting simulation\nprint(\"\\n🖍️ Position-based text highlighting simulation:\")\n\nif 'working_kg' in locals() and working_kg.entities:\n    sample_entities = list(working_kg.entities)[:5]\n    \n    print(f\"\\n📄 Demonstrating positioning concepts with {len(sample_entities)} entities:\")\n    \n    for entity in sample_entities:\n        print(f\"\\n📍 Entity: {entity.name}\")\n        print(f\"  • Type: {entity.entity_type}\")\n        \n        # Check for positioning capabilities\n        if hasattr(entity, 'has_position'):\n            has_pos = entity.has_position()\n            print(f\"  • Has position: {has_pos}\")\n            \n            if has_pos and hasattr(entity, 'get_char_position'):\n                try:\n                    char_pos = entity.get_char_position()\n                    print(f\"  • Character position: {char_pos}\")\n                    \n                    if hasattr(entity, 'get_position_confidence'):\n                        confidence = entity.get_position_confidence()\n                        print(f\"  • Position confidence: {confidence:.2f}\")\n                        \n                except Exception as e:\n                    print(f\"  • Position access error: {e}\")\n        else:\n            print(f\"  • Positioning: Not implemented yet\")\n            \n        # Show text chunk connections\n        if hasattr(entity, 'text_chunks') and entity.text_chunks:\n            print(f\"  • Connected to {len(entity.text_chunks)} text chunks\")\n        else:\n            print(f\"  • Text chunks: None\")\n            \nelse:\n    print(\"📝 Positioning demonstration:\")\n    print(\"  • This feature requires a built knowledge graph\")\n    print(\"  • Configure OpenAI API key to enable automatic extraction\")\n    print(\"  • Or use OptimizedKnowledgeGraph directly for manual positioning\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. 🧹 Cleanup and Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Final summary\nprint(\"🎉 AGraph Advanced Features Demo Complete!\")\n\n# Get final statistics\ntry:\n    final_stats = await agraph.get_stats()\n    print(\"\\n📊 Final AGraph Statistics:\")\n    for key, value in final_stats.items():\n        print(f\"  • {key}: {value}\")\n        \nexcept Exception as e:\n    print(f\"\\n📊 AGraph Status:\")\n    print(f\"  • Initialization: {'✓' if agraph.is_initialized() else '✗'}\")\n    print(f\"  • Has KG: {'✓' if agraph.has_knowledge_graph() else '✗'}\")\n\nif 'working_kg' in locals():\n    print(f\"\\n📈 Working Knowledge Graph:\")\n    print(f\"  • Entities: {len(working_kg.entities)}\")\n    print(f\"  • Relations: {len(working_kg.relations)}\")\n    \n    # Count positioned entities\n    positioned_count = 0\n    for entity in working_kg.entities:\n        if hasattr(entity, 'has_position') and entity.has_position():\n            positioned_count += 1\n    \n    print(f\"  • Entities with positioning: {positioned_count}\")\n\nprint(\"\\n✨ Key Features Demonstrated:\")\nprint(\"  ✅ AGraph initialization and configuration\")\nprint(\"  ✅ Knowledge graph building workflow\")\nprint(\"  ✅ Entity positioning concepts\")\nprint(\"  ✅ Document processing capabilities\")\nprint(\"  ✅ Performance monitoring\")\nprint(\"  ✅ Serialization and data export\")\nprint(\"  ✅ Advanced positioning analysis\")\n\nprint(\"\\n💡 Next Steps:\")\nprint(\"  • Configure OpenAI API key for automatic extraction\")\nprint(\"  • Try the AGraph_Quickstart.ipynb for basic usage\")\nprint(\"  • Explore real document processing with various formats\")\nprint(\"  • Build production knowledge graphs with your own data\")\n\nprint(\"\\n🚀 AGraph is ready for your knowledge graph projects!\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
