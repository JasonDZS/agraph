{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AGraph Advanced Features Demo\n",
    "\n",
    "This notebook demonstrates the advanced features of AGraph including:\n",
    "\n",
    "- üìä **Knowledge Graph Construction** - Building graphs from text documents\n",
    "- üéØ **Entity Positioning** - Precise location tracking in source documents  \n",
    "- üîç **Document Processing** - Multi-format document support\n",
    "- ‚ö° **Performance Optimization** - Caching and indexing capabilities\n",
    "- üîó **Relationship Extraction** - Finding connections between entities\n",
    "- üìà **Graph Analysis** - Clustering and community detection\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Make sure you have AGraph installed and configured:\n",
    "\n",
    "```bash\n",
    "pip install -e .\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. üöÄ Basic Setup and Initialization"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-31T15:31:34.069256Z",
     "start_time": "2025-08-31T15:31:33.468852Z"
    }
   },
   "source": [
    "# Import AGraph and related modules\n",
    "from agraph import AGraph\n",
    "from agraph.base.models.entities import Entity\n",
    "from agraph.base.models.relations import Relation\n",
    "from agraph.base.models.text import TextChunk\n",
    "from agraph.base.models.positioning import Position, CharInterval, AlignmentStatus\n",
    "from agraph.base.core.types import EntityType, RelationType\n",
    "from agraph.processor.factory import DocumentProcessorFactory\n",
    "\n",
    "import tempfile\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"üì¶ AGraph modules imported successfully!\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ AGraph modules imported successfully!\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-31T15:31:34.698083Z",
     "start_time": "2025-08-31T15:31:34.693182Z"
    }
   },
   "source": "# Initialize AGraph with optimized performance settings\nagraph = AGraph(\n    persist_directory=\"./workdir\",\n    enable_cache=True,\n    cache_ttl=3600,  # 1 hour cache\n    enable_performance_mode=True\n)\n\nprint(f\"‚úÖ AGraph initialized successfully!\")\nprint(f\"üìÅ Persist directory: {agraph.persist_directory}\")\nprint(f\"‚öôÔ∏è Configuration: {agraph.config.cache_dir if agraph.config else 'Default config'}\")\nprint(f\"üîß KG enabled: {agraph.enable_knowledge_graph}\")",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m2025-08-31 23:31:34.695\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36magraph.agraph\u001B[0m:\u001B[36m__init__\u001B[0m:\u001B[36m99\u001B[0m - \u001B[1mAGraph initialization completed, collection: agraph_knowledge, persist_dir: ./workdir, enable_kg: True\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ AGraph initialized successfully!\n",
      "üìÅ Persist directory: ./workdir\n",
      "‚öôÔ∏è Configuration: ./workdir/cache\n",
      "üîß KG enabled: True\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. üéØ Entity Positioning Features\n",
    "\n",
    "AGraph now supports precise entity positioning within source documents, enabling you to track exactly where each entity appears in the original text."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-31T15:31:36.422307Z",
     "start_time": "2025-08-31T15:31:36.419371Z"
    }
   },
   "source": "# Create a sample text chunk for processing\nsample_text = \"Apple Inc is a major technology company founded by Steve Jobs in California.\"\n\nprint(f\"üìù Sample text: {sample_text}\")\nprint(f\"üìÑ Text length: {len(sample_text)} characters\")\n\n# This text will be processed by AGraph to extract entities and relations\nprint(\"\\nüîç This text contains entities like:\")\nprint(\"  ‚Ä¢ Apple Inc (Organization)\")\nprint(\"  ‚Ä¢ Steve Jobs (Person)\")  \nprint(\"  ‚Ä¢ California (Location)\")\nprint(\"  ‚Ä¢ And relations like 'founded by' and 'located in'\")",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìù Sample text: Apple Inc is a major technology company founded by Steve Jobs in California.\n",
      "üìÑ Text length: 76 characters\n",
      "\n",
      "üîç This text contains entities like:\n",
      "  ‚Ä¢ Apple Inc (Organization)\n",
      "  ‚Ä¢ Steve Jobs (Person)\n",
      "  ‚Ä¢ California (Location)\n",
      "  ‚Ä¢ And relations like 'founded by' and 'located in'\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-31T15:31:37.311302Z",
     "start_time": "2025-08-31T15:31:37.302746Z"
    }
   },
   "source": "# Build knowledge graph from the sample text\nprint(\"üèóÔ∏è Building knowledge graph from sample text...\")\n\n# Note: This requires OpenAI API key for automatic entity/relation extraction\ntry:\n    # Build knowledge graph from the text\n    knowledge_graph = await agraph.build_from_texts([sample_text])\n    \n    print(f\"‚úÖ Knowledge graph built successfully!\")\n    print(f\"üìä Extracted entities: {len(knowledge_graph.entities)}\")\n    print(f\"üìä Extracted relations: {len(knowledge_graph.relations)}\")\n    \n    # Show extracted entities with positioning info\n    print(f\"\\nüéØ Extracted entities with positioning:\")\n    for entity in knowledge_graph.entities[:5]:  # Show first 5\n        print(f\"  ‚Ä¢ {entity.name} ({entity.entity_type})\")\n        if hasattr(entity, 'text_chunks') and entity.text_chunks:\n            print(f\"    Connected to {len(entity.text_chunks)} text chunks\")\n        if hasattr(entity, 'position') and entity.position:\n            print(f\"    Position info available: {entity.has_position()}\")\n            \nexcept Exception as e:\n    print(f\"‚ö†Ô∏è Automatic extraction failed: {e}\")\n    print(\"üí° This usually means OpenAI API key is not configured\")\n    print(\"   Set OPENAI_API_KEY environment variable to enable this feature\")\n    \n    # Create a manual demonstration instead\n    print(f\"\\nüîß Creating manual demonstration...\")\n    \n    # For demo purposes, we'll work with OptimizedKnowledgeGraph directly\n    from agraph.base.graphs.optimized import OptimizedKnowledgeGraph\n    kg = OptimizedKnowledgeGraph()\n    \n    # Create manual entities to show positioning concepts\n    apple = Entity(\n        name=\"Apple Inc\",\n        entity_type=EntityType.ORGANIZATION,\n        description=\"Technology company\"\n    )\n    kg.add_entity(apple)\n    \n    print(f\"üìù Manual demo entity created: {apple.name}\")",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèóÔ∏è Building knowledge graph from sample text...\n",
      "‚ö†Ô∏è Automatic extraction failed: AGraph not initialized, please call initialize() first\n",
      "üí° This usually means OpenAI API key is not configured\n",
      "   Set OPENAI_API_KEY environment variable to enable this feature\n",
      "\n",
      "üîß Creating manual demonstration...\n",
      "üìù Manual demo entity created: Apple Inc\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-31T15:31:38.506470Z",
     "start_time": "2025-08-31T15:31:38.324064Z"
    }
   },
   "source": "# Continue with the knowledge graph (either extracted or manual demo)\nif 'knowledge_graph' in locals():\n    # Use extracted knowledge graph\n    working_kg = knowledge_graph\n    print(f\"üìä Using extracted knowledge graph\")\nelse:\n    # Use manual demo knowledge graph\n    working_kg = kg\n    print(f\"üìä Using manual demo knowledge graph\")\n\nprint(f\"\\nüìà Current graph statistics:\")\nprint(f\"  ‚Ä¢ Entities: {len(working_kg.entities)}\")\nprint(f\"  ‚Ä¢ Relations: {len(working_kg.relations)}\")\n\n# Show sample entities\nprint(f\"\\nüìã Sample entities:\")\nfor entity in list(working_kg.entities)[:3]:\n    print(f\"  ‚Ä¢ {entity.name} ({entity.entity_type})\")\n    if hasattr(entity, 'description') and entity.description:\n        print(f\"    Description: {entity.description}\")\n    if hasattr(entity, 'properties') and entity.properties:\n        print(f\"    Properties: {entity.properties}\")",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Using manual demo knowledge graph\n",
      "\n",
      "üìà Current graph statistics:\n",
      "  ‚Ä¢ Entities: 1\n",
      "  ‚Ä¢ Relations: 0\n",
      "\n",
      "üìã Sample entities:\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'name'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mAttributeError\u001B[39m                            Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[5]\u001B[39m\u001B[32m, line 18\u001B[39m\n\u001B[32m     16\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33müìã Sample entities:\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m     17\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m entity \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mlist\u001B[39m(working_kg.entities)[:\u001B[32m3\u001B[39m]:\n\u001B[32m---> \u001B[39m\u001B[32m18\u001B[39m     \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m  ‚Ä¢ \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[43mentity\u001B[49m\u001B[43m.\u001B[49m\u001B[43mname\u001B[49m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m (\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mentity.entity_type\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m)\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m     19\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(entity, \u001B[33m'\u001B[39m\u001B[33mdescription\u001B[39m\u001B[33m'\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m entity.description:\n\u001B[32m     20\u001B[39m         \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m    Description: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mentity.description\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n",
      "\u001B[31mAttributeError\u001B[39m: 'str' object has no attribute 'name'"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. üîó Relationship Creation with Positioning\n",
    "\n",
    "Create relationships between entities and track their positions in the source text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Demonstrate positioning concepts (for manual demo case)\nif 'kg' in locals():\n    print(\"üîó Demonstrating positioning concepts:\")\n    \n    # Create additional entities to show relationships\n    steve_jobs = Entity(\n        name=\"Steve Jobs\",\n        entity_type=EntityType.PERSON,\n        description=\"Co-founder of Apple Inc\"\n    )\n    kg.add_entity(steve_jobs)\n    \n    california = Entity(\n        name=\"California\", \n        entity_type=EntityType.LOCATION,\n        description=\"US State\"\n    )\n    kg.add_entity(california)\n    \n    # Create relations\n    founded_relation = Relation(\n        head_entity=steve_jobs,\n        tail_entity=apple,\n        relation_type=RelationType.FOUNDED_BY,\n        description=\"Steve Jobs founded Apple Inc\"\n    )\n    kg.add_relation(founded_relation)\n    \n    located_relation = Relation(\n        head_entity=apple,\n        tail_entity=california,\n        relation_type=RelationType.LOCATED_IN, \n        description=\"Apple Inc is located in California\"\n    )\n    kg.add_relation(located_relation)\n    \n    print(f\"üîó Relations created:\")\n    for relation in [founded_relation, located_relation]:\n        print(f\"  ‚Ä¢ {relation.head_entity.name} -> {relation.relation_type} -> {relation.tail_entity.name}\")\n        \nelse:\n    print(\"üîó Using extracted relations from knowledge graph:\")\n    for relation in list(working_kg.relations)[:3]:\n        if relation.head_entity and relation.tail_entity:\n            print(f\"  ‚Ä¢ {relation.head_entity.name} -> {relation.relation_type} -> {relation.tail_entity.name}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. üîç Document Processing Features\n",
    "\n",
    "Demonstrate multi-format document processing capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize document processor factory\n",
    "processor_factory = DocumentProcessorFactory()\n",
    "\n",
    "print(\"üìÑ Document Processing Capabilities:\")\n",
    "print(f\"Supported extensions: {processor_factory.get_supported_extensions()}\")\n",
    "print(f\"Total processors: {len(processor_factory._processors)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample documents for processing\n",
    "with tempfile.TemporaryDirectory() as temp_dir:\n",
    "    temp_path = Path(temp_dir)\n",
    "    \n",
    "    # Create a text file\n",
    "    text_file = temp_path / \"company_info.txt\"\n",
    "    text_content = \"\"\"\n",
    "Microsoft Corporation is a multinational technology company.\n",
    "Founded in 1975 by Bill Gates and Paul Allen in Albuquerque, New Mexico.\n",
    "The company is now headquartered in Redmond, Washington.\n",
    "Microsoft develops software, hardware, and cloud services.\n",
    "\"\"\".strip()\n",
    "    text_file.write_text(text_content)\n",
    "    \n",
    "    # Create a JSON file\n",
    "    json_file = temp_path / \"product_data.json\"\n",
    "    json_data = {\n",
    "        \"company\": \"Microsoft\",\n",
    "        \"products\": [\n",
    "            {\"name\": \"Windows\", \"type\": \"Operating System\", \"launched\": 1985},\n",
    "            {\"name\": \"Office\", \"type\": \"Productivity Suite\", \"launched\": 1990},\n",
    "            {\"name\": \"Azure\", \"type\": \"Cloud Platform\", \"launched\": 2010}\n",
    "        ],\n",
    "        \"headquarters\": \"Redmond, Washington\"\n",
    "    }\n",
    "    json_file.write_text(json.dumps(json_data, indent=2))\n",
    "    \n",
    "    # Process documents\n",
    "    print(\"\\nüìÑ Processing documents:\")\n",
    "    \n",
    "    # Process text file\n",
    "    text_processor = processor_factory.get_processor(text_file)\n",
    "    text_result = text_processor.process(text_file)\n",
    "    text_metadata = text_processor.extract_metadata(text_file)\n",
    "    \n",
    "    print(f\"\\nüìù Text file processed:\")\n",
    "    print(f\"  ‚Ä¢ Content length: {len(text_result)} characters\")\n",
    "    print(f\"  ‚Ä¢ Word count: {text_metadata.get('word_count', 'N/A')}\")\n",
    "    print(f\"  ‚Ä¢ Line count: {text_metadata.get('line_count', 'N/A')}\")\n",
    "    \n",
    "    # Process JSON file\n",
    "    json_processor = processor_factory.get_processor(json_file)\n",
    "    json_result = json_processor.process(json_file, pretty_print=True)\n",
    "    json_metadata = json_processor.extract_metadata(json_file)\n",
    "    \n",
    "    print(f\"\\nüìä JSON file processed:\")\n",
    "    print(f\"  ‚Ä¢ Data type: {json_metadata.get('data_type', 'N/A')}\")\n",
    "    print(f\"  ‚Ä¢ Key count: {json_metadata.get('key_count', 'N/A')}\")\n",
    "    print(f\"  ‚Ä¢ Max depth: {json_metadata.get('max_depth', 'N/A')}\")\n",
    "    \n",
    "    print(f\"\\n‚ú® Sample JSON content preview:\")\n",
    "    print(json_result[:200] + \"...\" if len(json_result) > 200 else json_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. üèóÔ∏è Automated Knowledge Graph Building\n",
    "\n",
    "Use AGraph's builder to automatically extract entities and relations from documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample documents for graph building\n",
    "documents = [\n",
    "    {\n",
    "        \"content\": \"Tesla Inc is an electric vehicle company founded by Elon Musk. The company is headquartered in Austin, Texas.\",\n",
    "        \"source\": \"tesla_info.txt\"\n",
    "    },\n",
    "    {\n",
    "        \"content\": \"SpaceX, also founded by Elon Musk, develops spacecraft and rocket technology. The company is based in Hawthorne, California.\",\n",
    "        \"source\": \"spacex_info.txt\"\n",
    "    },\n",
    "    {\n",
    "        \"content\": \"Elon Musk is a South African entrepreneur and business magnate. He moved to California in the 1990s.\",\n",
    "        \"source\": \"elon_bio.txt\"\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"üìö Prepared {len(documents)} sample documents for processing\")\n",
    "for i, doc in enumerate(documents, 1):\n",
    "    print(f\"  {i}. {doc['source']}: {len(doc['content'])} characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build knowledge graph from documents\n",
    "print(\"üèóÔ∏è Building knowledge graph...\")\n",
    "\n",
    "# Add documents to AGraph\n",
    "for doc in documents:\n",
    "    # Create text chunks with positioning info\n",
    "    chunk = TextChunk(\n",
    "        content=doc[\"content\"],\n",
    "        source=doc[\"source\"],\n",
    "        start_index=0,\n",
    "        end_index=len(doc[\"content\"])\n",
    "    )\n",
    "    agraph.add_text_chunk(chunk)\n",
    "\n",
    "# Get updated stats\n",
    "stats = agraph.get_stats()\n",
    "print(f\"\\nüìä Graph statistics after document processing:\")\n",
    "print(f\"  ‚Ä¢ Total text chunks: {stats['text_chunk_count']}\")\n",
    "print(f\"  ‚Ä¢ Total entities: {stats['entity_count']}\")\n",
    "print(f\"  ‚Ä¢ Total relations: {stats['relation_count']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. üîç Entity Search and Positioning Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Search and analyze entities using AGraph's search functionality\nprint(\"üîç Searching entities using AGraph search:\")\n\ntry:\n    # Use AGraph's async search methods\n    import asyncio\n    \n    # Search for organization entities using text search\n    org_search_results = await agraph.search_entities(\"organization company\", top_k=5)\n    print(f\"\\nüè¢ Organization search results: {len(org_search_results)}\")\n    \n    for entity, score in org_search_results:\n        print(f\"\\nüìç Entity: {entity.name} (Score: {score:.3f})\")\n        print(f\"  ‚Ä¢ Type: {entity.entity_type}\")\n        print(f\"  ‚Ä¢ Description: {entity.description}\")\n        \n        # Check for positioning info if available\n        if hasattr(entity, 'has_position') and entity.has_position():\n            char_pos = entity.get_char_position()\n            print(f\"  ‚Ä¢ Character position: {char_pos}\")\n            print(f\"  ‚Ä¢ Position confidence: {entity.get_position_confidence():.2f}\")\n        else:\n            print(f\"  ‚Ä¢ Position: Not available in this demo\")\n            \nexcept Exception as e:\n    print(f\"‚ö†Ô∏è Search failed: {e}\")\n    print(\"üí° This may be due to vector store not being initialized\")\n    \n    # Fallback to manual knowledge graph operations\n    if 'working_kg' in locals():\n        print(f\"\\nüîç Using manual graph for demonstration:\")\n        all_entities = list(working_kg.entities)\n        print(f\"üìã Available entities:\")\n        for entity in all_entities:\n            print(f\"  ‚Ä¢ {entity.name} ({entity.entity_type})\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. ‚ö° Performance Features\n",
    "\n",
    "Demonstrate AGraph's performance optimization capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Demonstrate performance features\nimport time\n\nprint(\"‚ö° Performance optimization demo:\")\n\n# Test AGraph statistics\ntry:\n    stats = await agraph.get_stats()\n    print(f\"\\nüìä AGraph statistics:\")\n    for key, value in stats.items():\n        print(f\"  ‚Ä¢ {key}: {value}\")\n        \nexcept Exception as e:\n    print(f\"‚ö†Ô∏è Stats access failed: {e}\")\n\n# Test search performance if possible\nprint(f\"\\nüîç Testing search performance:\")\nstart_time = time.time()\n\ntry:\n    # Try to search for any text\n    search_results = await agraph.search_entities(\"Apple\", top_k=3)\n    search_time = time.time() - start_time\n    \n    print(f\"  ‚Ä¢ Search time: {search_time*1000:.2f}ms\")\n    print(f\"  ‚Ä¢ Results found: {len(search_results)}\")\n    \n    for entity, score in search_results:\n        print(f\"    - {entity.name}: {score:.3f}\")\n        \nexcept Exception as e:\n    search_time = time.time() - start_time\n    print(f\"  ‚Ä¢ Search test completed in {search_time*1000:.2f}ms\")\n    print(f\"  ‚Ä¢ Search capability: {type(e).__name__}\")\n\n# Show configuration info\nprint(f\"\\n‚öôÔ∏è Configuration info:\")\nprint(f\"  ‚Ä¢ Persist directory: {agraph.persist_directory}\")\nprint(f\"  ‚Ä¢ KG enabled: {agraph.enable_knowledge_graph}\")\nif agraph.config:\n    print(f\"  ‚Ä¢ Cache directory: {agraph.config.cache_dir}\")\n    print(f\"  ‚Ä¢ Chunk size: {agraph.config.chunk_size}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. üìä Graph Analysis and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Analyze graph structure\nprint(\"üìä Graph analysis:\")\n\n# Use the working knowledge graph\nif 'working_kg' in locals():\n    all_entities = list(working_kg.entities)\n    all_relations = list(working_kg.relations)\n    \n    print(f\"\\nüåê Graph overview:\")\n    print(f\"  ‚Ä¢ Total entities: {len(all_entities)}\")\n    print(f\"  ‚Ä¢ Total relations: {len(all_relations)}\")\n    \n    # Analyze entity types distribution\n    entity_types = {}\n    positioned_entities = 0\n    \n    for entity in all_entities:\n        entity_type = entity.entity_type\n        entity_types[entity_type] = entity_types.get(entity_type, 0) + 1\n        if hasattr(entity, 'has_position') and entity.has_position():\n            positioned_entities += 1\n    \n    print(f\"\\nüìà Entity type distribution:\")\n    for entity_type, count in entity_types.items():\n        print(f\"  ‚Ä¢ {entity_type}: {count}\")\n    \n    print(f\"\\nüéØ Positioning coverage:\")\n    if all_entities:\n        print(f\"  ‚Ä¢ Entities with positions: {positioned_entities}/{len(all_entities)} ({positioned_entities/len(all_entities)*100:.1f}%)\")\n    else:\n        print(f\"  ‚Ä¢ No entities available for analysis\")\n    \n    # Show relation types\n    relation_types = {}\n    positioned_relations = 0\n    \n    for relation in all_relations:\n        rel_type = relation.relation_type\n        relation_types[rel_type] = relation_types.get(rel_type, 0) + 1\n        if hasattr(relation, 'has_position') and relation.has_position():\n            positioned_relations += 1\n    \n    if relation_types:\n        print(f\"\\nüîó Relation type distribution:\")\n        for rel_type, count in relation_types.items():\n            print(f\"  ‚Ä¢ {rel_type}: {count}\")\n        \n        print(f\"\\nüéØ Relation positioning coverage:\")\n        print(f\"  ‚Ä¢ Relations with positions: {positioned_relations}/{len(all_relations)} ({positioned_relations/len(all_relations)*100:.1f}%)\")\n    else:\n        print(f\"\\nüîó No relations found for analysis\")\n        \nelse:\n    print(\"No working knowledge graph available for analysis\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. üíæ Serialization and Persistence\n",
    "\n",
    "Demonstrate how positioning information is preserved through serialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Test serialization with positioning\nprint(\"üíæ Testing serialization with positioning data:\")\n\n# Use available entities from working knowledge graph\nif 'working_kg' in locals() and working_kg.entities:\n    # Get first entity for serialization demo\n    demo_entity = list(working_kg.entities)[0]\n    \n    # Serialize the entity\n    entity_dict = demo_entity.to_dict()\n    print(f\"\\nüìù {demo_entity.name} serialization:\")\n    print(f\"  ‚Ä¢ Entity ID: {entity_dict.get('id')}\")\n    print(f\"  ‚Ä¢ Entity type: {entity_dict.get('entity_type')}\")\n    print(f\"  ‚Ä¢ Contains position data: {'position' in entity_dict}\")\n    \n    if 'position' in entity_dict and entity_dict['position']:\n        pos_data = entity_dict['position']\n        print(f\"  ‚Ä¢ Character interval: {pos_data.get('char_interval')}\")\n        print(f\"  ‚Ä¢ Alignment status: {pos_data.get('alignment_status')}\")\n        print(f\"  ‚Ä¢ Confidence: {pos_data.get('confidence')}\")\n    else:\n        print(f\"  ‚Ä¢ Position data: Not set for this entity\")\n    \n    # Test deserialization\n    try:\n        restored_entity = Entity.from_dict(entity_dict)\n        print(f\"\\nüîÑ Deserialization verification:\")\n        print(f\"  ‚Ä¢ Name preserved: {restored_entity.name == demo_entity.name}\")\n        print(f\"  ‚Ä¢ Type preserved: {restored_entity.entity_type == demo_entity.entity_type}\")\n        print(f\"  ‚Ä¢ ID preserved: {restored_entity.id == demo_entity.id}\")\n        \n    except Exception as e:\n        print(f\"‚ö†Ô∏è Deserialization failed: {e}\")\n        \nelse:\n    print(\"No entities available for serialization demo\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Export graph data with positioning information\nprint(\"üì§ Exporting complete graph data:\")\n\nif 'working_kg' in locals():\n    try:\n        # Export using knowledge graph's to_dict method\n        graph_data = working_kg.to_dict()\n        \n        print(f\"‚úÖ Graph data exported:\")\n        print(f\"  ‚Ä¢ Entities: {len(graph_data.get('entities', []))}\")\n        print(f\"  ‚Ä¢ Relations: {len(graph_data.get('relations', []))}\")\n        print(f\"  ‚Ä¢ Metadata keys: {list(graph_data.get('metadata', {}).keys())}\")\n        \n        # Calculate data size\n        import json\n        data_size = len(json.dumps(graph_data, ensure_ascii=False))\n        print(f\"  ‚Ä¢ Data size: ~{data_size / 1024:.1f} KB\")\n        \n        # Show sample entity data structure\n        if graph_data.get('entities'):\n            sample_entity = graph_data['entities'][0]\n            print(f\"\\nüìã Sample entity structure:\")\n            for key in ['id', 'name', 'entity_type', 'description']:\n                if key in sample_entity:\n                    print(f\"  ‚Ä¢ {key}: {sample_entity[key]}\")\n            \n            if 'position' in sample_entity:\n                print(f\"  ‚Ä¢ position: Available ‚úì\")\n            else:\n                print(f\"  ‚Ä¢ position: Not set\")\n                \n    except Exception as e:\n        print(f\"‚ö†Ô∏è Export failed: {e}\")\n        \nelse:\n    print(\"No knowledge graph available for export\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. üéØ Advanced Positioning Queries\n",
    "\n",
    "Demonstrate advanced querying capabilities with positioning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Advanced positioning analysis\nprint(\"üéØ Advanced positioning analysis:\")\n\nif 'working_kg' in locals():\n    all_entities = list(working_kg.entities)\n    \n    # Find entities by position characteristics\n    precisely_positioned = []\n    high_confidence = []\n    \n    for entity in all_entities:\n        if hasattr(entity, 'has_precise_position') and entity.has_precise_position():\n            precisely_positioned.append(entity)\n        if hasattr(entity, 'get_position_confidence') and entity.get_position_confidence() > 0.8:\n            high_confidence.append(entity)\n    \n    print(f\"\\nüìç Positioning quality metrics:\")\n    print(f\"  ‚Ä¢ Precisely aligned entities: {len(precisely_positioned)}\")\n    print(f\"  ‚Ä¢ High confidence positions: {len(high_confidence)}\")\n    \n    # Find overlapping entities (entities that share text positions)\n    overlapping_pairs = []\n    for i, entity1 in enumerate(all_entities):\n        for entity2 in all_entities[i+1:]:\n            if hasattr(entity1, 'overlaps_with') and entity1.overlaps_with(entity2):\n                overlapping_pairs.append((entity1, entity2))\n    \n    print(f\"\\nüîÑ Position overlaps:\")\n    print(f\"  ‚Ä¢ Overlapping entity pairs: {len(overlapping_pairs)}\")\n    for entity1, entity2 in overlapping_pairs[:3]:  # Show first 3\n        print(f\"    - {entity1.name} ‚Üî {entity2.name}\")\n        \n    # Show positioning demonstration\n    print(f\"\\nüí° Positioning features demonstration:\")\n    for entity in all_entities[:3]:\n        print(f\"  ‚Ä¢ {entity.name}:\")\n        print(f\"    - Has position method: {hasattr(entity, 'has_position')}\")\n        print(f\"    - Has char position method: {hasattr(entity, 'get_char_position')}\")\n        print(f\"    - Has positioning: {entity.has_position() if hasattr(entity, 'has_position') else 'Unknown'}\")\n        \nelse:\n    print(\"No knowledge graph available for positioning analysis\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Position-based text highlighting simulation\nprint(\"\\nüñçÔ∏è Position-based text highlighting simulation:\")\n\nif 'working_kg' in locals() and working_kg.entities:\n    sample_entities = list(working_kg.entities)[:5]\n    \n    print(f\"\\nüìÑ Demonstrating positioning concepts with {len(sample_entities)} entities:\")\n    \n    for entity in sample_entities:\n        print(f\"\\nüìç Entity: {entity.name}\")\n        print(f\"  ‚Ä¢ Type: {entity.entity_type}\")\n        \n        # Check for positioning capabilities\n        if hasattr(entity, 'has_position'):\n            has_pos = entity.has_position()\n            print(f\"  ‚Ä¢ Has position: {has_pos}\")\n            \n            if has_pos and hasattr(entity, 'get_char_position'):\n                try:\n                    char_pos = entity.get_char_position()\n                    print(f\"  ‚Ä¢ Character position: {char_pos}\")\n                    \n                    if hasattr(entity, 'get_position_confidence'):\n                        confidence = entity.get_position_confidence()\n                        print(f\"  ‚Ä¢ Position confidence: {confidence:.2f}\")\n                        \n                except Exception as e:\n                    print(f\"  ‚Ä¢ Position access error: {e}\")\n        else:\n            print(f\"  ‚Ä¢ Positioning: Not implemented yet\")\n            \n        # Show text chunk connections\n        if hasattr(entity, 'text_chunks') and entity.text_chunks:\n            print(f\"  ‚Ä¢ Connected to {len(entity.text_chunks)} text chunks\")\n        else:\n            print(f\"  ‚Ä¢ Text chunks: None\")\n            \nelse:\n    print(\"üìù Positioning demonstration:\")\n    print(\"  ‚Ä¢ This feature requires a built knowledge graph\")\n    print(\"  ‚Ä¢ Configure OpenAI API key to enable automatic extraction\")\n    print(\"  ‚Ä¢ Or use OptimizedKnowledgeGraph directly for manual positioning\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. üßπ Cleanup and Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Final summary\nprint(\"üéâ AGraph Advanced Features Demo Complete!\")\n\n# Get final statistics\ntry:\n    final_stats = await agraph.get_stats()\n    print(\"\\nüìä Final AGraph Statistics:\")\n    for key, value in final_stats.items():\n        print(f\"  ‚Ä¢ {key}: {value}\")\n        \nexcept Exception as e:\n    print(f\"\\nüìä AGraph Status:\")\n    print(f\"  ‚Ä¢ Initialization: {'‚úì' if agraph.is_initialized() else '‚úó'}\")\n    print(f\"  ‚Ä¢ Has KG: {'‚úì' if agraph.has_knowledge_graph() else '‚úó'}\")\n\nif 'working_kg' in locals():\n    print(f\"\\nüìà Working Knowledge Graph:\")\n    print(f\"  ‚Ä¢ Entities: {len(working_kg.entities)}\")\n    print(f\"  ‚Ä¢ Relations: {len(working_kg.relations)}\")\n    \n    # Count positioned entities\n    positioned_count = 0\n    for entity in working_kg.entities:\n        if hasattr(entity, 'has_position') and entity.has_position():\n            positioned_count += 1\n    \n    print(f\"  ‚Ä¢ Entities with positioning: {positioned_count}\")\n\nprint(\"\\n‚ú® Key Features Demonstrated:\")\nprint(\"  ‚úÖ AGraph initialization and configuration\")\nprint(\"  ‚úÖ Knowledge graph building workflow\")\nprint(\"  ‚úÖ Entity positioning concepts\")\nprint(\"  ‚úÖ Document processing capabilities\")\nprint(\"  ‚úÖ Performance monitoring\")\nprint(\"  ‚úÖ Serialization and data export\")\nprint(\"  ‚úÖ Advanced positioning analysis\")\n\nprint(\"\\nüí° Next Steps:\")\nprint(\"  ‚Ä¢ Configure OpenAI API key for automatic extraction\")\nprint(\"  ‚Ä¢ Try the AGraph_Quickstart.ipynb for basic usage\")\nprint(\"  ‚Ä¢ Explore real document processing with various formats\")\nprint(\"  ‚Ä¢ Build production knowledge graphs with your own data\")\n\nprint(\"\\nüöÄ AGraph is ready for your knowledge graph projects!\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
