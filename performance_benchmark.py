"""
Pipelineæ€§èƒ½åŸºå‡†æµ‹è¯•å’Œåˆ†æå·¥å…·ã€‚

ç”¨äºå¯¹æ¯”æ–°æ—§æ¶æ„çš„æ€§èƒ½å·®å¼‚ï¼Œè¯†åˆ«ç“¶é¢ˆå’Œä¼˜åŒ–æœºä¼šã€‚
"""

import asyncio
import time
import tracemalloc
import psutil
import gc
from typing import Dict, List, Any, Optional
from dataclasses import dataclass
from pathlib import Path

# æµ‹è¯•ç”¨çš„ç¤ºä¾‹æ•°æ®
SAMPLE_TEXTS = [
    "Apple Inc. is an American multinational technology company that specializes in consumer electronics.",
    "Microsoft Corporation is an American multinational technology corporation which produces computer software.",
    "Google LLC is an American multinational technology company that focuses on Internet-related services.",
    "Amazon.com Inc. is an American multinational technology company focusing on e-commerce and cloud computing.",
    "Tesla Inc. is an American electric vehicle and clean energy company based in Palo Alto, California."
] * 10  # 50ä¸ªæ–‡æœ¬ç”¨äºæµ‹è¯•

LARGE_SAMPLE_TEXTS = SAMPLE_TEXTS * 20  # 1000ä¸ªæ–‡æœ¬ç”¨äºå‹åŠ›æµ‹è¯•

@dataclass
class PerformanceMetrics:
    """æ€§èƒ½æŒ‡æ ‡æ•°æ®ç±»"""
    execution_time: float
    memory_peak_mb: float
    memory_current_mb: float
    cpu_percent: float
    success: bool
    error: Optional[str] = None
    step_timings: Dict[str, float] = None
    cache_hit_rate: float = 0.0
    
    def __post_init__(self):
        if self.step_timings is None:
            self.step_timings = {}


class PerformanceBenchmark:
    """Pipelineæ€§èƒ½åŸºå‡†æµ‹è¯•å™¨"""
    
    def __init__(self):
        self.process = psutil.Process()
        self.results = {}
    
    async def run_legacy_benchmark(self, texts: List[str], iterations: int = 3) -> PerformanceMetrics:
        """è¿è¡ŒLegacyå®ç°çš„åŸºå‡†æµ‹è¯•"""
        try:
            # åŠ¨æ€å¯¼å…¥é¿å…åˆå§‹åŒ–é—®é¢˜ - ä½¿ç”¨å¤‡ä»½çš„legacyç‰ˆæœ¬è¿›è¡Œå¯¹æ¯”
            from agraph.builder.compatibility import BackwardCompatibleKnowledgeGraphBuilder
            
            total_time = 0
            memory_peaks = []
            cpu_usage = []
            all_step_timings = {}
            
            for i in range(iterations):
                print(f"Legacyæµ‹è¯• - è¿­ä»£ {i+1}/{iterations}")
                
                # å†…å­˜è¿½è¸ª
                tracemalloc.start()
                gc.collect()
                
                start_memory = self.process.memory_info().rss / 1024 / 1024
                start_time = time.time()
                
                try:
                    builder = BackwardCompatibleKnowledgeGraphBuilder(
                        use_legacy=True, 
                        show_deprecation_warnings=False,
                        enable_knowledge_graph=True
                    )
                    kg = await builder.build_from_text(
                        texts, 
                        graph_name=f"legacy_test_{i}",
                        use_cache=False  # é¿å…ç¼“å­˜å½±å“æ€§èƒ½æµ‹è¯•
                    )
                    
                    end_time = time.time()
                    end_memory = self.process.memory_info().rss / 1024 / 1024
                    
                    # å†…å­˜å³°å€¼
                    current, peak = tracemalloc.get_traced_memory()
                    tracemalloc.stop()
                    
                    iteration_time = end_time - start_time
                    total_time += iteration_time
                    memory_peaks.append(peak / 1024 / 1024)
                    cpu_usage.append(self.process.cpu_percent())
                    
                    print(f"  æ‰§è¡Œæ—¶é—´: {iteration_time:.2f}s")
                    print(f"  å†…å­˜ä½¿ç”¨: {end_memory - start_memory:.1f}MB")
                    print(f"  å®ä½“æ•°: {len(kg.entities)}, å…³ç³»æ•°: {len(kg.relations)}")
                    
                except Exception as e:
                    tracemalloc.stop()
                    return PerformanceMetrics(
                        execution_time=0,
                        memory_peak_mb=0,
                        memory_current_mb=0,
                        cpu_percent=0,
                        success=False,
                        error=str(e)
                    )
            
            return PerformanceMetrics(
                execution_time=total_time / iterations,
                memory_peak_mb=sum(memory_peaks) / len(memory_peaks),
                memory_current_mb=self.process.memory_info().rss / 1024 / 1024,
                cpu_percent=sum(cpu_usage) / len(cpu_usage),
                success=True
            )
            
        except Exception as e:
            return PerformanceMetrics(
                execution_time=0,
                memory_peak_mb=0,
                memory_current_mb=0,
                cpu_percent=0,
                success=False,
                error=f"Legacyæµ‹è¯•å¤±è´¥: {str(e)}"
            )
    
    async def run_pipeline_benchmark(self, texts: List[str], iterations: int = 3) -> PerformanceMetrics:
        """è¿è¡ŒPipelineå®ç°çš„åŸºå‡†æµ‹è¯•"""
        try:
            # ä½¿ç”¨ä¸»å…¥å£ç‚¹ï¼Œç°åœ¨é»˜è®¤æŒ‡å‘Pipelineç‰ˆæœ¬
            from agraph import KnowledgeGraphBuilder
            
            total_time = 0
            memory_peaks = []
            cpu_usage = []
            all_step_timings = {}
            cache_hits = 0
            total_operations = 0
            
            for i in range(iterations):
                print(f"Pipelineæµ‹è¯• - è¿­ä»£ {i+1}/{iterations}")
                
                # å†…å­˜è¿½è¸ª
                tracemalloc.start()
                gc.collect()
                
                start_memory = self.process.memory_info().rss / 1024 / 1024
                start_time = time.time()
                
                try:
                    builder = KnowledgeGraphBuilder(enable_knowledge_graph=True)
                    kg = await builder.build_from_text(
                        texts, 
                        graph_name=f"pipeline_test_{i}",
                        use_cache=False  # é¿å…ç¼“å­˜å½±å“æ€§èƒ½æµ‹è¯•
                    )
                    
                    end_time = time.time()
                    end_memory = self.process.memory_info().rss / 1024 / 1024
                    
                    # å†…å­˜å³°å€¼
                    current, peak = tracemalloc.get_traced_memory()
                    tracemalloc.stop()
                    
                    iteration_time = end_time - start_time
                    total_time += iteration_time
                    memory_peaks.append(peak / 1024 / 1024)
                    cpu_usage.append(self.process.cpu_percent())
                    
                    # è·å–ç®¡é“æŒ‡æ ‡
                    try:
                        metrics = builder.get_pipeline_metrics()
                        # åˆ†æç¼“å­˜å‘½ä¸­ç‡ç­‰æŒ‡æ ‡
                    except:
                        pass
                    
                    print(f"  æ‰§è¡Œæ—¶é—´: {iteration_time:.2f}s")
                    print(f"  å†…å­˜ä½¿ç”¨: {end_memory - start_memory:.1f}MB")
                    print(f"  å®ä½“æ•°: {len(kg.entities)}, å…³ç³»æ•°: {len(kg.relations)}")
                    
                except Exception as e:
                    tracemalloc.stop()
                    return PerformanceMetrics(
                        execution_time=0,
                        memory_peak_mb=0,
                        memory_current_mb=0,
                        cpu_percent=0,
                        success=False,
                        error=str(e)
                    )
            
            return PerformanceMetrics(
                execution_time=total_time / iterations,
                memory_peak_mb=sum(memory_peaks) / len(memory_peaks),
                memory_current_mb=self.process.memory_info().rss / 1024 / 1024,
                cpu_percent=sum(cpu_usage) / len(cpu_usage),
                success=True,
                cache_hit_rate=cache_hits / max(total_operations, 1)
            )
            
        except Exception as e:
            return PerformanceMetrics(
                execution_time=0,
                memory_peak_mb=0,
                memory_current_mb=0,
                cpu_percent=0,
                success=False,
                error=f"Pipelineæµ‹è¯•å¤±è´¥: {str(e)}"
            )
    
    async def run_comprehensive_benchmark(self) -> Dict[str, Any]:
        """è¿è¡Œç»¼åˆæ€§èƒ½åŸºå‡†æµ‹è¯•"""
        print("ğŸš€ å¼€å§‹ç»¼åˆæ€§èƒ½åŸºå‡†æµ‹è¯•...\n")
        
        results = {
            "test_environment": self._get_system_info(),
            "small_dataset": {},
            "large_dataset": {},
            "comparison": {}
        }
        
        # å°æ•°æ®é›†æµ‹è¯• (50ä¸ªæ–‡æœ¬)
        print("ğŸ“Š å°æ•°æ®é›†æµ‹è¯• (50ä¸ªæ–‡æœ¬):")
        print("-" * 40)
        
        legacy_small = await self.run_legacy_benchmark(SAMPLE_TEXTS)
        pipeline_small = await self.run_pipeline_benchmark(SAMPLE_TEXTS)
        
        results["small_dataset"] = {
            "legacy": legacy_small,
            "pipeline": pipeline_small
        }
        
        # å¤§æ•°æ®é›†æµ‹è¯• (1000ä¸ªæ–‡æœ¬)
        print("\nğŸ“Š å¤§æ•°æ®é›†æµ‹è¯• (1000ä¸ªæ–‡æœ¬):")
        print("-" * 40)
        
        legacy_large = await self.run_legacy_benchmark(LARGE_SAMPLE_TEXTS, iterations=1)
        pipeline_large = await self.run_pipeline_benchmark(LARGE_SAMPLE_TEXTS, iterations=1)
        
        results["large_dataset"] = {
            "legacy": legacy_large,
            "pipeline": pipeline_large
        }
        
        # æ€§èƒ½å¯¹æ¯”åˆ†æ
        results["comparison"] = self._analyze_performance(results)
        
        return results
    
    def _get_system_info(self) -> Dict[str, Any]:
        """è·å–ç³»ç»Ÿä¿¡æ¯"""
        return {
            "cpu_count": psutil.cpu_count(),
            "memory_total_gb": psutil.virtual_memory().total / 1024**3,
            "python_version": __import__("sys").version,
            "platform": __import__("platform").platform()
        }
    
    def _analyze_performance(self, results: Dict[str, Any]) -> Dict[str, Any]:
        """åˆ†ææ€§èƒ½å·®å¼‚"""
        comparison = {}
        
        for dataset in ["small_dataset", "large_dataset"]:
            if dataset not in results:
                continue
                
            legacy = results[dataset].get("legacy")
            pipeline = results[dataset].get("pipeline")
            
            if not legacy or not pipeline or not legacy.success or not pipeline.success:
                comparison[dataset] = {"status": "æµ‹è¯•å¤±è´¥æˆ–ä¸å®Œæ•´"}
                continue
            
            # æ—¶é—´å¯¹æ¯”
            time_improvement = ((legacy.execution_time - pipeline.execution_time) / legacy.execution_time) * 100
            
            # å†…å­˜å¯¹æ¯”
            memory_improvement = ((legacy.memory_peak_mb - pipeline.memory_peak_mb) / legacy.memory_peak_mb) * 100
            
            comparison[dataset] = {
                "time_improvement_percent": time_improvement,
                "memory_improvement_percent": memory_improvement,
                "pipeline_faster": time_improvement > 0,
                "pipeline_memory_efficient": memory_improvement > 0,
                "performance_verdict": self._get_verdict(time_improvement, memory_improvement)
            }
        
        return comparison
    
    def _get_verdict(self, time_improvement: float, memory_improvement: float) -> str:
        """è·å–æ€§èƒ½è¯„ä»·"""
        if time_improvement > 10 and memory_improvement > 5:
            return "æ˜¾è‘—æ€§èƒ½æå‡"
        elif time_improvement > 5 or memory_improvement > 5:
            return "æ€§èƒ½æœ‰æ‰€æå‡"
        elif time_improvement > -5 and memory_improvement > -5:
            return "æ€§èƒ½ç›¸è¿‘"
        else:
            return "æ€§èƒ½æœ‰æ‰€ä¸‹é™"
    
    def generate_report(self, results: Dict[str, Any]) -> str:
        """ç”Ÿæˆæ€§èƒ½æµ‹è¯•æŠ¥å‘Š"""
        report = ["# AGraph Pipelineæ€§èƒ½åŸºå‡†æµ‹è¯•æŠ¥å‘Š\n"]
        
        # ç³»ç»Ÿç¯å¢ƒ
        env = results["test_environment"]
        report.append("## æµ‹è¯•ç¯å¢ƒ")
        report.append(f"- CPUæ ¸å¿ƒæ•°: {env['cpu_count']}")
        report.append(f"- å†…å­˜æ€»é‡: {env['memory_total_gb']:.1f}GB")
        report.append(f"- Pythonç‰ˆæœ¬: {env['python_version'].split()[0]}")
        report.append(f"- æ“ä½œç³»ç»Ÿ: {env['platform']}")
        report.append("")
        
        # æµ‹è¯•ç»“æœ
        for dataset_name, dataset in [("small_dataset", "å°æ•°æ®é›†(50æ–‡æœ¬)"), ("large_dataset", "å¤§æ•°æ®é›†(1000æ–‡æœ¬)")]:
            if dataset_name not in results:
                continue
                
            report.append(f"## {dataset} æµ‹è¯•ç»“æœ")
            
            legacy = results[dataset_name].get("legacy")
            pipeline = results[dataset_name].get("pipeline")
            
            if not legacy or not pipeline:
                report.append("âŒ æµ‹è¯•æ•°æ®ä¸å®Œæ•´")
                continue
            
            # Legacyç»“æœ
            report.append("### Legacyå®ç°")
            if legacy.success:
                report.append(f"- âœ… æ‰§è¡ŒæˆåŠŸ")
                report.append(f"- â±ï¸ æ‰§è¡Œæ—¶é—´: {legacy.execution_time:.2f}ç§’")
                report.append(f"- ğŸ’¾ å†…å­˜å³°å€¼: {legacy.memory_peak_mb:.1f}MB")
                report.append(f"- ğŸ–¥ï¸ CPUä½¿ç”¨: {legacy.cpu_percent:.1f}%")
            else:
                report.append(f"- âŒ æ‰§è¡Œå¤±è´¥: {legacy.error}")
            
            # Pipelineç»“æœ
            report.append("\n### Pipelineå®ç°")
            if pipeline.success:
                report.append(f"- âœ… æ‰§è¡ŒæˆåŠŸ")
                report.append(f"- â±ï¸ æ‰§è¡Œæ—¶é—´: {pipeline.execution_time:.2f}ç§’")
                report.append(f"- ğŸ’¾ å†…å­˜å³°å€¼: {pipeline.memory_peak_mb:.1f}MB")
                report.append(f"- ğŸ–¥ï¸ CPUä½¿ç”¨: {pipeline.cpu_percent:.1f}%")
                if pipeline.cache_hit_rate > 0:
                    report.append(f"- ğŸ¯ ç¼“å­˜å‘½ä¸­ç‡: {pipeline.cache_hit_rate:.1%}")
            else:
                report.append(f"- âŒ æ‰§è¡Œå¤±è´¥: {pipeline.error}")
            
            # å¯¹æ¯”åˆ†æ
            comparison = results["comparison"].get(dataset_name)
            if comparison and "time_improvement_percent" in comparison:
                report.append("\n### æ€§èƒ½å¯¹æ¯”")
                time_imp = comparison["time_improvement_percent"]
                memory_imp = comparison["memory_improvement_percent"]
                
                if time_imp > 0:
                    report.append(f"- âš¡ æ‰§è¡Œæ—¶é—´æå‡: {time_imp:.1f}%")
                else:
                    report.append(f"- âš¡ æ‰§è¡Œæ—¶é—´å˜åŒ–: {time_imp:.1f}% (ç¨æ…¢)")
                
                if memory_imp > 0:
                    report.append(f"- ğŸ’¾ å†…å­˜ä½¿ç”¨ä¼˜åŒ–: {memory_imp:.1f}%")
                else:
                    report.append(f"- ğŸ’¾ å†…å­˜ä½¿ç”¨å˜åŒ–: {memory_imp:.1f}% (ç¨é«˜)")
                
                report.append(f"- ğŸ“Š **è¯„ä»·**: {comparison['performance_verdict']}")
            
            report.append("")
        
        return "\n".join(report)


async def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸ”¬ AGraph Pipelineæ€§èƒ½åŸºå‡†æµ‹è¯•")
    print("=" * 50)
    
    benchmark = PerformanceBenchmark()
    results = await benchmark.run_comprehensive_benchmark()
    
    # ç”ŸæˆæŠ¥å‘Š
    report = benchmark.generate_report(results)
    print("\n" + "=" * 50)
    print("ğŸ“‹ æµ‹è¯•æŠ¥å‘Š:")
    print("=" * 50)
    print(report)
    
    # ä¿å­˜æŠ¥å‘Šåˆ°æ–‡ä»¶
    report_file = Path("pipeline_performance_report.md")
    report_file.write_text(report, encoding="utf-8")
    print(f"\nğŸ“„ æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file.absolute()}")


if __name__ == "__main__":
    asyncio.run(main())