# AGraph

AGraph is a modern knowledge graph toolkit for building, managing, and querying knowledge graphs.
It provides a clean API for handling entities and relations, with integrated vector database support
for semantic search and intelligent Q&A.

## Features

- **Pipeline Architecture**: Modern pipeline-based processing with 83% complexity reduction
- **Easy to Use**: Intuitive API design for quick adoption
- **Smart Construction**: Automatically extract entities and relations from text
- **Intelligent Caching**: Advanced caching system with error recovery capabilities
- **Performance Monitoring**: Detailed metrics and performance tracking
- **Semantic Search**: Vector similarity-based intelligent search
- **Smart Q&A**: RAG-based conversational system using knowledge graphs
- **Streaming Support**: Real-time streaming conversations and incremental updates
- **Type Safe**: Complete type annotations and validation
- **Multiple Storage**: Support for ChromaDB and other vector databases
- **Configuration Management**: Persistent configuration with workdir support

## Installation

### Basic Installation

```bash
# Basic installation with core functionality
pip install agraph

# Or with uv (recommended)
uv add agraph
```

### Optional Dependencies

AGraph provides optional dependency groups for specific use cases:

```bash
# API Server Dependencies
uv add "agraph[api]"        # FastAPI web server
uv add "agraph[server]"     # Production server with Gunicorn
uv add "agraph[all]"        # All optional dependencies

# Individual components
uv add "agraph[vectordb]"   # Enhanced vector database support
uv add "agraph[jupyter]"    # Jupyter notebook support
```

### Development Installation

```bash
# Clone repository
git clone https://github.com/JasonDZS/agraph.git
cd agraph

# Install with development dependencies
make install-dev
# or
uv add -e ".[dev]"
```

### Installation Options Summary

| Option | Dependencies | Use Case |
|--------|-------------|----------|
| `agraph` | Core only | Basic knowledge graph functionality |
| `agraph[api]` | + FastAPI, Uvicorn | REST API development |
| `agraph[server]` | + API + Gunicorn | Production deployment |
| `agraph[vectordb]` | + ChromaDB | Enhanced vector operations |
| `agraph[jupyter]` | + Jupyter support | Notebook development |
| `agraph[all]` | All above | Complete installation |
| `agraph[dev]` | All + dev tools | Development environment |

## Quick Start

### Basic Usage

```python
#!/usr/bin/env python3
"""
AGraph å¿«é€Ÿå¼€å§‹ç¤ºä¾‹ (Pipelineæ¶æ„ç‰ˆæœ¬)
"""

import asyncio
import sys
import time
from pathlib import Path
from agraph import AGraph, get_settings
from agraph.config import update_settings, save_config_to_workdir

# è®¾ç½®å·¥ä½œç›®å½•å¹¶ä¿å­˜é…ç½®
workdir = "./workdir/agraph_quickstart-cache"
update_settings({"workdir": workdir})

# ä¿å­˜é…ç½®åˆ°å·¥ä½œç›®å½•
try:
    config_path = save_config_to_workdir()
    print(f"âœ… é…ç½®å·²ä¿å­˜åˆ°: {config_path}")
except Exception as e:
    print(f"âš ï¸  é…ç½®ä¿å­˜å¤±è´¥: {e}")

settings = get_settings()

async def main():
    print("ğŸš€ AGraph å¿«é€Ÿå¼€å§‹ç¤ºä¾‹")
    print("=" * 40)

    # ç¤ºä¾‹æ–‡æœ¬æ•°æ®
    sample_texts = [
        "Apple Inc. is an American multinational technology company.",
        "Apple Inc. is headquartered in Cupertino, California.",
        "Apple Inc. develops iPhone and iPad products.",
        "The company was founded by Steve Jobs, Steve Wozniak, and Ronald Wayne.",
        "Apple is known for its innovative design and user experience."
    ]

    # 1. åˆ›å»ºAGraphå®ä¾‹å¹¶åˆå§‹åŒ– (Pipelineæ¶æ„)
    print("\nğŸ“¦ 1. åˆå§‹åŒ–AGraph (Pipelineæ¶æ„)...")
    print("   ğŸ—ï¸ ä½¿ç”¨æ–°çš„Pipelineæ¶æ„ (83%å¤æ‚åº¦é™ä½)")
    print("   âš¡ æ™ºèƒ½ç¼“å­˜å’Œé”™è¯¯æ¢å¤")
    print("   ğŸ“Š è¯¦ç»†çš„æ€§èƒ½ç›‘æ§å’ŒæŒ‡æ ‡")

    async with AGraph(
        collection_name="quickstart_demo",
        persist_directory=settings.workdir,
        vector_store_type="chroma",
        use_openai_embeddings=True,
        enable_knowledge_graph=True,  # å¯ç”¨çŸ¥è¯†å›¾è°±åŠŸèƒ½
    ) as agraph:
        await agraph.initialize()
        print("âœ… AGraphåˆå§‹åŒ–æˆåŠŸ (å†…éƒ¨ä½¿ç”¨Pipelineæ¶æ„)")

        # 2. ä»æ–‡æœ¬æ„å»ºçŸ¥è¯†å›¾è°± (ä½¿ç”¨Pipelineæ¶æ„)
        print("\nğŸ—ï¸ 2. æ„å»ºçŸ¥è¯†å›¾è°± (Pipelineæ¶æ„)...")
        print("   ğŸ“‹ Pipelineæ­¥éª¤: æ–‡æœ¬åˆ†å— â†’ å®ä½“æå– â†’ å…³ç³»æå– â†’ èšç±» â†’ ç»„è£…")

        start_time = time.time()
        knowledge_graph = await agraph.build_from_texts(
            texts=sample_texts,
            graph_name="ç§‘æŠ€å…¬å¸çŸ¥è¯†å›¾è°±",
            graph_description="å…³äºç§‘æŠ€å…¬å¸çš„åŸºç¡€çŸ¥è¯†å›¾è°±",
            use_cache=True,  # å¯ç”¨ç¼“å­˜ä»¥åŠ å¿«åç»­æ„å»ºé€Ÿåº¦
            save_to_vector_store=True,  # ä¿å­˜åˆ°å‘é‡å­˜å‚¨
        )
        build_time = time.time() - start_time

        print("âœ… çŸ¥è¯†å›¾è°±æ„å»ºæˆåŠŸ!")
        print(f"   â±ï¸ æ„å»ºæ—¶é—´: {build_time:.2f}ç§’ (Pipelineä¼˜åŒ–)")
        print(f"   ğŸ“Š å®ä½“: {len(knowledge_graph.entities)} ä¸ª")
        print(f"   ğŸ”— å…³ç³»: {len(knowledge_graph.relations)} ä¸ª")
        print(f"   ğŸ“„ æ–‡æœ¬å—: {len(knowledge_graph.text_chunks)} ä¸ª")

        # 3. è¯­ä¹‰æœç´¢æ¼”ç¤º
        print("\nğŸ” 3. è¯­ä¹‰æœç´¢æ¼”ç¤º...")

        # æœç´¢å®ä½“
        print("æœç´¢å®ä½“ 'technology company':")
        entities = await agraph.search_entities("technology company", top_k=3)
        for i, (entity, score) in enumerate(entities):
            print(f"   {i+1}. {entity.name} ({entity.entity_type})")

        # æœç´¢æ–‡æœ¬
        print("\næœç´¢æ–‡æœ¬ 'headquarters':")
        text_chunks = await agraph.search_text_chunks("headquarters", top_k=2)
        for i, (chunk, score) in enumerate(text_chunks):
            preview = chunk.content[:60] + "..." if len(chunk.content) > 60 else chunk.content
            print(f"   {i+1}. {preview}")

        # 4. æ™ºèƒ½é—®ç­”æ¼”ç¤º
        print("\nğŸ’¬ 4. æ™ºèƒ½é—®ç­”æ¼”ç¤º...")
        questions = [
            "Where is Apple Inc. headquartered?",
            "Who founded Apple?",
            "What products does Apple develop?"
        ]

        for i, question in enumerate(questions):
            print(f"\nâ“ é—®é¢˜ {i+1}: {question}")
            try:
                # æµå¼è°ƒç”¨
                async for chunk_data in await agraph.chat(question, stream=True):
                    if chunk_data["chunk"]:
                        print(chunk_data["chunk"], end="", flush=True)
                    if chunk_data["finished"]:
                        print(f"\n   ğŸ“Š æ£€ç´¢äº† {len(chunk_data['context'].get('entities', []))} ä¸ªå®ä½“")
                        break
            except Exception as e:
                print(f"ğŸ¤– å›ç­”: æŠ±æ­‰ï¼Œæ— æ³•å›ç­”è¿™ä¸ªé—®é¢˜: {e}")

        # 5. ç³»ç»Ÿä¿¡æ¯
        print("\nğŸ“Š 5. ç³»ç»Ÿä¿¡æ¯...")
        stats = await agraph.get_stats()
        if 'vector_store' in stats:
            vs_stats = stats['vector_store']
            print("å‘é‡å­˜å‚¨:")
            print(f"   - å®ä½“: {vs_stats.get('entities', 0)}")
            print(f"   - å…³ç³»: {vs_stats.get('relations', 0)}")
            print(f"   - æ–‡æœ¬å—: {vs_stats.get('text_chunks', 0)}")

    print("\nâœ… å¿«é€Ÿå¼€å§‹æ¼”ç¤ºå®Œæˆ!")

if __name__ == "__main__":
    asyncio.run(main())
```

### REST API Server

AGraph provides a comprehensive REST API server for web applications:

```bash
# Install API dependencies
uv add "agraph[api]"

# Start development server
agraph-server --host 0.0.0.0 --port 8000 --reload

# Start production server
agraph-server --host 0.0.0.0 --port 8000 --workers 4
```

#### API Server Features

- **FastAPI-based**: Modern, fast, and auto-documented REST API
- **Modular Architecture**: Router-based design for maintainability
- **Auto-documentation**: Swagger UI at `/docs` and ReDoc at `/redoc`
- **Streaming Support**: Real-time streaming chat responses
- **Cache Inspection**: View cached entities, relations, and clusters
- **Production Ready**: Gunicorn support for production deployment

#### API Endpoints

- **Configuration**: `GET/POST /config` - Manage system configuration
- **Document Processing**: `POST /documents/upload`, `POST /documents/from-text`
- **Knowledge Graph**: `POST /knowledge-graph/build` - Build graphs from data
- **Search**: `POST /search` - Search entities, relations, and text chunks
- **Chat**: `POST /chat`, `POST /chat/stream` - RAG-based conversations
- **Cache**: `GET /cache/{type}` - View cached data with pagination
- **System**: `GET /system/stats`, `POST /system/clear-all` - System management

#### Server Options

```bash
agraph-server --help                    # Show all options
agraph-server --host 0.0.0.0           # Bind to all interfaces
agraph-server --port 8080              # Custom port
agraph-server --reload                 # Auto-reload for development
agraph-server --workers 4              # Multiple workers for production
agraph-server --log-level debug        # Set logging level
agraph-server --env-file .env          # Load environment variables
```

#### Production Deployment

```bash
# Install production dependencies
uv add "agraph[server]"

# Run with Gunicorn (production)
gunicorn -w 4 -k uvicorn.workers.UvicornWorker -b 0.0.0.0:8000 agraph.api.app:app

# Or use the built-in production command
python -c "from agraph.api.server import run_with_gunicorn; run_with_gunicorn()"
```

### Environment Configuration

Create a `.env` file to configure OpenAI API and other settings:

```env
# OpenAI Configuration
OPENAI_API_KEY=your_openai_api_key
OPENAI_BASE_URL=https://api.openai.com/v1
OPENAI_MODEL=gpt-3.5-turbo

# Workdir Configuration (optional)
AGRAPH_WORKDIR=./workdir/agraph-cache
```

### Configuration Management

AGraph provides persistent configuration management:

```python
from agraph.config import update_settings, save_config_to_workdir, get_settings

# Update settings programmatically
update_settings({"workdir": "./custom/workdir"})

# Save configuration to workdir for persistence
config_path = save_config_to_workdir()
print(f"é…ç½®å·²ä¿å­˜åˆ°: {config_path}")

# Get current settings
settings = get_settings()
print(f"å½“å‰å·¥ä½œç›®å½•: {settings.workdir}")
```

## Detailed Example

See `examples/agraph_quickstart.py` for a comprehensive example featuring the new Pipeline architecture:

### Pipeline Architecture Features
- **83% Complexity Reduction**: Streamlined processing pipeline
- **Intelligent Caching**: Smart caching with error recovery
- **Performance Monitoring**: Detailed metrics and timing information
- **Enhanced Error Handling**: Robust error recovery mechanisms

### Example Features Demonstrated
- Reading multiple document formats (`.txt`, `.md`, `.json`, `.csv`)
- Pipeline-based knowledge graph construction
- Semantic search for entities and text chunks
- Streaming Q&A conversations with context retrieval
- System statistics and performance monitoring
- Configuration management with persistent settings

### Pipeline Processing Steps
1. **Text Chunking** (æ–‡æœ¬åˆ†å—): Intelligent text segmentation
2. **Entity Extraction** (å®ä½“æå–): Advanced entity recognition
3. **Relation Extraction** (å…³ç³»æå–): Relationship identification
4. **Clustering** (èšç±»): Entity and relation clustering
5. **Graph Assembly** (ç»„è£…): Final knowledge graph construction

Run the example:

```bash
# Ensure there are text files in examples/documents/ directory
python examples/agraph_quickstart.py
```

### Sample Output
```
ğŸš€ AGraph å¿«é€Ÿå¼€å§‹ç¤ºä¾‹
========================================
ğŸ“¦ 1. åˆå§‹åŒ–AGraph (Pipelineæ¶æ„)...
   ğŸ—ï¸ ä½¿ç”¨æ–°çš„Pipelineæ¶æ„ (83%å¤æ‚åº¦é™ä½)
   âš¡ æ™ºèƒ½ç¼“å­˜å’Œé”™è¯¯æ¢å¤
   ğŸ“Š è¯¦ç»†çš„æ€§èƒ½ç›‘æ§å’ŒæŒ‡æ ‡
âœ… AGraphåˆå§‹åŒ–æˆåŠŸ (å†…éƒ¨ä½¿ç”¨Pipelineæ¶æ„)

ğŸ—ï¸ 2. æ„å»ºçŸ¥è¯†å›¾è°± (Pipelineæ¶æ„)...
   ğŸ“‹ Pipelineæ­¥éª¤: æ–‡æœ¬åˆ†å— â†’ å®ä½“æå– â†’ å…³ç³»æå– â†’ èšç±» â†’ ç»„è£…
âœ… çŸ¥è¯†å›¾è°±æ„å»ºæˆåŠŸ!
   â±ï¸ æ„å»ºæ—¶é—´: 2.34ç§’ (Pipelineä¼˜åŒ–)
   ğŸ“Š å®ä½“: 15 ä¸ª
   ğŸ”— å…³ç³»: 8 ä¸ª
   ğŸ“„ æ–‡æœ¬å—: 5 ä¸ª
```

## Development

### Development Commands

```bash
# Run tests
make test

# Format code
make format

# Lint code
make lint

# Build package
make build

# Clean build files
make clean
```

### Code Quality

The project follows strict code quality standards:

- **Black** code formatting (line length 100)
- **isort** import sorting
- **mypy** type checking
- **flake8** style checking
- **pylint** code quality analysis

## Dependencies

### Core Dependencies

- `chromadb`: Vector database
- `openai`: OpenAI API client
- `pydantic`: Data validation and settings management
- `networkx`: Graph data structures
- `loguru`: Logging management
- `tiktoken`: Text tokenization

### Optional Dependency Groups

- `vectordb`: ChromaDB vector database support
- `jupyter`: Jupyter environment support
- `dev`: Development tools and testing framework
- `docs`: Documentation generation tools

## License

MIT License

## Contributing

Contributions are welcome! Please follow these steps:

1. Fork the project
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## Support

- [Documentation](https://agraph.readthedocs.io)
- [Issue Tracker](https://github.com/JasonDZS/agraph/issues)
- [Discussions](https://github.com/JasonDZS/agraph/discussions)
