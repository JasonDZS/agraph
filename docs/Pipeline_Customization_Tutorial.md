# AGraph ç®¡é“å®šåˆ¶èƒ½åŠ› - ä½¿ç”¨æ•™ç¨‹

## ğŸ“‹ ç›®å½•
- [ç®€ä»‹](#ç®€ä»‹)
- [å¿«é€Ÿå¼€å§‹](#å¿«é€Ÿå¼€å§‹)
- [ç®¡é“æ¶æ„æ¦‚è¿°](#ç®¡é“æ¶æ„æ¦‚è¿°)
- [åŸºç¡€ç®¡é“ä½¿ç”¨](#åŸºç¡€ç®¡é“ä½¿ç”¨)
- [è‡ªå®šä¹‰ç®¡é“åˆ›å»º](#è‡ªå®šä¹‰ç®¡é“åˆ›å»º)
- [é«˜çº§ç®¡é“å®šåˆ¶](#é«˜çº§ç®¡é“å®šåˆ¶)
- [æ€§èƒ½ç›‘æ§å’Œè°ƒè¯•](#æ€§èƒ½ç›‘æ§å’Œè°ƒè¯•)
- [æœ€ä½³å®è·µ](#æœ€ä½³å®è·µ)
- [æ•…éšœæ’é™¤](#æ•…éšœæ’é™¤)

---

## ç®€ä»‹

AGraph v0.2.1+ å¼•å…¥äº†å…¨æ–°çš„ç®¡é“æ¶æ„ï¼Œå…è®¸ç”¨æˆ·çµæ´»å®šåˆ¶çŸ¥è¯†å›¾è°±æ„å»ºæµç¨‹ã€‚ç›¸æ¯”åŸæœ‰çš„å•ä½“æ–¹æ³•ï¼Œæ–°çš„ç®¡é“ç³»ç»Ÿæä¾›äº†ï¼š

- âœ… **æ¨¡å—åŒ–è®¾è®¡**: æ¯ä¸ªæ­¥éª¤ç‹¬ç«‹ä¸”å¯å¤ç”¨
- âœ… **çµæ´»é…ç½®**: æ”¯æŒè·³è¿‡ã€æ›¿æ¢æˆ–æ·»åŠ è‡ªå®šä¹‰æ­¥éª¤
- âœ… **æ€§èƒ½ç›‘æ§**: å†…ç½®çš„æ‰§è¡ŒæŒ‡æ ‡å’Œé”™è¯¯è·Ÿè¸ª
- âœ… **å¹¶è¡Œå°±ç»ª**: ä¸ºæœªæ¥çš„å¹¶è¡Œæ‰§è¡Œåšå¥½å‡†å¤‡
- âœ… **æ— ç¼å…¼å®¹**: ç°æœ‰ä»£ç æ— éœ€ä¿®æ”¹å³å¯å—ç›Š

---

## å¿«é€Ÿå¼€å§‹

### æ— ä¿®æ”¹å‡çº§ (æ¨è)

ç°æœ‰ç”¨æˆ·æ— éœ€ä¿®æ”¹ä»»ä½•ä»£ç ï¼Œè‡ªåŠ¨äº«å—æ–°æ¶æ„çš„æ€§èƒ½æå‡ï¼š

```python
from agraph.builder import KnowledgeGraphBuilder

# å®Œå…¨ç›¸åŒçš„APIï¼Œå†…éƒ¨è‡ªåŠ¨ä½¿ç”¨æ–°ç®¡é“æ¶æ„
builder = KnowledgeGraphBuilder()
texts = [\"Apple Inc. is a technology company.\", \"Microsoft was founded by Bill Gates.\"]
kg = await builder.build_from_text(texts, graph_name=\"tech_companies\")
```

### æ˜ç¡®ä½¿ç”¨æ–°æ¶æ„

å¦‚æœæ‚¨å¸Œæœ›æ˜ç¡®ä½¿ç”¨æ–°çš„ç®¡é“åŠŸèƒ½ï¼š

```python
from agraph.builder.builder import KnowledgeGraphBuilder

builder = KnowledgeGraphBuilder()
texts = [\"Apple Inc. is a technology company.\", \"Microsoft was founded by Bill Gates.\"]
kg = await builder.build_from_text(texts, graph_name =\"tech_companies\")

# è·å–é¢å¤–çš„ç®¡é“æŒ‡æ ‡
metrics = builder.get_pipeline_metrics()
print(f\"æ„å»ºçŠ¶æ€: {metrics}\")
```

---

## ç®¡é“æ¶æ„æ¦‚è¿°

### æ ¸å¿ƒç»„ä»¶

```mermaid
graph LR
    A[BuildContext] --> B[BuildPipeline]
    B --> C[BuildStep]
    C --> D[StepResult]
    E[PipelineFactory] --> B
    F[PipelineBuilder] --> B
```

#### å…³é”®æ¦‚å¿µ

1. **BuildContext**: ç®¡é“æ‰§è¡Œä¸Šä¸‹æ–‡ï¼Œå­˜å‚¨è¾“å…¥æ•°æ®ã€ä¸­é—´ç»“æœå’ŒçŠ¶æ€ä¿¡æ¯
2. **BuildPipeline**: ç®¡é“ç¼–æ’å™¨ï¼ŒæŒ‰åºæ‰§è¡Œå„ä¸ªæ­¥éª¤
3. **BuildStep**: æŠ½è±¡æ­¥éª¤ç±»ï¼Œæ‰€æœ‰å¤„ç†æ­¥éª¤çš„åŸºç±»
4. **StepResult**: æ­¥éª¤æ‰§è¡Œç»“æœï¼ŒåŒ…å«æ•°æ®ã€é”™è¯¯ä¿¡æ¯å’Œå…ƒæ•°æ®
5. **PipelineFactory**: ç®¡é“å·¥å‚ï¼Œåˆ›å»ºé¢„è®¾ç®¡é“é…ç½®
6. **PipelineBuilder**: ç®¡é“æ„å»ºå™¨ï¼Œä½¿ç”¨æµå¼APIåˆ›å»ºè‡ªå®šä¹‰ç®¡é“

### æ ‡å‡†æ„å»ºæ­¥éª¤

| æ­¥éª¤ | åŠŸèƒ½ | è¾“å…¥ | è¾“å‡º |
|------|------|------|------|
| **DocumentProcessing** | æ–‡æ¡£è§£æ | æ–‡æ¡£è·¯å¾„ | æå–çš„æ–‡æœ¬ |
| **TextChunking** | æ–‡æœ¬åˆ†å— | æ–‡æœ¬åˆ—è¡¨ | æ–‡æœ¬å—åˆ—è¡¨ |
| **EntityExtraction** | å®ä½“æå– | æ–‡æœ¬å— | å®ä½“åˆ—è¡¨ |
| **RelationExtraction** | å…³ç³»æå– | æ–‡æœ¬å—+å®ä½“ | å…³ç³»åˆ—è¡¨ |
| **ClusterFormation** | èšç±»åˆ†æ | å®ä½“+å…³ç³» | é›†ç¾¤åˆ—è¡¨ |
| **GraphAssembly** | å›¾è°±ç»„è£… | æ‰€æœ‰ç»„ä»¶ | çŸ¥è¯†å›¾è°± |

---

## åŸºç¡€ç®¡é“ä½¿ç”¨

### 1. é¢„è®¾ç®¡é“ç±»å‹

```python
from agraph.builder.builder import KnowledgeGraphBuilder

builder = KnowledgeGraphBuilder()

# 1. æ ‡å‡†ç®¡é“ (åŒ…å«æ–‡æ¡£å¤„ç†)
documents = [\"document1.pdf\", \"document2.docx\"]
kg = await builder.build_from_documents(documents)

# 2. æ–‡æœ¬ç®¡é“ (è·³è¿‡æ–‡æ¡£å¤„ç†ï¼Œä¼˜åŒ–æ€§èƒ½)
texts = [\"Direct text input...\"]
kg = await builder.build_from_text(texts)

# 3. æœ€å°ç®¡é“ (ä»…æ–‡æœ¬åˆ†å—å’Œå›¾è°±ç»„è£…)
minimal_pipeline = builder.create_minimal_pipeline()
# é€‚ç”¨äºå¿«é€ŸåŸå‹æˆ–ç®€å•æ–‡æœ¬å¤„ç†

# 4. ç¦ç”¨çŸ¥è¯†å›¾è°±åŠŸèƒ½çš„ç®¡é“
builder_simple = KnowledgeGraphBuilder(enable_knowledge_graph = False)
kg = await builder_simple.build_from_text(texts)  # ä»…å¤„ç†æ–‡æœ¬å—
```

### 2. ç¼“å­˜å’Œæ¢å¤æ§åˆ¶

```python
# ä½¿ç”¨ç¼“å­˜åŠ é€Ÿé‡å¤æ„å»º
kg = await builder.build_from_text(
    texts,
    use_cache=True,  # å¯ç”¨ç¼“å­˜
    graph_name=\"cached_graph\"
)

# ä»ç‰¹å®šæ­¥éª¤æ¢å¤æ„å»º (è·³è¿‡å·²å®Œæˆçš„æ­¥éª¤)
kg = await builder.build_from_text(
    texts,
    from_step=\"entity_extraction\",  # ä»å®ä½“æå–å¼€å§‹
    graph_name=\"resumed_graph\"
)

# æ¸…ç†ç¼“å­˜
builder.clear_cache()  # æ¸…ç†æ‰€æœ‰ç¼“å­˜
builder.clear_cache(from_step=\"relation_extraction\")  # æ¸…ç†ç‰¹å®šæ­¥éª¤åçš„ç¼“å­˜
```

---

## è‡ªå®šä¹‰ç®¡é“åˆ›å»º

### 1. ä½¿ç”¨å·¥å‚æ–¹æ³•

```python
from agraph.builder.builder import KnowledgeGraphBuilder
from agraph.config import BuildSteps

builder = KnowledgeGraphBuilder()

# åˆ›å»ºä»…åŒ…å«ç‰¹å®šæ­¥éª¤çš„ç®¡é“
custom_config = {
    BuildSteps.TEXT_CHUNKING: builder.text_chunker_handler,
    BuildSteps.ENTITY_EXTRACTION: builder.entity_handler,
    BuildSteps.GRAPH_ASSEMBLY: builder.graph_assembler
}

custom_pipeline = builder.create_custom_pipeline(custom_config)

# æ‰‹åŠ¨æ‰§è¡Œè‡ªå®šä¹‰ç®¡é“
from agraph.builder.steps.context import BuildContext

context = BuildContext(
    texts = [\"Custom pipeline text processing...\"],
graph_name =\"custom_graph\"
)

kg = await custom_pipeline.execute(context)
```

### 2. ä½¿ç”¨Builderæ¨¡å¼

```python
# ä½¿ç”¨æµå¼APIæ„å»ºç®¡é“
custom_pipeline = (builder.pipeline_builder
    .with_text_chunking(builder.text_chunker_handler)
    .with_entity_extraction(builder.entity_handler)
    .with_relation_extraction(builder.relation_handler)
    .with_graph_assembly(builder.graph_assembler)
    .build())

# æ‰§è¡Œè‡ªå®šä¹‰ç®¡é“
context = BuildContext(texts=texts, graph_name=\"fluent_graph\")
kg = await custom_pipeline.execute(context)
```

### 3. æ¡ä»¶æ­¥éª¤æ‰§è¡Œ

```python
# æ ¹æ®æ•°æ®ç‰¹å¾åŠ¨æ€å†³å®šæ­¥éª¤
def create_adaptive_pipeline(text_length: int) -> BuildPipeline:
    builder_instance = builder.pipeline_builder

    # æ€»æ˜¯åŒ…å«æ–‡æœ¬åˆ†å—
    builder_instance.with_text_chunking(builder.text_chunker_handler)

    # ä»…å¯¹é•¿æ–‡æœ¬è¿›è¡Œå®ä½“æå–
    if text_length > 1000:
        builder_instance.with_entity_extraction(builder.entity_handler)
        builder_instance.with_relation_extraction(builder.relation_handler)

    # ä»…å¯¹å¤æ‚æ–‡æ¡£è¿›è¡Œèšç±»
    if text_length > 5000:
        builder_instance.with_cluster_formation(builder.cluster_handler)

    # æ€»æ˜¯åŒ…å«å›¾è°±ç»„è£…
    builder_instance.with_graph_assembly(builder.graph_assembler)

    return builder_instance.build()

# ä½¿ç”¨è‡ªé€‚åº”ç®¡é“
total_length = sum(len(text) for text in texts)
adaptive_pipeline = create_adaptive_pipeline(total_length)
context = BuildContext(texts=texts, graph_name=\"adaptive_graph\")
kg = await adaptive_pipeline.execute(context)
```

---

## é«˜çº§ç®¡é“å®šåˆ¶

### 1. è‡ªå®šä¹‰æ­¥éª¤å¼€å‘

åˆ›å»ºè‡ªå®šä¹‰å¤„ç†æ­¥éª¤ï¼š

```python
from agraph.builder.steps.base import BuildStep, StepResult
from agraph.builder.steps.context import BuildContext
from typing import List

class CustomTextPreprocessingStep(BuildStep):
    \"\"\"è‡ªå®šä¹‰æ–‡æœ¬é¢„å¤„ç†æ­¥éª¤\"\"\"

    def __init__(self, cache_manager, preprocessing_config: dict):
        super().__init__(\"custom_preprocessing\", cache_manager)
        self.config = preprocessing_config

    async def _execute_step(self, context: BuildContext) -> StepResult[List[str]]:
        try:
            processed_texts = []
            for text in context.texts:
                # åº”ç”¨è‡ªå®šä¹‰é¢„å¤„ç†é€»è¾‘
                processed_text = self._apply_preprocessing(text)
                processed_texts.append(processed_text)

            return StepResult.success_result(
                processed_texts,
                metadata={
                    \"preprocessing_rules\": len(self.config),
                    \"processed_count\": len(processed_texts)
                }
            )
        except Exception as e:
            return StepResult.failure_result(f\"é¢„å¤„ç†å¤±è´¥: {str(e)}\")

    def _apply_preprocessing(self, text: str) -> str:
        # å®ç°æ‚¨çš„é¢„å¤„ç†é€»è¾‘
        if self.config.get(\"remove_urls\", False):
            import re
            text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)

        if self.config.get(\"normalize_whitespace\", False):
            text = re.sub(r'\\s+', ' ', text.strip())

        return text

    def _get_cache_input_data(self, context: BuildContext):
        return (context.texts, str(self.config))

    def _get_expected_result_type(self):
        return list

# ä½¿ç”¨è‡ªå®šä¹‰æ­¥éª¤
preprocessing_config = {\"remove_urls\": True, \"normalize_whitespace\": True}
custom_step = CustomTextPreprocessingStep(builder.cache_manager, preprocessing_config)

# å°†è‡ªå®šä¹‰æ­¥éª¤æ’å…¥ç®¡é“
from agraph.builder.pipeline import BuildPipeline
custom_pipeline = BuildPipeline(builder.cache_manager)
custom_pipeline.add_step(custom_step)  # æ·»åŠ é¢„å¤„ç†æ­¥éª¤
custom_pipeline.add_step(TextChunkingStep(builder.text_chunker_handler, builder.cache_manager))
custom_pipeline.add_step(GraphAssemblyStep(builder.graph_assembler, builder.cache_manager))
```

### 2. å¹¶è¡Œæ­¥éª¤é…ç½® (å®éªŒæ€§)

```python
# ä¸ºæœªæ¥çš„å¹¶è¡Œæ‰§è¡Œå‡†å¤‡ç®¡é“ç»“æ„
parallel_ready_pipeline = builder.pipeline_factory.create_parallel_pipeline(
    text_chunker_handler=builder.text_chunker_handler,
    entity_handler=builder.entity_handler,
    relation_handler=builder.relation_handler,
    cluster_handler=builder.cluster_handler,
    graph_assembler=builder.graph_assembler
)

# æ³¨æ„: å½“å‰ç‰ˆæœ¬ä»ä¸ºé¡ºåºæ‰§è¡Œï¼Œä½†æ¶æ„å·²ä¸ºå¹¶è¡Œåšå¥½å‡†å¤‡
```

### 3. æ­¥éª¤çº§åˆ«é…ç½®

```python
# ä¸ºç‰¹å®šæ­¥éª¤é…ç½®ä¸åŒçš„å¤„ç†å‚æ•°
from agraph.builder.handler.entity_handler import EntityHandler
from agraph.builder.extractors import LLMEntityExtractor

# åˆ›å»ºé«˜ç½®ä¿¡åº¦çš„å®ä½“æå–å™¨
high_confidence_extractor = LLMEntityExtractor({
    \"llm_provider\": \"openai\",
    \"llm_model\": \"gpt-4\",
    \"min_confidence\": 0.9  # æé«˜ç½®ä¿¡åº¦é˜ˆå€¼
})

high_confidence_handler = EntityHandler(builder.cache_manager, high_confidence_extractor)

# ä½¿ç”¨å®šåˆ¶çš„å¤„ç†å™¨åˆ›å»ºç®¡é“
precision_pipeline = (builder.pipeline_builder
    .with_text_chunking(builder.text_chunker_handler)
    .with_entity_extraction(high_confidence_handler)  # ä½¿ç”¨é«˜ç²¾åº¦æå–å™¨
    .with_graph_assembly(builder.graph_assembler)
    .build())
```

---

## æ€§èƒ½ç›‘æ§å’Œè°ƒè¯•

### 1. æ‰§è¡ŒæŒ‡æ ‡æ”¶é›†

```python
# è·å–è¯¦ç»†çš„æ‰§è¡ŒæŒ‡æ ‡
metrics = builder.get_pipeline_metrics()
print(\"ç®¡é“æ‰§è¡ŒæŒ‡æ ‡:\")
print(f\"ç¼“å­˜ä¿¡æ¯: {metrics['cache_info']}\")
print(f\"æ„å»ºçŠ¶æ€: {metrics['build_status']}\")

# è·å–ç®¡é“çº§åˆ«çš„æ€§èƒ½æ•°æ®
pipeline = builder.pipeline_factory.create_text_only_pipeline(...)
pipeline_metrics = pipeline.get_pipeline_metrics()

print(\"ç®¡é“æ€§èƒ½æŒ‡æ ‡:\")
print(f\"æ€»æ‰§è¡Œæ¬¡æ•°: {pipeline_metrics['execution_metrics']['total_executions']}\")
print(f\"æˆåŠŸç‡: {pipeline_metrics['execution_metrics']['success_rate_percent']:.1f}%\")
print(f\"å¹³å‡æ‰§è¡Œæ—¶é—´: {pipeline_metrics['execution_metrics']['average_execution_time']:.2f}ç§’\")
```

### 2. æ­¥éª¤çº§åˆ«è°ƒè¯•

```python
# å¯ç”¨è¯¦ç»†æ—¥å¿—è®°å½•
import logging
from agraph.logger import logger

logger.setLevel(logging.DEBUG)

# æ‰§è¡Œç®¡é“å¹¶æŸ¥çœ‹æ¯ä¸ªæ­¥éª¤çš„è¯¦ç»†ä¿¡æ¯
context = BuildContext(texts=texts, graph_name=\"debug_graph\")
kg = await pipeline.execute(context)

# æ£€æŸ¥æ‰§è¡Œæ‘˜è¦
summary = context.get_execution_summary()
print(f\"å®Œæˆçš„æ­¥éª¤: {summary['completed_steps']}\")
print(f\"è·³è¿‡çš„æ­¥éª¤: {summary['skipped_steps']}\")
print(f\"é”™è¯¯æ•°é‡: {summary['error_count']}\")

# è·å–ç‰¹å®šæ­¥éª¤çš„å…ƒæ•°æ®
entity_metadata = context.get_metadata_for_step(\"entity_extraction\")
print(f\"å®ä½“æå–æŒ‡æ ‡: {entity_metadata}\")
```

### 3. é”™è¯¯å¤„ç†å’Œæ¢å¤

```python
try:
    kg = await builder.build_from_text(texts)
except Exception as e:
    print(f\"æ„å»ºå¤±è´¥: {e}\")

    # æ£€æŸ¥æ„å»ºçŠ¶æ€ä»¥äº†è§£å¤±è´¥ä½ç½®
    status = builder.get_build_status()
    print(f\"å½“å‰æ­¥éª¤: {status.get('current_step')}\")
    print(f\"é”™è¯¯æ¶ˆæ¯: {status.get('error_message')}\")

    # å°è¯•ä»ä¸Šä¸€ä¸ªæˆåŠŸçš„æ­¥éª¤æ¢å¤
    if status.get('completed_step'):
        print(f\"å°è¯•ä» {status['completed_step']} åçš„æ­¥éª¤æ¢å¤...\")
        # è¿™é‡Œå¯ä»¥å®ç°æ¢å¤é€»è¾‘
```

---

## æœ€ä½³å®è·µ

### 1. é€‰æ‹©åˆé€‚çš„ç®¡é“ç±»å‹

```python
# æ ¹æ®ç”¨ä¾‹é€‰æ‹©æœ€ä¼˜ç®¡é“

# ğŸ“„ å¤„ç†å¤§é‡æ–‡æ¡£ - ä½¿ç”¨æ ‡å‡†ç®¡é“
if input_type == \"documents\":
    kg = await builder.build_from_documents(documents)

# ğŸ“ å¤„ç†çº¯æ–‡æœ¬ - ä½¿ç”¨æ–‡æœ¬ç®¡é“ (æ›´å¿«)
elif input_type == \"texts\":
    kg = await builder.build_from_text(texts)

# âš¡ å¿«é€ŸåŸå‹æˆ–ç®€å•å¤„ç† - ä½¿ç”¨æœ€å°ç®¡é“
elif need_speed:
    minimal_pipeline = builder.create_minimal_pipeline()
    context = BuildContext(texts=texts)
    kg = await minimal_pipeline.execute(context)

# ğŸ¯ ç‰¹å®šéœ€æ±‚ - ä½¿ç”¨è‡ªå®šä¹‰ç®¡é“
else:
    custom_pipeline = builder.create_custom_pipeline(custom_config)
```

### 2. ç¼“å­˜ç­–ç•¥ä¼˜åŒ–

```python
# å¼€å‘é˜¶æ®µ - å…³é—­ç¼“å­˜ä»¥è·å¾—æœ€æ–°ç»“æœ
kg = await builder.build_from_text(texts, use_cache=False)

# ç”Ÿäº§ç¯å¢ƒ - å¯ç”¨ç¼“å­˜ä»¥æé«˜æ€§èƒ½
kg = await builder.build_from_text(texts, use_cache=True)

# å¢é‡å¤„ç† - ä»ç‰¹å®šæ­¥éª¤å¼€å§‹
if need_update_entities_only:
    kg = await builder.build_from_text(
        texts,
        from_step=\"entity_extraction\",
        use_cache=True
    )

# å®šæœŸæ¸…ç† - é˜²æ­¢ç¼“å­˜è¿‡å¤§
if cache_size > threshold:
    builder.clear_cache()
```

### 3. èµ„æºç®¡ç†

```python
# ä½¿ç”¨å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨ç¡®ä¿èµ„æºæ¸…ç†
async with KnowledgeGraphBuilder() as builder:
    kg = await builder.build_from_text(texts)
    # è‡ªåŠ¨æ¸…ç†èµ„æº

# æˆ–æ‰‹åŠ¨ç®¡ç†èµ„æº
builder = KnowledgeGraphBuilder()
try:
    kg = await builder.build_from_text(texts)
finally:
    await builder.aclose()  # æ¸…ç†å¼‚æ­¥èµ„æº
```

### 4. æ‰¹é‡å¤„ç†ä¼˜åŒ–

```python
# æ‰¹é‡å¤„ç†å¤šä¸ªæ–‡æ¡£é›†åˆ
async def process_multiple_datasets(datasets: List[List[str]]) -> List[OptimizedKnowledgeGraph]:
    builder = KnowledgeGraphBuilder()
    results = []

    for i, texts in enumerate(datasets):
        try:
            kg = await builder.build_from_text(
                texts,
                graph_name=f\"dataset_{i}\",
                use_cache=True  # åˆ©ç”¨ç¼“å­˜åŠ é€Ÿ
            )
            results.append(kg)
        except Exception as e:
            print(f\"æ•°æ®é›† {i} å¤„ç†å¤±è´¥: {e}\")
            continue

    await builder.aclose()
    return results
```

---

## æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜å’Œè§£å†³æ–¹æ¡ˆ

#### 1. å¯¼å…¥é”™è¯¯

```python
# é”™è¯¯: ImportError: cannot import name 'KnowledgeGraphBuilder'
# è§£å†³: ç¡®ä¿ä½¿ç”¨æ­£ç¡®çš„å¯¼å…¥è·¯å¾„
from agraph.builder.builder import KnowledgeGraphBuilder

# æˆ–ä½¿ç”¨å…¼å®¹æ€§å¯¼å…¥
from agraph.builder import KnowledgeGraphBuilder  # è‡ªåŠ¨ä½¿ç”¨æ–°æ¶æ„
```

#### 2. æ­¥éª¤æ‰§è¡Œå¤±è´¥
```python
# æ£€æŸ¥æ­¥éª¤ä¾èµ–å…³ç³»
context = BuildContext(texts=texts)

# ç¡®ä¿æ­¥éª¤é¡ºåºæ­£ç¡®
correct_order = [
    \"text_chunking\",      # å¿…é¡»é¦–å…ˆæ‰§è¡Œ
    \"entity_extraction\",   # ä¾èµ–äºchunks
    \"relation_extraction\", # ä¾èµ–äºchunkså’Œentities
    \"cluster_formation\",   # ä¾èµ–äºentitieså’Œrelations
    \"graph_assembly\"       # ä¾èµ–äºæ‰€æœ‰ç»„ä»¶
]

# æ£€æŸ¥enable_knowledge_graphè®¾ç½®
if not context.enable_knowledge_graph:
    print(\"çŸ¥è¯†å›¾è°±æ­¥éª¤å°†è¢«è·³è¿‡\")
```

#### 3. ç¼“å­˜é—®é¢˜
```python
# æ¸…ç†æŸåçš„ç¼“å­˜
builder.clear_cache()

# æ£€æŸ¥ç¼“å­˜çŠ¶æ€
cache_info = builder.get_cache_info()
print(f\"ç¼“å­˜çŠ¶æ€: {cache_info}\")

# å¼ºåˆ¶é‡æ–°å¤„ç†ç‰¹å®šæ­¥éª¤
builder.clear_cache(from_step=\"entity_extraction\")
```

#### 4. æ€§èƒ½é—®é¢˜
```python
# å¯ç”¨æ€§èƒ½åˆ†æ
import time
start_time = time.time()

kg = await builder.build_from_text(texts)

end_time = time.time()
print(f\"æ€»æ‰§è¡Œæ—¶é—´: {end_time - start_time:.2f} ç§’\")

# æ£€æŸ¥å„æ­¥éª¤è€—æ—¶
metrics = builder.get_pipeline_metrics()
# åˆ†æå“ªä¸ªæ­¥éª¤æœ€è€—æ—¶å¹¶ä¼˜åŒ–
```

#### 5. å†…å­˜ä½¿ç”¨è¿‡é«˜
```python
# å¯¹äºå¤§å‹æ–‡æ¡£ï¼Œä½¿ç”¨åˆ†æ‰¹å¤„ç†
def chunk_texts(texts: List[str], batch_size: int = 100):
    for i in range(0, len(texts), batch_size):
        yield texts[i:i + batch_size]

results = []
for batch in chunk_texts(large_text_list, batch_size=50):
    kg = await builder.build_from_text(batch)
    results.append(kg)
    # å¯é€‰: åˆå¹¶ç»“æœæˆ–ä¿å­˜ä¸­é—´ç»“æœ
```

---

### è·å–å¸®åŠ©

å¦‚æœé‡åˆ°é—®é¢˜ï¼š

1. **æ£€æŸ¥æ—¥å¿—**: å¯ç”¨DEBUGçº§åˆ«æ—¥å¿—æŸ¥çœ‹è¯¦ç»†ä¿¡æ¯
2. **éªŒè¯é…ç½®**: ç¡®ä¿æ‰€æœ‰é…ç½®å‚æ•°æ­£ç¡®
3. **æµ‹è¯•ç®€åŒ–ç‰ˆæœ¬**: å…ˆç”¨æœ€å°ç®¡é“æµ‹è¯•åŸºæœ¬åŠŸèƒ½
4. **æŸ¥çœ‹æŒ‡æ ‡**: ä½¿ç”¨å†…ç½®æŒ‡æ ‡è¯Šæ–­æ€§èƒ½é—®é¢˜
5. **æ¯”è¾ƒå®ç°**: ä½¿ç”¨MigrationHelperæ¯”è¾ƒæ–°æ—§å®ç°

```python
from agraph.builder.compatibility import MigrationHelper

# å¯¹æ¯”æµ‹è¯•å¸®åŠ©è¯Šæ–­é—®é¢˜
results = MigrationHelper.compare_implementations(
    texts=problem_texts,
    graph_name=\"diagnostic_test\"
)

report = MigrationHelper.generate_migration_report(results)
print(report)
```

---

## æ€»ç»“

AGraphçš„æ–°ç®¡é“æ¶æ„æä¾›äº†å¼ºå¤§çš„å®šåˆ¶èƒ½åŠ›ï¼ŒåŒæ—¶ä¿æŒäº†æ˜“ç”¨æ€§ã€‚å…³é”®ä¼˜åŠ¿ï¼š

- ğŸ”§ **çµæ´»æ€§**: å®Œå…¨å¯å®šåˆ¶çš„å¤„ç†æµç¨‹
- ğŸš€ **æ€§èƒ½**: æ™ºèƒ½ç¼“å­˜å’Œä¼˜åŒ–çš„æ‰§è¡Œè·¯å¾„
- ğŸ” **å¯è§‚æµ‹æ€§**: è¯¦ç»†çš„æŒ‡æ ‡å’Œè°ƒè¯•ä¿¡æ¯
- ğŸ›¡ï¸ **å¯é æ€§**: å¼ºå¤§çš„é”™è¯¯å¤„ç†å’Œæ¢å¤æœºåˆ¶
- ğŸ”„ **å…¼å®¹æ€§**: æ— ç¼å‡çº§è·¯å¾„

å¼€å§‹ä½¿ç”¨æ–°çš„ç®¡é“åŠŸèƒ½ï¼Œäº«å—æ›´å¥½çš„å¼€å‘ä½“éªŒå’Œæ›´é«˜çš„æ€§èƒ½ï¼

---

*æœ¬æ•™ç¨‹åŸºäºAGraph v0.2.1+ã€‚å¦‚æœ‰é—®é¢˜æˆ–å»ºè®®ï¼Œæ¬¢è¿åé¦ˆã€‚*
