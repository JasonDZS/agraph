# AGraph ä½¿ç”¨æ•™ç¨‹

æœ¬æ•™ç¨‹å°†å¼•å¯¼ä½ é€šè¿‡å®é™…ç¤ºä¾‹å­¦ä¹ AGraphçŸ¥è¯†å›¾è°±å·¥å…·åŒ…çš„æ ¸å¿ƒåŠŸèƒ½ã€‚

## ç›®å½•

1. [ç¯å¢ƒå‡†å¤‡](#ç¯å¢ƒå‡†å¤‡)
2. [å¿«é€Ÿå¼€å§‹](#å¿«é€Ÿå¼€å§‹)
3. [æ ¸å¿ƒåŠŸèƒ½è¯¦è§£](#æ ¸å¿ƒåŠŸèƒ½è¯¦è§£)
4. [é«˜çº§ç”¨æ³•](#é«˜çº§ç”¨æ³•)
5. [æœ€ä½³å®è·µ](#æœ€ä½³å®è·µ)
6. [æ•…éšœæ’é™¤](#æ•…éšœæ’é™¤)

## ç¯å¢ƒå‡†å¤‡

### ç³»ç»Ÿè¦æ±‚

- Python 3.7+
- OpenAI APIå¯†é’¥ï¼ˆç”¨äºLLMå’ŒembeddingæœåŠ¡ï¼‰

### å®‰è£…ä¾èµ–

```bash
# å¼€å‘å®‰è£…
make install-dev

# æˆ–è€…åŸºç¡€å®‰è£…
pip install -e .
```

### ç¯å¢ƒé…ç½®

è®¾ç½®ç¯å¢ƒå˜é‡æˆ–åˆ›å»º`.env`æ–‡ä»¶ï¼š

```bash
OPENAI_API_KEY=your_api_key_here
OPENAI_BASE_URL=https://api.openai.com/v1  # å¯é€‰ï¼Œé»˜è®¤å€¼
OPENAI_MODEL=gpt-3.5-turbo  # å¯é€‰ï¼Œé»˜è®¤å€¼
```

## å¿«é€Ÿå¼€å§‹

### 1. åŸºæœ¬è®¾ç½®

```python
import asyncio
from pathlib import Path
from agraph import AGraph, get_settings

# é…ç½®å·¥ä½œç›®å½•
settings = get_settings()
settings.workdir = str(Path("workdir/my_project"))
```

### 2. åˆå§‹åŒ–AGraphå®ä¾‹

```python
async def main():
    async with AGraph(
        collection_name="my_knowledge_graph",
        persist_directory=settings.workdir,
        vector_store_type="chroma",
        use_openai_embeddings=True
    ) as agraph:
        await agraph.initialize()
        print("âœ… AGraphåˆå§‹åŒ–æˆåŠŸ")
```

### 3. å‡†å¤‡æ–‡æ¡£æ•°æ®

AGraphæ”¯æŒå¤šç§æ–‡æ¡£æ ¼å¼ï¼š

```python
# æ–¹å¼1: ç›´æ¥æä¾›æ–‡æœ¬åˆ—è¡¨
sample_texts = [
    "æˆ‘ä»¬å…¬å¸æ˜¯ä¸€å®¶ä¸“æ³¨äºäººå·¥æ™ºèƒ½æŠ€æœ¯çš„ç§‘æŠ€ä¼ä¸šã€‚",
    "å›¢é˜Ÿç”±50åå·¥ç¨‹å¸ˆç»„æˆï¼Œä¸»è¦ç ”å‘æœºå™¨å­¦ä¹ å’Œæ·±åº¦å­¦ä¹ äº§å“ã€‚",
    "å…¬å¸æ€»éƒ¨ä½äºåŒ—äº¬ï¼Œåœ¨ä¸Šæµ·è®¾æœ‰ç ”å‘ä¸­å¿ƒã€‚"
]

# æ–¹å¼2: ä»æ–‡ä»¶è¯»å–
documents_dir = Path("documents")
sample_texts = []

for file_path in documents_dir.glob("*.txt"):
    with open(file_path, 'r', encoding='utf-8') as f:
        sample_texts.append(f.read())
```

### 4. æ„å»ºçŸ¥è¯†å›¾è°±

```python
# ä»æ–‡æœ¬æ„å»ºçŸ¥è¯†å›¾è°±
knowledge_graph = await agraph.build_from_texts(
    texts=sample_texts,
    graph_name="ä¼ä¸šçŸ¥è¯†å›¾è°±",
    graph_description="åŸºäºä¼ä¸šæ–‡æ¡£æ„å»ºçš„çŸ¥è¯†å›¾è°±",
    use_cache=True,  # å¯ç”¨ç¼“å­˜åŠ é€Ÿ
    save_to_vector_store=True  # ä¿å­˜åˆ°å‘é‡å­˜å‚¨
)

print(f"ğŸ“Š æ„å»ºå®Œæˆ: {len(knowledge_graph.entities)} ä¸ªå®ä½“, {len(knowledge_graph.relations)} ä¸ªå…³ç³»")
```

## æ ¸å¿ƒåŠŸèƒ½è¯¦è§£

### è¯­ä¹‰æœç´¢

#### æœç´¢å®ä½“

```python
# æŒ‰åç§°æœç´¢å®ä½“
entities = await agraph.search_entities("å…¬å¸", top_k=5)
for entity, score in entities:
    print(f"å®ä½“: {entity.name} ({entity.entity_type}) - ç›¸ä¼¼åº¦: {score:.3f}")
```

#### æœç´¢æ–‡æœ¬å—

```python
# æŒ‰å†…å®¹æœç´¢æ–‡æœ¬å—
text_chunks = await agraph.search_text_chunks("äººå·¥æ™ºèƒ½æŠ€æœ¯", top_k=3)
for chunk, score in text_chunks:
    preview = chunk.content[:100] + "..." if len(chunk.content) > 100 else chunk.content
    print(f"æ–‡æœ¬: {preview} - ç›¸ä¼¼åº¦: {score:.3f}")
```

### æ™ºèƒ½é—®ç­”

#### åŸºç¡€é—®ç­”

```python
# ç®€å•é—®ç­”
question = "å…¬å¸çš„ä¸»è¦ä¸šåŠ¡æ˜¯ä»€ä¹ˆï¼Ÿ"
response = await agraph.chat(question)
print(f"å›ç­”: {response}")
```

#### æµå¼é—®ç­”

```python
# æµå¼å“åº”ï¼Œå®æ—¶æ˜¾ç¤ºç”Ÿæˆè¿‡ç¨‹
async for chunk_data in await agraph.chat(question, stream=True):
    if chunk_data["chunk"]:
        print(chunk_data["chunk"], end="", flush=True)
    if chunk_data["finished"]:
        print(f"\nâœ… å®Œæ•´å›ç­”: {chunk_data['answer']}")

        # æ˜¾ç¤ºæ£€ç´¢ä¸Šä¸‹æ–‡ä¿¡æ¯
        context = chunk_data['context']
        entities_used = len(context.get('entities', []))
        chunks_used = len(context.get('text_chunks', []))
        print(f"ğŸ“Š ä½¿ç”¨äº† {entities_used} ä¸ªå®ä½“, {chunks_used} ä¸ªæ–‡æ¡£ç‰‡æ®µ")
        break
```

### å›¾è°±åˆ†æ

#### æŸ¥çœ‹ç»Ÿè®¡ä¿¡æ¯

```python
# è·å–ç³»ç»Ÿç»Ÿè®¡
stats = await agraph.get_stats()

if 'vector_store' in stats:
    vs_stats = stats['vector_store']
    print("å‘é‡å­˜å‚¨ç»Ÿè®¡:")
    print(f"  - å®ä½“æ•°: {vs_stats.get('entities', 0)}")
    print(f"  - å…³ç³»æ•°: {vs_stats.get('relations', 0)}")
    print(f"  - æ–‡æœ¬å—æ•°: {vs_stats.get('text_chunks', 0)}")
```

#### å®ä½“å…³ç³»æ¢ç´¢

```python
# è·å–ç‰¹å®šå®ä½“çš„å…³ç³»
entity_name = "å…¬å¸"
entities = await agraph.search_entities(entity_name, top_k=1)
if entities:
    entity = entities[0][0]
    print(f"å®ä½“: {entity.name}")
    print(f"ç±»å‹: {entity.entity_type}")
    print(f"å±æ€§: {entity.properties}")
    print(f"åˆ«å: {entity.aliases}")
```

## é«˜çº§ç”¨æ³•

### ç¼“å­˜æœºåˆ¶

AGraphæä¾›æ™ºèƒ½ç¼“å­˜æ¥æé«˜æ€§èƒ½ï¼š

```python
# å¯ç”¨ç¼“å­˜æ„å»º
knowledge_graph = await agraph.build_from_texts(
    texts=sample_texts,
    graph_name="cached_graph",
    use_cache=True,  # ç¬¬ä¸€æ¬¡æ„å»ºåä¼šç¼“å­˜ç»“æœ
    cache_ttl=3600   # ç¼“å­˜1å°æ—¶
)

# åç»­ç›¸åŒæ–‡æœ¬çš„æ„å»ºå°†ç›´æ¥ä½¿ç”¨ç¼“å­˜
```

### æŒä¹…åŒ–å­˜å‚¨

```python
# æŒ‡å®šæŒä¹…åŒ–ç›®å½•
async with AGraph(
    collection_name="persistent_graph",
    persist_directory="/path/to/storage",
    vector_store_type="chroma"
) as agraph:
    # æ•°æ®ä¼šè‡ªåŠ¨ä¿å­˜åˆ°æŒ‡å®šç›®å½•
    # ä¸‹æ¬¡å¯åŠ¨æ—¶ä¼šè‡ªåŠ¨åŠ è½½
    pass
```

### è‡ªå®šä¹‰é…ç½®

```python
from agraph import get_settings

settings = get_settings()
# è‡ªå®šä¹‰LLMæ¨¡å‹
settings.openai_model = "gpt-4"
# è‡ªå®šä¹‰embeddingæ¨¡å‹
settings.embedding_model = "text-embedding-ada-002"
# è‡ªå®šä¹‰æ–‡æœ¬åˆ†å—å¤§å°
settings.chunk_size = 1000
settings.chunk_overlap = 200
```

## æœ€ä½³å®è·µ

### 1. æ–‡æ¡£é¢„å¤„ç†

```python
def preprocess_texts(texts):
    """æ–‡æœ¬é¢„å¤„ç†æœ€ä½³å®è·µ"""
    processed = []
    for text in texts:
        # æ¸…ç†ç©ºç™½å­—ç¬¦
        text = text.strip()
        # è¿‡æ»¤è¿‡çŸ­çš„æ–‡æœ¬
        if len(text) < 50:
            continue
        # è§„èŒƒåŒ–ç¼–ç 
        text = text.encode('utf-8', errors='ignore').decode('utf-8')
        processed.append(text)
    return processed

sample_texts = preprocess_texts(raw_texts)
```

### 2. é”™è¯¯å¤„ç†

```python
async def robust_build():
    try:
        knowledge_graph = await agraph.build_from_texts(
            texts=sample_texts,
            graph_name="robust_graph",
            use_cache=True
        )
        return knowledge_graph
    except Exception as e:
        print(f"æ„å»ºå¤±è´¥: {e}")
        # é™çº§å¤„ç†æˆ–é‡è¯•é€»è¾‘
        return None
```

### 3. æ€§èƒ½ä¼˜åŒ–

```python
# æ‰¹é‡å¤„ç†å¤§é‡æ–‡æ¡£
async def process_large_dataset(texts, batch_size=10):
    results = []
    for i in range(0, len(texts), batch_size):
        batch = texts[i:i+batch_size]
        graph = await agraph.build_from_texts(
            texts=batch,
            graph_name=f"batch_{i//batch_size}",
            use_cache=True
        )
        results.append(graph)
    return results
```

### 4. è´¨é‡ç›‘æ§

```python
async def monitor_quality():
    stats = await agraph.get_stats()

    # æ£€æŸ¥å®ä½“æ•°é‡æ˜¯å¦åˆç†
    entity_count = stats['vector_store'].get('entities', 0)
    text_chunks = stats['vector_store'].get('text_chunks', 0)

    if entity_count == 0:
        print("âš ï¸ è­¦å‘Š: æ²¡æœ‰æå–åˆ°å®ä½“")
    elif entity_count / text_chunks < 0.1:
        print("âš ï¸ è­¦å‘Š: å®ä½“å¯†åº¦è¿‡ä½ï¼Œå¯èƒ½éœ€è¦è°ƒæ•´æå–å‚æ•°")
```

## æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

#### 1. åˆå§‹åŒ–å¤±è´¥

```python
# æ£€æŸ¥APIå¯†é’¥
import os
if not os.getenv('OPENAI_API_KEY'):
    print("âŒ è¯·è®¾ç½®OPENAI_API_KEYç¯å¢ƒå˜é‡")

# æ£€æŸ¥ç½‘ç»œè¿æ¥
try:
    await agraph.initialize()
except Exception as e:
    print(f"åˆå§‹åŒ–å¤±è´¥: {e}")
    # å¯èƒ½æ˜¯ç½‘ç»œé—®é¢˜æˆ–APIé™é¢
```

#### 2. æ„å»ºé€Ÿåº¦æ…¢

```python
# å¯ç”¨ç¼“å­˜
knowledge_graph = await agraph.build_from_texts(
    texts=sample_texts,
    use_cache=True  # é‡è¦ï¼
)

# å‡å°‘æ–‡æœ¬é‡
if len(sample_texts) > 100:
    sample_texts = sample_texts[:100]  # å…ˆç”¨å°æ•°æ®é›†æµ‹è¯•
```

#### 3. å†…å­˜ä¸è¶³

```python
# åˆ†æ‰¹å¤„ç†
batch_size = 5  # æ ¹æ®ç³»ç»Ÿå†…å­˜è°ƒæ•´
for i in range(0, len(texts), batch_size):
    batch = texts[i:i+batch_size]
    # å¤„ç†æ‰¹æ¬¡
```

### è°ƒè¯•æŠ€å·§

```python
# å¯ç”¨è¯¦ç»†æ—¥å¿—
import logging
logging.basicConfig(level=logging.DEBUG)

# æ£€æŸ¥å‘é‡å­˜å‚¨çŠ¶æ€
stats = await agraph.get_stats()
print(f"è°ƒè¯•ä¿¡æ¯: {stats}")

# æµ‹è¯•ç®€å•æŸ¥è¯¢
simple_entities = await agraph.search_entities("æµ‹è¯•", top_k=1)
print(f"æµ‹è¯•æŸ¥è¯¢ç»“æœ: {simple_entities}")
```

## å®Œæ•´ç¤ºä¾‹

è¿™é‡Œæ˜¯ä¸€ä¸ªå®Œæ•´çš„å·¥ä½œç¤ºä¾‹ï¼š

```python
#!/usr/bin/env python3
import asyncio
from pathlib import Path
from agraph import AGraph, get_settings

async def complete_example():
    # é…ç½®
    settings = get_settings()
    settings.workdir = str(Path("workdir/tutorial"))

    # ç¤ºä¾‹æ–‡æ¡£
    sample_texts = [
        "TechCorpæ˜¯ä¸€å®¶æˆç«‹äº2018å¹´çš„äººå·¥æ™ºèƒ½å…¬å¸ã€‚å…¬å¸ä¸“æ³¨äºè‡ªç„¶è¯­è¨€å¤„ç†å’Œè®¡ç®—æœºè§†è§‰æŠ€æœ¯ã€‚",
        "å…¬å¸æ€»éƒ¨ä½äºåŒ—äº¬ä¸­å…³æ‘ï¼Œå‘˜å·¥æ€»æ•°120äººï¼Œå…¶ä¸­ç ”å‘äººå‘˜å 80%ã€‚",
        "TechCorpçš„ä¸»è¦äº§å“åŒ…æ‹¬æ™ºèƒ½å®¢æœç³»ç»Ÿã€æ–‡æ¡£åˆ†æå¹³å°å’Œå›¾åƒè¯†åˆ«APIã€‚",
        "å…¬å¸åœ¨2023å¹´å®Œæˆäº†Bè½®èèµ„ï¼Œèèµ„é‡‘é¢5000ä¸‡ç¾å…ƒï¼Œç”±çº¢æ‰èµ„æœ¬é¢†æŠ•ã€‚"
    ]

    # åˆå§‹åŒ–å¹¶æ„å»º
    async with AGraph(
        collection_name="techcorp_knowledge",
        persist_directory=settings.workdir,
        vector_store_type="chroma",
        use_openai_embeddings=True
    ) as agraph:
        await agraph.initialize()

        # æ„å»ºçŸ¥è¯†å›¾è°±
        knowledge_graph = await agraph.build_from_texts(
            texts=sample_texts,
            graph_name="TechCorpçŸ¥è¯†å›¾è°±",
            graph_description="å…³äºTechCorpå…¬å¸çš„çŸ¥è¯†å›¾è°±",
            use_cache=True,
            save_to_vector_store=True
        )

        print(f"âœ… æ„å»ºå®Œæˆ: {len(knowledge_graph.entities)} å®ä½“, {len(knowledge_graph.relations)} å…³ç³»")

        # è¯­ä¹‰æœç´¢
        entities = await agraph.search_entities("å…¬å¸", top_k=3)
        print("\nğŸ” å®ä½“æœç´¢ç»“æœ:")
        for entity, score in entities:
            print(f"  - {entity.name} ({entity.entity_type})")

        # æ™ºèƒ½é—®ç­”
        questions = [
            "TechCorpæ˜¯ä»€ä¹ˆæ—¶å€™æˆç«‹çš„ï¼Ÿ",
            "å…¬å¸æœ‰å¤šå°‘å‘˜å·¥ï¼Ÿ",
            "ä¸»è¦äº§å“æœ‰å“ªäº›ï¼Ÿ"
        ]

        print("\nğŸ’¬ é—®ç­”æ¼”ç¤º:")
        for question in questions:
            print(f"\nâ“ {question}")
            response = await agraph.chat(question)
            print(f"ğŸ¤– {response}")

        # ç³»ç»Ÿç»Ÿè®¡
        stats = await agraph.get_stats()
        print(f"\nğŸ“Š ç³»ç»Ÿç»Ÿè®¡: {stats}")

if __name__ == "__main__":
    asyncio.run(complete_example())
```

è¿è¡Œè¿™ä¸ªç¤ºä¾‹ï¼š

```bash
python complete_example.py
```

## æ€»ç»“

AGraphæä¾›äº†ä¸€ä¸ªç®€å•è€Œå¼ºå¤§çš„æ¥å£æ¥æ„å»ºå’ŒæŸ¥è¯¢çŸ¥è¯†å›¾è°±ã€‚é€šè¿‡æœ¬æ•™ç¨‹ï¼Œä½ å·²ç»å­¦ä¼šäº†ï¼š

- âœ… ç¯å¢ƒè®¾ç½®å’Œåˆå§‹åŒ–
- âœ… ä»æ–‡æœ¬æ„å»ºçŸ¥è¯†å›¾è°±
- âœ… è¯­ä¹‰æœç´¢å’Œæ™ºèƒ½é—®ç­”
- âœ… æ€§èƒ½ä¼˜åŒ–å’Œé”™è¯¯å¤„ç†
- âœ… æœ€ä½³å®è·µå’Œè°ƒè¯•æŠ€å·§

ç°åœ¨ä½ å¯ä»¥å¼€å§‹åœ¨è‡ªå·±çš„é¡¹ç›®ä¸­ä½¿ç”¨AGraphäº†ï¼

## æ›´å¤šèµ„æº

- [APIå‚è€ƒæ–‡æ¡£](../source/modules.rst)
- [å‘é‡æ•°æ®åº“æ•™ç¨‹](vectordb_tutorial.md)
- [å¯¼å…¥å¯¼å‡ºåŠŸèƒ½](import_export_tutorial.md)
- [è‡ªå®šä¹‰å‘é‡æ•°æ®åº“æŒ‡å—](custom_vectordb_guide.md)
