# è‡ªå®šä¹‰å‘é‡æ•°æ®åº“å®ç°æŒ‡å—

æœ¬æŒ‡å—å°†æ•™æ‚¨å¦‚ä½•åˆ›å»ºè‡ªå·±çš„å‘é‡æ•°æ®åº“å®ç°ï¼ŒåŒ…æ‹¬æ¥å£è®¾è®¡ã€æ ¸å¿ƒåŠŸèƒ½å®ç°å’Œé›†æˆåˆ° agraph ç³»ç»Ÿä¸­ã€‚

## ç›®å½•

1. [æ¶æ„æ¦‚è¿°](#æ¶æ„æ¦‚è¿°)
2. [æ¥å£å®šä¹‰](#æ¥å£å®šä¹‰)
3. [å®ç°æ­¥éª¤](#å®ç°æ­¥éª¤)
4. [å®Œæ•´ç¤ºä¾‹](#å®Œæ•´ç¤ºä¾‹)
5. [æ³¨å†Œå’Œä½¿ç”¨](#æ³¨å†Œå’Œä½¿ç”¨)
6. [æµ‹è¯•å’ŒéªŒè¯](#æµ‹è¯•å’ŒéªŒè¯)
7. [æ€§èƒ½ä¼˜åŒ–](#æ€§èƒ½ä¼˜åŒ–)
8. [æœ€ä½³å®è·µ](#æœ€ä½³å®è·µ)

## æ¶æ„æ¦‚è¿°

agraph çš„å‘é‡æ•°æ®åº“ç³»ç»Ÿé‡‡ç”¨æ¨¡å—åŒ–è®¾è®¡ï¼Œæ”¯æŒå¤šç§å­˜å‚¨åç«¯ï¼š

```shell
VectorStore (æ¥å£)
â”œâ”€â”€ VectorStoreCore (æ ¸å¿ƒåŠŸèƒ½)
â”œâ”€â”€ EntityStore (å®ä½“å­˜å‚¨)
â”œâ”€â”€ RelationStore (å…³ç³»å­˜å‚¨)
â”œâ”€â”€ ClusterStore (é›†ç¾¤å­˜å‚¨)
â””â”€â”€ TextChunkStore (æ–‡æœ¬å—å­˜å‚¨)
```

### è®¾è®¡åŸåˆ™

1. **æ¥å£éš”ç¦»** - åªå®ç°éœ€è¦çš„åŠŸèƒ½æ¥å£
2. **å¯æ’æ‹”æ¶æ„** - è½»æ¾æ›¿æ¢å­˜å‚¨åç«¯
3. **å¼‚æ­¥æ”¯æŒ** - æ‰€æœ‰æ“ä½œéƒ½æ˜¯å¼‚æ­¥çš„
4. **ç±»å‹å®‰å…¨** - å®Œæ•´çš„ç±»å‹æ³¨è§£
5. **é”™è¯¯å¤„ç†** - ç»Ÿä¸€çš„å¼‚å¸¸ç®¡ç†

## æ¥å£å®šä¹‰

### æ ¸å¿ƒæ¥å£

```python
from agraph.vectordb.interfaces import VectorStore
from agraph.vectordb.exceptions import VectorStoreError

class CustomVectorStore(VectorStore):
    """è‡ªå®šä¹‰å‘é‡å­˜å‚¨å®ç°"""

    def __init__(self, collection_name: str = "knowledge_graph", **kwargs):
        super().__init__(collection_name, **kwargs)
        # åˆå§‹åŒ–è‡ªå®šä¹‰å‚æ•°

    async def initialize(self) -> None:
        """åˆå§‹åŒ–å­˜å‚¨è¿æ¥å’Œé…ç½®"""
        # å®ç°åˆå§‹åŒ–é€»è¾‘
        self._is_initialized = True

    async def close(self) -> None:
        """å…³é—­å­˜å‚¨è¿æ¥"""
        # å®ç°æ¸…ç†é€»è¾‘
        self._is_initialized = False
```

### å¿…éœ€æ–¹æ³•

æ¯ä¸ªå‘é‡å­˜å‚¨å®ç°å¿…é¡»æä¾›ä»¥ä¸‹æ–¹æ³•ï¼š

#### å®ä½“æ“ä½œ

- `add_entity()` - æ·»åŠ å®ä½“
- `update_entity()` - æ›´æ–°å®ä½“
- `delete_entity()` - åˆ é™¤å®ä½“
- `get_entity()` - è·å–å®ä½“
- `search_entities()` - æœç´¢å®ä½“
- `batch_add_entities()` - æ‰¹é‡æ·»åŠ å®ä½“

#### å…³ç³»æ“ä½œ

- `add_relation()` - æ·»åŠ å…³ç³»
- `update_relation()` - æ›´æ–°å…³ç³»
- `delete_relation()` - åˆ é™¤å…³ç³»
- `get_relation()` - è·å–å…³ç³»
- `search_relations()` - æœç´¢å…³ç³»
- `batch_add_relations()` - æ‰¹é‡æ·»åŠ å…³ç³»

#### å·¥å…·æ–¹æ³•

- `get_stats()` - è·å–ç»Ÿè®¡ä¿¡æ¯
- `clear_all()` - æ¸…ç©ºæ‰€æœ‰æ•°æ®

## å®ç°æ­¥éª¤

### æ­¥éª¤1ï¼šè®¾è®¡æ•°æ®æ¨¡å‹

é¦–å…ˆç¡®å®šå¦‚ä½•å­˜å‚¨å®ä½“ã€å…³ç³»å’Œå‘é‡ï¼š

```python
class CustomVectorStore(VectorStore):
    def __init__(self, storage_path: str, **kwargs):
        super().__init__(**kwargs)
        self.storage_path = storage_path
        self._entities = {}  # å†…å­˜å­˜å‚¨ç¤ºä¾‹
        self._embeddings = {}  # å‘é‡å­˜å‚¨
```

### æ­¥éª¤2ï¼šå®ç°å‘é‡æ“ä½œ

å®ç°å‘é‡è®¡ç®—å’Œç›¸ä¼¼åº¦æœç´¢ï¼š

```python
import numpy as np

def _calculate_similarity(self, vec1: List[float], vec2: List[float]) -> float:
    """è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦"""
    arr1 = np.array(vec1)
    arr2 = np.array(vec2)

    if np.linalg.norm(arr1) == 0 or np.linalg.norm(arr2) == 0:
        return 0.0

    similarity = np.dot(arr1, arr2) / (np.linalg.norm(arr1) * np.linalg.norm(arr2))
    return float(similarity)

def _generate_embedding(self, text: str) -> List[float]:
    """ç”Ÿæˆæ–‡æœ¬åµŒå…¥ï¼ˆç®€å•å®ç°ï¼‰"""
    # è¿™é‡Œå¯ä»¥é›†æˆä»»ä½•åµŒå…¥æ¨¡å‹
    # ç¤ºä¾‹ï¼šç®€å•å­—ç¬¦é¢‘ç‡å‘é‡
    char_counts = [0.0] * 128
    for char in text.lower():
        if ord(char) < 128:
            char_counts[ord(char)] += 1.0

    # å½’ä¸€åŒ–
    total = sum(char_counts)
    if total > 0:
        char_counts = [c / total for c in char_counts]

    return char_counts
```

### æ­¥éª¤3ï¼šå®ç°æ ¸å¿ƒæ–¹æ³•

```python
async def add_entity(
    self, entity: Entity, embedding: Optional[List[float]] = None
) -> bool:
    """æ·»åŠ å®ä½“åˆ°å­˜å‚¨"""
    try:
        if not self._is_initialized:
            raise VectorStoreError("Store not initialized")

        # ç”ŸæˆåµŒå…¥å‘é‡ï¼ˆå¦‚æœæœªæä¾›ï¼‰
        if embedding is None:
            text = f"{entity.name} {entity.description}"
            embedding = self._generate_embedding(text)

        # éªŒè¯åµŒå…¥å‘é‡
        if not self._validate_embedding(embedding):
            raise VectorStoreError(f"Invalid embedding for entity {entity.id}")

        # å­˜å‚¨å®ä½“å’Œå‘é‡
        self._entities[entity.id] = entity
        self._embeddings[entity.id] = embedding

        return True

    except Exception as e:
        raise VectorStoreError(f"Failed to add entity {entity.id}: {e}") from e

async def search_entities(
    self,
    query: Union[str, List[float]],
    top_k: int = 10,
    filter_dict: Optional[Dict[str, Any]] = None
) -> List[Tuple[Entity, float]]:
    """æœç´¢ç›¸ä¼¼å®ä½“"""
    try:
        # ç”ŸæˆæŸ¥è¯¢å‘é‡
        if isinstance(query, str):
            query_vector = self._generate_embedding(query)
        else:
            query_vector = query

        results = []

        for entity_id, entity in self._entities.items():
            # åº”ç”¨è¿‡æ»¤æ¡ä»¶
            if filter_dict and not self._apply_filters(entity, filter_dict):
                continue

            # è®¡ç®—ç›¸ä¼¼åº¦
            if entity_id in self._embeddings:
                entity_vector = self._embeddings[entity_id]
                similarity = self._calculate_similarity(query_vector, entity_vector)
                results.append((entity, similarity))

        # æ’åºå¹¶è¿”å›å‰ top_k ä¸ªç»“æœ
        results.sort(key=lambda x: x[1], reverse=True)
        return results[:top_k]

    except Exception as e:
        raise VectorStoreError(f"Failed to search entities: {e}") from e

def _apply_filters(self, entity: Entity, filter_dict: Dict[str, Any]) -> bool:
    """åº”ç”¨è¿‡æ»¤æ¡ä»¶"""
    for key, value in filter_dict.items():
        if hasattr(entity, key):
            if getattr(entity, key) != value:
                return False
    return True
```

## å®Œæ•´ç¤ºä¾‹

ä»¥ä¸‹æ˜¯ä¸€ä¸ªåŸºäº SQLite çš„å®Œæ•´å‘é‡å­˜å‚¨å®ç°ç¤ºä¾‹æ–‡ä»¶ï¼Œæ‚¨å¯ä»¥å‚è€ƒå®ç°ï¼š

> ğŸ“„ **ç¤ºä¾‹æ–‡ä»¶**: [sqlite_vectorstore.py](./examples/sqlite_vectorstore.py)

è¯¥ç¤ºä¾‹åŒ…å«ï¼š

- å®Œæ•´çš„ SQLite æ•°æ®åº“è¡¨ç»“æ„
- å‘é‡åºåˆ—åŒ–å’Œååºåˆ—åŒ–
- å®ä½“çš„å®Œæ•´ CRUD æ“ä½œ
- ç›¸ä¼¼åº¦è®¡ç®—å’Œæœç´¢
- æ‰¹é‡æ“ä½œæ”¯æŒ

## æ³¨å†Œå’Œä½¿ç”¨

### æ³¨å†Œè‡ªå®šä¹‰å­˜å‚¨

```python
from agraph.vectordb import VectorStoreFactory, VectorStoreType

# æ–¹å¼1ï¼šç›´æ¥æ³¨å†Œ
from my_package import SQLiteVectorStore
VectorStoreFactory.register_store_class("sqlite", SQLiteVectorStore)

# æ–¹å¼2ï¼šä½¿ç”¨è£…é¥°å™¨
@VectorStoreFactory.register("sqlite")
class SQLiteVectorStore(VectorStore):
    # å®ç°...
    pass

# ä½¿ç”¨è‡ªå®šä¹‰å­˜å‚¨
store = VectorStoreFactory.create_store(
    "sqlite",
    db_path="./my_vectors.db",
    embedding_dimension=256
)
```

### åˆ›å»ºä¾¿æ·å‡½æ•°

```python
def create_sqlite_store(db_path: str = "vectorstore.db", **kwargs) -> SQLiteVectorStore:
    """ä¾¿æ·åˆ›å»ºå‡½æ•°"""
    return SQLiteVectorStore(db_path=db_path, **kwargs)

# å¯¼å‡ºåˆ° __init__.py
__all__ = ["create_sqlite_store", "SQLiteVectorStore"]

# ä½¿ç”¨
from my_vectordb import create_sqlite_store
store = create_sqlite_store("./knowledge_base.db")
```

## æµ‹è¯•å’ŒéªŒè¯

### å•å…ƒæµ‹è¯•æ¡†æ¶

```python
import unittest
from agraph.base import Entity, EntityType

class TestCustomVectorStore(unittest.TestCase):
    async def setUp(self):
        self.store = CustomVectorStore()
        await self.store.initialize()

    async def test_add_and_get_entity(self):
        entity = Entity(
            name="Python",
            entity_type=EntityType.CONCEPT,
            description="ç¼–ç¨‹è¯­è¨€"
        )

        # æµ‹è¯•æ·»åŠ 
        result = await self.store.add_entity(entity)
        self.assertTrue(result)

        # æµ‹è¯•è·å–
        retrieved = await self.store.get_entity(entity.id)
        self.assertIsNotNone(retrieved)
        self.assertEqual(retrieved.name, "Python")

    async def test_search_functionality(self):
        # æ·»åŠ æµ‹è¯•æ•°æ®
        entities = [
            Entity(name="Python", entity_type=EntityType.CONCEPT),
            Entity(name="Java", entity_type=EntityType.CONCEPT),
            Entity(name="JavaScript", entity_type=EntityType.CONCEPT)
        ]

        for entity in entities:
            await self.store.add_entity(entity)

        # æœç´¢æµ‹è¯•
        results = await self.store.search_entities("ç¼–ç¨‹", top_k=3)
        self.assertGreater(len(results), 0)

        # éªŒè¯ç»“æœæ ¼å¼
        for entity, score in results:
            self.assertIsInstance(entity, Entity)
            self.assertIsInstance(score, float)
            self.assertGreaterEqual(score, 0.0)
            self.assertLessEqual(score, 1.0)

    async def tearDown(self):
        await self.store.close()
```

### æ€§èƒ½åŸºå‡†æµ‹è¯•

```python
import time
import asyncio
from typing import List

async def benchmark_operations():
    """æ€§èƒ½åŸºå‡†æµ‹è¯•"""
    store = CustomVectorStore()
    await store.initialize()

    # æµ‹è¯•æ‰¹é‡æ·»åŠ æ€§èƒ½
    entities = [
        Entity(name=f"Entity_{i}", entity_type=EntityType.CONCEPT)
        for i in range(1000)
    ]

    start_time = time.time()
    results = await store.batch_add_entities(entities)
    add_time = time.time() - start_time

    print(f"æ‰¹é‡æ·»åŠ  1000 ä¸ªå®ä½“:")
    print(f"  è€—æ—¶: {add_time:.2f} ç§’")
    print(f"  æˆåŠŸç‡: {sum(results) / len(results) * 100:.1f}%")
    print(f"  å¹³å‡é€Ÿåº¦: {len(entities) / add_time:.0f} ä¸ª/ç§’")

    # æµ‹è¯•æœç´¢æ€§èƒ½
    search_queries = ["test", "æ¦‚å¿µ", "å®ä½“", "æ•°æ®", "ä¿¡æ¯"]

    start_time = time.time()
    for query in search_queries:
        await store.search_entities(query, top_k=10)
    search_time = time.time() - start_time

    print(f"\næœç´¢æ€§èƒ½ ({len(search_queries)} æ¬¡æŸ¥è¯¢):")
    print(f"  æ€»è€—æ—¶: {search_time:.3f} ç§’")
    print(f"  å¹³å‡è€—æ—¶: {search_time / len(search_queries) * 1000:.1f} æ¯«ç§’/æŸ¥è¯¢")

    await store.close()

# è¿è¡ŒåŸºå‡†æµ‹è¯•
asyncio.run(benchmark_operations())
```

## æ€§èƒ½ä¼˜åŒ–

### 1. å‘é‡ç´¢å¼•ä¼˜åŒ–

```python
# ä½¿ç”¨ Faiss è¿›è¡Œé«˜æ•ˆå‘é‡æœç´¢
import faiss
import numpy as np

class IndexedVectorStore(CustomVectorStore):
    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        self._index: Optional[faiss.Index] = None
        self._id_mapping: Dict[int, str] = {}
        self._embedding_dimension = 128

    async def _build_index(self):
        """æ„å»ºå‘é‡ç´¢å¼•"""
        if not self._embeddings:
            return

        vectors = list(self._embeddings.values())
        ids = list(self._embeddings.keys())

        # åˆ›å»º Faiss ç´¢å¼•
        self._index = faiss.IndexFlatIP(self._embedding_dimension)

        # æ·»åŠ å‘é‡
        vector_array = np.array(vectors, dtype=np.float32)
        self._index.add(vector_array)

        # å»ºç«‹ ID æ˜ å°„
        self._id_mapping = {i: entity_id for i, entity_id in enumerate(ids)}

    async def search_entities(self, query, top_k=10, filter_dict=None):
        """ä½¿ç”¨ç´¢å¼•è¿›è¡Œå¿«é€Ÿæœç´¢"""
        if self._index is None:
            await self._build_index()

        # ç”ŸæˆæŸ¥è¯¢å‘é‡
        if isinstance(query, str):
            query_vector = self._generate_embedding(query)
        else:
            query_vector = query

        # æœç´¢
        query_array = np.array([query_vector], dtype=np.float32)
        scores, indices = self._index.search(query_array, top_k)

        results = []
        for i, score in zip(indices[0], scores[0]):
            if i != -1:  # æœ‰æ•ˆç´¢å¼•
                entity_id = self._id_mapping[i]
                entity = self._entities[entity_id]

                # åº”ç”¨è¿‡æ»¤
                if filter_dict and not self._apply_filters(entity, filter_dict):
                    continue

                results.append((entity, float(score)))

        return results
```

### 2. æ‰¹é‡æ“ä½œä¼˜åŒ–

```python
async def optimized_batch_add_entities(
    self,
    entities: List[Entity],
    embeddings: Optional[List[List[float]]] = None
) -> List[bool]:
    """ä¼˜åŒ–çš„æ‰¹é‡æ·»åŠ """
    try:
        # å‡†å¤‡æ‰¹é‡æ•°æ®
        batch_entities = {}
        batch_embeddings = {}

        for i, entity in enumerate(entities):
            embedding = embeddings[i] if embeddings else None
            if embedding is None:
                text = f"{entity.name} {entity.description}"
                embedding = self._generate_embedding(text)

            if self._validate_embedding(embedding):
                batch_entities[entity.id] = entity
                batch_embeddings[entity.id] = embedding

        # æ‰¹é‡å­˜å‚¨ï¼ˆå…·ä½“å®ç°å–å†³äºåç«¯ï¼‰
        self._entities.update(batch_entities)
        self._embeddings.update(batch_embeddings)

        # é‡å»ºç´¢å¼•
        if hasattr(self, '_build_index'):
            await self._build_index()

        return [True] * len(entities)

    except Exception as e:
        raise VectorStoreError(f"Batch add failed: {e}") from e
```

### 3. å†…å­˜ç®¡ç†å’Œç¼“å­˜

```python
from collections import OrderedDict

class CachedVectorStore(CustomVectorStore):
    def __init__(self, cache_size: int = 1000, **kwargs):
        super().__init__(**kwargs)
        self.cache_size = cache_size
        self._entity_cache: OrderedDict[str, Entity] = OrderedDict()

    async def get_entity(self, entity_id: str) -> Optional[Entity]:
        """å¸¦ç¼“å­˜çš„å®ä½“è·å–"""
        # æ£€æŸ¥ç¼“å­˜
        if entity_id in self._entity_cache:
            # ç§»åŠ¨åˆ°æœ«å°¾ï¼ˆLRUï¼‰
            self._entity_cache.move_to_end(entity_id)
            return self._entity_cache[entity_id]

        # ä»å­˜å‚¨åŠ è½½
        entity = await self._load_entity_from_storage(entity_id)

        if entity:
            # æ·»åŠ åˆ°ç¼“å­˜
            self._add_to_cache(entity_id, entity)

        return entity

    def _add_to_cache(self, entity_id: str, entity: Entity):
        """æ·»åŠ åˆ°ç¼“å­˜ï¼Œç®¡ç†ç¼“å­˜å¤§å°"""
        if len(self._entity_cache) >= self.cache_size:
            # ç§»é™¤æœ€æ—§çš„æ¡ç›®
            self._entity_cache.popitem(last=False)

        self._entity_cache[entity_id] = entity

    async def _load_entity_from_storage(self, entity_id: str) -> Optional[Entity]:
        """ä»å­˜å‚¨åç«¯åŠ è½½å®ä½“"""
        # å…·ä½“å®ç°å–å†³äºå­˜å‚¨åç«¯
        return self._entities.get(entity_id)
```

## æœ€ä½³å®è·µ

### 1. é”™è¯¯å¤„ç†å’Œæ¢å¤

```python
from agraph.vectordb.exceptions import (
    VectorStoreError,
    VectorStoreNotInitializedError,
    VectorStoreOperationError
)

async def robust_add_entity(self, entity: Entity,
                          embedding: Optional[List[float]] = None,
                          max_retries: int = 3) -> bool:
    """å¸¦é‡è¯•çš„å¥å£®æ·»åŠ æ–¹æ³•"""
    if not self._is_initialized:
        raise VectorStoreNotInitializedError("Store not initialized")

    for attempt in range(max_retries):
        try:
            # éªŒè¯è¾“å…¥
            if not entity or not entity.id:
                raise VectorStoreOperationError("Invalid entity: missing ID")

            # ç”ŸæˆåµŒå…¥
            if embedding is None:
                if not entity.name and not entity.description:
                    raise VectorStoreOperationError(
                        "Cannot generate embedding: missing name and description"
                    )

                text = f"{entity.name} {entity.description}"
                embedding = self._generate_embedding(text)

            # éªŒè¯åµŒå…¥
            if not self._validate_embedding(embedding):
                raise VectorStoreOperationError(f"Invalid embedding for entity {entity.id}")

            # æ‰§è¡Œå­˜å‚¨æ“ä½œ
            result = await self._store_entity(entity, embedding)

            if result:
                logger.info(f"Successfully added entity {entity.id}")
                return True

        except VectorStoreError:
            # é‡æ–°æŠ›å‡ºå·²çŸ¥é”™è¯¯ï¼ˆä¸é‡è¯•ï¼‰
            raise
        except Exception as e:
            logger.warning(
                f"Attempt {attempt + 1} failed for entity {entity.id}: {e}"
            )

            if attempt == max_retries - 1:
                # æœ€åä¸€æ¬¡å°è¯•å¤±è´¥
                raise VectorStoreOperationError(
                    f"Failed to add entity {entity.id} after {max_retries} attempts: {e}"
                ) from e

            # ç­‰å¾…åé‡è¯•
            await asyncio.sleep(0.1 * (2 ** attempt))  # æŒ‡æ•°é€€é¿

    return False
```

### 2. ç›‘æ§å’ŒæŒ‡æ ‡

```python
import time
from dataclasses import dataclass, field
from typing import Dict, Any

@dataclass
class VectorStoreMetrics:
    """å‘é‡å­˜å‚¨æ€§èƒ½æŒ‡æ ‡"""
    operations_count: int = 0
    errors_count: int = 0
    total_search_time: float = 0.0
    total_add_time: float = 0.0
    cache_hits: int = 0
    cache_misses: int = 0

    @property
    def error_rate(self) -> float:
        return self.errors_count / max(1, self.operations_count)

    @property
    def avg_search_time(self) -> float:
        return self.total_search_time / max(1, self.operations_count)

class MonitoredVectorStore(CustomVectorStore):
    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        self.metrics = VectorStoreMetrics()

    async def add_entity(self, entity: Entity, embedding: Optional[List[float]] = None) -> bool:
        """å¸¦ç›‘æ§çš„æ·»åŠ æ–¹æ³•"""
        start_time = time.time()

        try:
            result = await super().add_entity(entity, embedding)

            # è®°å½•æˆåŠŸæŒ‡æ ‡
            self.metrics.operations_count += 1
            self.metrics.total_add_time += time.time() - start_time

            return result

        except Exception as e:
            # è®°å½•é”™è¯¯æŒ‡æ ‡
            self.metrics.errors_count += 1
            raise

    def get_performance_report(self) -> Dict[str, Any]:
        """è·å–æ€§èƒ½æŠ¥å‘Š"""
        return {
            "operations": self.metrics.operations_count,
            "errors": self.metrics.errors_count,
            "error_rate": f"{self.metrics.error_rate:.2%}",
            "avg_search_time": f"{self.metrics.avg_search_time:.3f}s",
            "cache_hit_rate": f"{self.metrics.cache_hits / max(1, self.metrics.cache_hits + self.metrics.cache_misses):.2%}"
        }
```

### 3. é…ç½®å’Œç¯å¢ƒç®¡ç†

```python
from dataclasses import dataclass
from typing import Optional
import os

@dataclass
class VectorStoreConfig:
    """å‘é‡å­˜å‚¨é…ç½®"""
    storage_path: str = "vectorstore.db"
    embedding_dimension: int = 128
    cache_size: int = 1000
    batch_size: int = 100
    max_retries: int = 3
    enable_monitoring: bool = True

    @classmethod
    def from_env(cls) -> "VectorStoreConfig":
        """ä»ç¯å¢ƒå˜é‡åˆ›å»ºé…ç½®"""
        return cls(
            storage_path=os.getenv("VECTOR_STORE_PATH", "vectorstore.db"),
            embedding_dimension=int(os.getenv("EMBEDDING_DIM", "128")),
            cache_size=int(os.getenv("CACHE_SIZE", "1000")),
            batch_size=int(os.getenv("BATCH_SIZE", "100")),
            max_retries=int(os.getenv("MAX_RETRIES", "3")),
            enable_monitoring=os.getenv("ENABLE_MONITORING", "true").lower() == "true"
        )

# ä½¿ç”¨é…ç½®
config = VectorStoreConfig.from_env()
store = CustomVectorStore(
    storage_path=config.storage_path,
    embedding_dimension=config.embedding_dimension,
    cache_size=config.cache_size
)
```

## æ€»ç»“

åˆ›å»ºè‡ªå®šä¹‰å‘é‡æ•°æ®åº“å®ç°çš„å…³é”®æ­¥éª¤ï¼š

1. **ğŸ—ï¸ æ¶æ„è®¾è®¡** - åŸºäº agraph çš„æ¥å£ç³»ç»Ÿè®¾è®¡
2. **âš¡ æ ¸å¿ƒå®ç°** - å®ç°å‘é‡å­˜å‚¨ã€æœç´¢å’Œç®¡ç†åŠŸèƒ½
3. **ğŸ”§ æ€§èƒ½ä¼˜åŒ–** - ç´¢å¼•ã€ç¼“å­˜å’Œæ‰¹é‡æ“ä½œä¼˜åŒ–
4. **ğŸ§ª æµ‹è¯•éªŒè¯** - å®Œå–„çš„å•å…ƒæµ‹è¯•å’Œæ€§èƒ½æµ‹è¯•
5. **ğŸ“Š ç›‘æ§è¿ç»´** - æŒ‡æ ‡æ”¶é›†ã€é”™è¯¯å¤„ç†å’Œæ—¥å¿—ç®¡ç†
6. **ğŸš€ é›†æˆéƒ¨ç½²** - æ³¨å†Œåˆ°å·¥å‚ç³»ç»Ÿï¼Œä¾¿äºä½¿ç”¨

é€šè¿‡éµå¾ªè¿™äº›æŒ‡å—å’Œæœ€ä½³å®è·µï¼Œæ‚¨å¯ä»¥åˆ›å»ºé«˜æ•ˆã€å¯é çš„è‡ªå®šä¹‰å‘é‡æ•°æ®åº“å®ç°ï¼Œæ»¡è¶³ç‰¹å®šçš„ä¸šåŠ¡éœ€æ±‚ã€‚

## ç›¸å…³é“¾æ¥

- [å‘é‡æ•°æ®åº“ä½¿ç”¨æ•™ç¨‹](./vectordb_tutorial.md)
- [API å‚è€ƒæ–‡æ¡£](../source/agraph.vectordb.rst)
- [å®Œæ•´ç¤ºä¾‹ä»£ç ](./examples/)
